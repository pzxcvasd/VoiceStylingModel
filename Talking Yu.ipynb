{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOBwYjjgZ4qtdRPj9mfOS5/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RCotV2O1_Lra"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"kUS4vZga-n0t"},"source":["## PREPARE"]},{"cell_type":"markdown","metadata":{"id":"emo9JzM7CpX9"},"source":["datasets_yoo, audio, fine_output 폴더 만들기."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ogUYjFfhcxTG","outputId":"6d511af7-5b29-48f7-9a9c-e90775e2153a","executionInfo":{"status":"ok","timestamp":1701593052259,"user_tz":-540,"elapsed":9,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU Connected, your runtime has 13.6 gigabytes of available RAM\n","\n"]}],"source":["#@title Connect and check GPU and runtime\n","from psutil import virtual_memory\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","ram_gb = virtual_memory().total / 1e9\n","if gpu_info.find('failed') >= 0:\n","    print('Not connected to a GPU', end=\"\")\n","elif gpu_info.find('not found') >= 0:\n","    print('Not connected to a GPU', end=\"\")\n","else:\n","    print('GPU Connected', end=\"\")\n","print(', your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n"]},{"cell_type":"markdown","metadata":{"id":"HJQ4TI0_Qowr"},"source":["### Setup Notebook, Install dependencies\n","<small>Run both cells to install system and needed functions.  \n","_If Colab for some reason crashes re-run cell 0.2 before contining._</small>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r8wG_tIaOV0Q","outputId":"91e76469-2924-4ea7-e908-88f1660c3755","executionInfo":{"status":"ok","timestamp":1701593165852,"user_tz":-540,"elapsed":102590,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'bark'...\n","remote: Enumerating objects: 1527, done.\u001b[K\n","remote: Counting objects: 100% (376/376), done.\u001b[K\n","remote: Compressing objects: 100% (69/69), done.\u001b[K\n","remote: Total 1527 (delta 334), reused 309 (delta 307), pack-reused 1151\u001b[K\n","Receiving objects: 100% (1527/1527), 19.82 MiB | 13.53 MiB/s, done.\n","Resolving deltas: 100% (776/776), done.\n","/content/bark\n","Ignoring sox: markers 'platform_system == \"Darwin\"' don't match your environment\n","Ignoring soundfile: markers 'platform_system == \"Windows\"' don't match your environment\n","Ignoring fairseq: markers 'platform_system == \"Windows\"' don't match your environment\n","Ignoring fairseq: markers 'platform_system == \"Darwin\"' don't match your environment\n","Ignoring pywin32: markers 'platform_system == \"Windows\"' don't match your environment\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 1)) (67.7.2)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 2)) (4.35.2)\n","Collecting diffusers (from -r old_setup_files/requirements-pip.txt (line 3))\n","  Downloading diffusers-0.24.0-py3-none-any.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpeg-downloader (from -r old_setup_files/requirements-pip.txt (line 4))\n","  Downloading ffmpeg_downloader-0.2.0-py3-none-any.whl (27 kB)\n","Collecting ffmpeg (from -r old_setup_files/requirements-pip.txt (line 5))\n","  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ffmpeg-python (from -r old_setup_files/requirements-pip.txt (line 6))\n","  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n","Collecting sox (from -r old_setup_files/requirements-pip.txt (line 7))\n","  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n","Collecting fairseq (from -r old_setup_files/requirements-pip.txt (line 12))\n","  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 13)) (0.10.1)\n","Collecting boto3 (from -r old_setup_files/requirements-pip.txt (line 14))\n","  Downloading boto3-1.33.6-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m545.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting funcy (from -r old_setup_files/requirements-pip.txt (line 15))\n","  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 16)) (1.23.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 17)) (1.11.4)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 18)) (0.15.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 19)) (4.66.1)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 20)) (7.34.0)\n","Requirement already satisfied: huggingface_hub>0.15 in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 21)) (0.19.4)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 22)) (13.7.0)\n","Collecting pathvalidate (from -r old_setup_files/requirements-pip.txt (line 23))\n","  Downloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\n","Collecting rich-argparse (from -r old_setup_files/requirements-pip.txt (line 24))\n","  Downloading rich_argparse-1.4.0-py3-none-any.whl (19 kB)\n","Collecting encodec (from -r old_setup_files/requirements-pip.txt (line 25))\n","  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 26)) (5.2.0)\n","Collecting pydub (from -r old_setup_files/requirements-pip.txt (line 27))\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 28)) (2.31.0)\n","Collecting audio2numpy (from -r old_setup_files/requirements-pip.txt (line 29))\n","  Downloading audio2numpy-0.1.2-py3-none-any.whl (10 kB)\n","Collecting faiss-cpu (from -r old_setup_files/requirements-pip.txt (line 30))\n","  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 31)) (1.3.2)\n","Collecting audiolm-pytorch (from -r old_setup_files/requirements-pip.txt (line 32))\n","  Downloading audiolm_pytorch-1.8.5-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting universal-startfile (from -r old_setup_files/requirements-pip.txt (line 33))\n","  Downloading universal_startfile-0.2-py3-none-any.whl (3.4 kB)\n","Collecting gradio>=3.34.0 (from -r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading gradio-4.7.1-py3-none-any.whl (16.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r old_setup_files/requirements-pip.txt (line 2)) (3.13.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r old_setup_files/requirements-pip.txt (line 2)) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r old_setup_files/requirements-pip.txt (line 2)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r old_setup_files/requirements-pip.txt (line 2)) (2023.6.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r old_setup_files/requirements-pip.txt (line 2)) (0.4.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers->-r old_setup_files/requirements-pip.txt (line 3)) (9.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->-r old_setup_files/requirements-pip.txt (line 3)) (6.8.0)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ffmpeg-downloader->-r old_setup_files/requirements-pip.txt (line 4)) (1.4.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python->-r old_setup_files/requirements-pip.txt (line 6)) (0.18.3)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (1.16.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (3.0.6)\n","Collecting hydra-core<1.1,>=1.0.7 (from fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n","  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting omegaconf<2.1 (from fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n","  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n","Collecting sacrebleu>=1.4.12 (from fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n","  Downloading sacrebleu-2.3.3-py3-none-any.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.4/106.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (2.1.0+cu118)\n","Collecting bitarray (from fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n","  Downloading bitarray-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (2.1.0+cu118)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (3.0.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (1.2.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (4.4.2)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (0.58.1)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (0.12.1)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (1.8.0)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (4.5.0)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (0.3)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (1.0.7)\n","Collecting botocore<1.34.0,>=1.33.6 (from boto3->-r old_setup_files/requirements-pip.txt (line 14))\n","  Downloading botocore-1.33.6-py3-none-any.whl (11.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->-r old_setup_files/requirements-pip.txt (line 14))\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.9.0,>=0.8.2 (from boto3->-r old_setup_files/requirements-pip.txt (line 14))\n","  Downloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jedi>=0.16 (from ipython->-r old_setup_files/requirements-pip.txt (line 20))\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (3.0.41)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (4.9.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>0.15->-r old_setup_files/requirements-pip.txt (line 21)) (2023.6.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->-r old_setup_files/requirements-pip.txt (line 22)) (3.0.0)\n","Collecting einops (from encodec->-r old_setup_files/requirements-pip.txt (line 25))\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r old_setup_files/requirements-pip.txt (line 28)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r old_setup_files/requirements-pip.txt (line 28)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r old_setup_files/requirements-pip.txt (line 28)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r old_setup_files/requirements-pip.txt (line 28)) (2023.11.17)\n","Collecting accelerate>=0.24.0 (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting beartype>=0.16.1 (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n","  Downloading beartype-0.16.4-py3-none-any.whl (819 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.1/819.1 kB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ema-pytorch>=0.2.2 (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n","  Downloading ema_pytorch-0.3.1-py3-none-any.whl (4.8 kB)\n","Collecting gateloop-transformer>=0.0.24 (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n","  Downloading gateloop_transformer-0.1.1-py3-none-any.whl (9.7 kB)\n","Collecting local-attention>=1.9.0 (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n","  Downloading local_attention-1.9.0-py3-none-any.whl (8.2 kB)\n","Collecting sentencepiece (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting vector-quantize-pytorch>=1.11.8 (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n","  Downloading vector_quantize_pytorch-1.11.8-py3-none-any.whl (24 kB)\n","Collecting aiofiles<24.0,>=22.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (4.2.2)\n","Collecting fastapi (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ffmpy (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gradio-client==0.7.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading gradio_client-0.7.0-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpx (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (6.1.1)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (3.1.2)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (2.1.3)\n","Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (3.7.1)\n","Collecting orjson~=3.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (1.5.3)\n","Collecting pydantic>=2.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-multipart (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting semantic-version~=2.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting tomlkit==0.12.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n","Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (0.9.0)\n","Collecting uvicorn>=0.14.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio-client==0.7.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.24.0->audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32)) (5.9.5)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (0.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (4.19.2)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (0.12.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.0,>=1.33.6->boto3->-r old_setup_files/requirements-pip.txt (line 14)) (2.8.2)\n","Collecting rotary-embedding-torch (from gateloop-transformer>=0.0.24->audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n","  Downloading rotary_embedding_torch-0.4.0-py3-none-any.whl (5.1 kB)\n","Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->-r old_setup_files/requirements-pip.txt (line 20)) (0.8.3)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->-r old_setup_files/requirements-pip.txt (line 22)) (0.1.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (4.45.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (3.1.1)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->-r old_setup_files/requirements-pip.txt (line 13)) (0.41.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (2023.3.post1)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->-r old_setup_files/requirements-pip.txt (line 20)) (0.7.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->-r old_setup_files/requirements-pip.txt (line 13)) (4.0.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r old_setup_files/requirements-pip.txt (line 20)) (0.2.12)\n","Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n","Collecting pydantic-core==2.14.5 (from pydantic>=2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions>=4.1.1 (from librosa->-r old_setup_files/requirements-pip.txt (line 13))\n","  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Collecting portalocker (from sacrebleu>=1.4.12->fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n","  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (0.9.0)\n","Collecting colorama (from sacrebleu>=1.4.12->fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (4.9.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->-r old_setup_files/requirements-pip.txt (line 13)) (3.2.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (2.21)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (3.2.1)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (2.1.0)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (8.1.7)\n","Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n","Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (3.7.1)\n","Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httpcore==1.* (from httpx->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n","  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (1.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->-r old_setup_files/requirements-pip.txt (line 3)) (3.17.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (1.2.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (2023.11.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (0.31.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (0.13.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.0,>=1.33.6->boto3->-r old_setup_files/requirements-pip.txt (line 14)) (1.16.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (1.3.0)\n","Building wheels for collected packages: ffmpeg, fairseq, encodec, antlr4-python3-runtime, ffmpy\n","  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6080 sha256=4ee21b22571aaab3c1e59be0432b9285ec10f037cf0794938808f3ede7c510e0\n","  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n","  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291817 sha256=fecf407e7e3fd172c36aa623a028e9cfac3effa32f2bac232b5eaeeb361cb9cf\n","  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n","  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45759 sha256=b58adebfd3f1795e689cd1bbc862e29dd8eab00a54739c3ee846cb0f23cc0cf1\n","  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=f3556660bdb77c260ec18f13929fb5dbe246bbb08000078cc4abdd405d81a338\n","  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n","  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=7346446bd0443aa4d82a8bc0250428f2ee3bec0724d5f4ad0f196615a3947c6a\n","  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n","Successfully built ffmpeg fairseq encodec antlr4-python3-runtime ffmpy\n","Installing collected packages: sentencepiece, pydub, funcy, ffmpy, ffmpeg, faiss-cpu, bitarray, antlr4-python3-runtime, websockets, universal-startfile, typing-extensions, tomlkit, sox, shellingham, semantic-version, python-multipart, portalocker, pathvalidate, orjson, jmespath, jedi, h11, ffmpeg-python, einops, colorama, beartype, audio2numpy, annotated-types, aiofiles, uvicorn, starlette, sacrebleu, pydantic-core, omegaconf, httpcore, ffmpeg-downloader, botocore, vector-quantize-pytorch, s3transfer, rotary-embedding-torch, rich-argparse, pydantic, local-attention, hydra-core, httpx, ema-pytorch, diffusers, accelerate, gradio-client, gateloop-transformer, fastapi, fairseq, encodec, boto3, gradio, audiolm-pytorch\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 1.10.13\n","    Uninstalling pydantic-1.10.13:\n","      Successfully uninstalled pydantic-1.10.13\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.25.0 aiofiles-23.2.1 annotated-types-0.6.0 antlr4-python3-runtime-4.8 audio2numpy-0.1.2 audiolm-pytorch-1.8.5 beartype-0.16.4 bitarray-2.8.3 boto3-1.33.6 botocore-1.33.6 colorama-0.4.6 diffusers-0.24.0 einops-0.7.0 ema-pytorch-0.3.1 encodec-0.1.1 fairseq-0.12.2 faiss-cpu-1.7.4 fastapi-0.104.1 ffmpeg-1.4 ffmpeg-downloader-0.2.0 ffmpeg-python-0.2.0 ffmpy-0.3.1 funcy-2.0 gateloop-transformer-0.1.1 gradio-4.7.1 gradio-client-0.7.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 hydra-core-1.0.7 jedi-0.19.1 jmespath-1.0.1 local-attention-1.9.0 omegaconf-2.0.6 orjson-3.9.10 pathvalidate-3.2.0 portalocker-2.8.2 pydantic-2.5.2 pydantic-core-2.14.5 pydub-0.25.1 python-multipart-0.0.6 rich-argparse-1.4.0 rotary-embedding-torch-0.4.0 s3transfer-0.8.2 sacrebleu-2.3.3 semantic-version-2.10.0 sentencepiece-0.1.99 shellingham-1.5.4 sox-1.4.1 starlette-0.27.0 tomlkit-0.12.0 typing-extensions-4.8.0 universal-startfile-0.2 uvicorn-0.24.0.post1 vector-quantize-pytorch-1.11.8 websockets-11.0.3\n","Requirement already satisfied: encodec in /usr/local/lib/python3.10/dist-packages (0.1.1)\n","Requirement already satisfied: rich-argparse in /usr/local/lib/python3.10/dist-packages (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from encodec) (1.23.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from encodec) (2.1.0+cu118)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from encodec) (2.1.0+cu118)\n","Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from encodec) (0.7.0)\n","Requirement already satisfied: rich>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from rich-argparse) (13.7.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.0.0->rich-argparse) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.0.0->rich-argparse) (2.16.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (4.8.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (2.1.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.0.0->rich-argparse) (0.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->encodec) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->encodec) (1.3.0)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n","Collecting devtools\n","  Downloading devtools-0.12.2-py3-none-any.whl (19 kB)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n","Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n","Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n","Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.8.0)\n","Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n","Collecting asttokens<3.0.0,>=2.0.0 (from devtools)\n","  Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n","Collecting executing>=1.1.1 (from devtools)\n","  Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: pygments>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from devtools) (2.16.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens<3.0.0,>=2.0.0->devtools) (1.16.0)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.0.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.11.17)\n","Installing collected packages: executing, asttokens, devtools\n","Successfully installed asttokens-2.4.1 devtools-0.12.2 executing-2.0.1\n"]}],"source":["#@title 0.1 - Install system\n","from IPython.display import clear_output\n","!git clone https://github.com/JonathanFly/bark.git\n","%cd bark\n","!pip install -r old_setup_files/requirements-pip.txt\n","!pip install encodec rich-argparse\n","!pip install librosa pydub devtools\n","\n","#clear_output()\n","#print('Cell completed.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKTvqvVkOwXM"},"outputs":[],"source":[" #@title 0.2 - Setup required functions and helpers\n","import os\n","import time\n","from bark_infinity import config\n","import numpy as np\n","\n","logger = config.logger\n","logger.setLevel(\"WARNING\")\n","\n","from bark_infinity import generation #이걸 보고.. conversion 가능하게 수정..\n","from bark_infinity import api\n","\n","import rich\n","from rich import print\n","from rich import pretty\n","from rich.pretty import pprint\n","from rich import inspect\n","\n","import librosa\n","from pydub import AudioSegment\n","import ipywidgets as widgets\n","from IPython.display import display, Audio\n","from io import BytesIO\n","\n","# None of this code, just fiddlign with Colab stuff\n","# Just to save Colab with outputs and float32 wavs are GIGANTO\n","# actually this doesn't work, the iPython widget converts it back to float32? or I messed up\n","\n","def display_audio_int16_but(audio_arr_segments, file_name, sample_rate=generation.SAMPLE_RATE,  width='200px'):\n","    file_name_label = widgets.Label(value=f\"Playing: {file_name}\")\n","    file_name_label.layout.width = width\n","    audio_data_int16 = audio_arr_segments\n","    if isinstance(audio_data_int16, list):\n","        audio_data_int16 = np.concatenate(audio_data_int16)\n","\n","    #audio_data_int16 = np.int16(audio_data_int16 * np.iinfo(np.int16).max)\n","\n","\n","    audio_widget = Audio(audio_data_int16, rate=sample_rate)\n","    display(file_name_label, audio_widget)\n","\n","\n","def on_button_click(button):\n","    audio_data, sample_rate = librosa.load(button.wav_path, sr=None)\n","    file_name = os.path.basename(button.wav_path)\n","    display_audio_int16_but(audio_data,file_name, sample_rate)\n","\n","def display_wav_files(directory, matchType=\".wav\"):\n","    subdirs, wav_files = [], []\n","\n","    for item in os.listdir(directory):\n","        item_path = os.path.join(directory, item)\n","\n","        if os.path.isfile(item_path) and item_path.endswith(matchType):\n","            wav_files.append(item_path)\n","        elif os.path.isdir(item_path):\n","            subdirs.append(item_path)\n","\n","    wav_files.sort(key=lambda x: os.path.basename(x))\n","\n","    for wav_file in wav_files:\n","\n","        filename = os.path.basename(wav_file)\n","        print(f\" {filename}\")\n","        display( Audio(filename=wav_file, rate=generation.SAMPLE_RATE) )\n","        #button = widgets.Button(description=f\"Play {filename}\")\n","        #button.wav_path = wav_file\n","        #button.on_click(on_button_click)\n","        #display(button)\n","\n","    for subdir in sorted(subdirs):\n","        print(f\"<{subdir}>\")\n","        display_wav_files(subdir, matchType)\n","\n","def display_mp4_files(directory):\n","    return display_wav_files(directory, '.mp4')\n"]},{"cell_type":"markdown","metadata":{"id":"VbIE0Bv8jxtN"},"source":["### Gradio App"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BQfEqnxMpUk1","outputId":"311f8822-a9f6-4ce1-d893-08969bdbc6d4","executionInfo":{"status":"ok","timestamp":1701593492408,"user_tz":-540,"elapsed":13943,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-12-03 08:51:23.226026: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-03 08:51:23.226094: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-03 08:51:23.226133: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-03 08:51:24.689117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Pytorch version: 2.1\n","=== GPU Information ===\n","GPU Device: Tesla T4\n","Total memory: 14.74786376953125 GB\n","CUDA Version: 11.8\n","PyTorch Version: 2.1.0+cu118\n","   GPU Memory Free: 14.65 GB, Total: 14.75 GB\n","   >9 GB Memory Free, CPU Offloading Disabled.\n","OFFLOAD_CPU: False (Default is True for < 9GB GPU Memory Free)\n","USE_SMALL_MODELS: False (Default is False)\n","GLOBAL_ENABLE_MPS (Apple): False (Default is False)\n","SUNO_HALF_PRECISION: False (Default is False)\n","SUNO_HALF_BFLOAT16: False (Default is False)\n","SUNO_DISABLE_COMPILE: False (Default is False)\n","SUNO_USE_DIRECTML (AMD): False (Default is False)\n","Torch Num CPU Threads: 1\n","Bark Model Location: /root/.cache/suno/bark_v0 (Env var 'XDG_CACHE_HOME' to override)\n","HF_HOME: /content/bark/bark_infinity/data/models/unclassified\n","\n","FFmpeg status, this should say version 6.0\n","FFmpeg binaries directory: None\n","FFmpeg Version: None\n","FFmpeg Path: /root/.local/share/ffmpeg-downloader/ffmpeg/ffmpeg\n","FFprobe Path: /root/.local/share/ffmpeg-downloader/ffmpeg/ffprobe\n","FFplay Path: /root/.local/share/ffmpeg-downloader/ffmpeg/ffplay\n","\n","Traceback (most recent call last):\n","  File \"/content/bark/bark_webui.py\", line 1520, in <module>\n","    audio_prompt_input = gr.Audio(\n","  File \"/usr/local/lib/python3.10/dist-packages/gradio/component_meta.py\", line 152, in wrapper\n","    return fn(self, **kwargs)\n","TypeError: Audio.__init__() got an unexpected keyword argument 'info'\n"]}],"source":["#@markdown Run the WebUI with all features.<br>\n","#@markdown When loaded click the second link to launch WebUI in another window.\n","!python bark_webui.py --share"]},{"cell_type":"markdown","metadata":{"id":"nPlQyNOS-jNf"},"source":["## TRAIN-VAL"]},{"cell_type":"markdown","metadata":{"id":"LKLe_gYkQ59l"},"source":["### hubert model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsEiB211L-G6"},"outputs":[],"source":["\n","from bark.generation import _load_codec_model, generate_text_semantic\n","from encodec.utils import convert_audio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VBfHNXKJMYq2"},"outputs":[],"source":["import json\n","import os.path\n","from zipfile import ZipFile\n","\n","import numpy\n","import torch\n","from torch import nn, optim\n","from torch.serialization import MAP_LOCATION\n","\n","\n","class CustomTokenizer(nn.Module):\n","    def __init__(self, hidden_size=1024, input_size=768, output_size=10000, version=0):\n","        super(CustomTokenizer, self).__init__()\n","        next_size = input_size\n","        if version == 0:\n","            self.lstm = nn.LSTM(input_size, hidden_size, 2, batch_first=True)\n","            next_size = hidden_size\n","        if version == 1:\n","            self.lstm = nn.LSTM(input_size, hidden_size, 2, batch_first=True)\n","            self.intermediate = nn.Linear(hidden_size, 4096)\n","            next_size = 4096\n","\n","        self.fc = nn.Linear(next_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","        self.optimizer: optim.Optimizer = None\n","        self.lossfunc = nn.CrossEntropyLoss()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.version = version\n","\n","    def forward(self, x):\n","        x, _ = self.lstm(x)\n","        if self.version == 1:\n","            x = self.intermediate(x)\n","        x = self.fc(x)\n","        x = self.softmax(x)\n","        return x\n","\n","    @torch.no_grad()\n","    def get_token(self, x):\n","        \"\"\"\n","        Used to get the token for the first\n","        :param x: An array with shape (N, input_size) where N is a whole number greater or equal to 1, and input_size is the input size used when creating the model.\n","        :return: An array with shape (N,) where N is the same as N from the input. Every number in the array is a whole number in range 0...output_size - 1 where output_size is the output size used when creating the model.\n","        \"\"\"\n","        return torch.argmax(self(x), dim=1)\n","\n","    def prepare_training(self):\n","        self.optimizer = optim.Adam(self.parameters(), 0.001)\n","\n","    def train_step(self, x_train, y_train, log_loss=False):\n","        # y_train = y_train[:-1]\n","        # y_train = y_train[1:]\n","\n","        optimizer = self.optimizer\n","        lossfunc = self.lossfunc\n","        # Zero the gradients\n","        self.zero_grad()\n","\n","        # Forward pass\n","        y_pred = self(x_train)\n","\n","        y_train_len = len(y_train)\n","        y_pred_len = y_pred.shape[0]\n","\n","        if y_train_len > y_pred_len:\n","            diff = y_train_len - y_pred_len\n","            y_train = y_train[diff:]\n","        elif y_train_len < y_pred_len:\n","            diff = y_pred_len - y_train_len\n","            y_pred = y_pred[:-diff, :]\n","\n","        y_train_hot = torch.zeros(len(y_train), self.output_size)\n","        y_train_hot[range(len(y_train)), y_train] = 1\n","        y_train_hot = y_train_hot.to('cuda')\n","\n","        # Calculate the loss\n","        loss = lossfunc(y_pred, y_train_hot)\n","\n","        # Print loss\n","        if log_loss:\n","            print('Loss', loss.item())\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update the weights\n","        optimizer.step()\n","\n","    def save(self, path):\n","        info_path = '.'.join(os.path.basename(path).split('.')[:-1]) + '/.info'\n","        torch.save(self.state_dict(), path)\n","        data_from_model = Data(self.input_size, self.hidden_size, self.output_size, self.version)\n","        with ZipFile(path, 'a') as model_zip:\n","            model_zip.writestr(info_path, data_from_model.save())\n","            model_zip.close()\n","\n","    @staticmethod\n","    def load_from_checkpoint(path, map_location: MAP_LOCATION = None):\n","        old = True\n","        with ZipFile(path) as model_zip:\n","            filesMatch = [file for file in model_zip.namelist() if file.endswith('/.info')]\n","            file = filesMatch[0] if filesMatch else None\n","            if file:\n","                old = False\n","                data_from_model = Data.load(model_zip.read(file).decode('utf-8'))\n","            model_zip.close()\n","        if old:\n","            model = CustomTokenizer()\n","        else:\n","            model = CustomTokenizer(data_from_model.hidden_size, data_from_model.input_size, data_from_model.output_size, data_from_model.version)\n","        model.load_state_dict(torch.load(path))\n","        if map_location:\n","            model = model.to(map_location)\n","        return model\n","\n","\n","\n","class Data:\n","    input_size: int\n","    hidden_size: int\n","    output_size: int\n","    version: int\n","\n","    def __init__(self, input_size=768, hidden_size=1024, output_size=10000, version=0):\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.version = version\n","\n","    @staticmethod\n","    def load(string):\n","        data = json.loads(string)\n","        return Data(data['input_size'], data['hidden_size'], data['output_size'], data['version'])\n","\n","    def save(self):\n","        data = {\n","            'input_size': self.input_size,\n","            'hidden_size': self.hidden_size,\n","            'output_size': self.output_size,\n","            'version': self.version,\n","        }\n","        return json.dumps(data)\n","\n","\n","def auto_train(data_path, save_path='model.pth', load_model: str | None = None, save_epochs=1):\n","    data_x, data_y = [], []\n","\n","    if load_model and os.path.isfile(load_model):\n","        print('Loading model from', load_model)\n","        model_training = CustomTokenizer.load_from_checkpoint(load_model, 'cuda')\n","    else:\n","        print('Creating new model.')\n","        model_training = CustomTokenizer(version=1).to('cuda')  # Settings for the model to run without lstm\n","    save_path = os.path.join(data_path, save_path)\n","    base_save_path = '.'.join(save_path.split('.')[:-1])\n","\n","    sem_string = '_semantic.npy'\n","    feat_string = '_semantic_features.npy'\n","\n","    ready = os.path.join(data_path, 'ready')\n","    for input_file in os.listdir(ready):\n","        full_path = os.path.join(ready, input_file)\n","        if input_file.endswith(sem_string):\n","            data_y.append(numpy.load(full_path))\n","        elif input_file.endswith(feat_string):\n","            data_x.append(numpy.load(full_path))\n","    model_training.prepare_training()\n","\n","    epoch = 1\n","\n","    while 1:\n","        for i in range(save_epochs):\n","            j = 0\n","            for x, y in zip(data_x, data_y):\n","                model_training.train_step(torch.tensor(x).to('cuda'), torch.tensor(y).to('cuda'), j % 50 == 0)  # Print loss every 50 steps\n","                j += 1\n","        save_p = save_path\n","        save_p_2 = f'{base_save_path}_epoch_{epoch}.pth'\n","        model_training.save(save_p)\n","        model_training.save(save_p_2)\n","        print(f'Epoch {epoch} completed')\n","        epoch += 1\n","\n","import os.path\n","import shutil\n","import urllib.request\n","\n","import huggingface_hub\n","\n","\n","class HuBERTManager:\n","    @staticmethod\n","    def make_sure_hubert_installed(download_url: str = 'https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt', file_name: str = 'hubert.pt'):\n","        install_dir = os.path.join('data', 'models', 'hubert')\n","        if not os.path.isdir(install_dir):\n","            os.makedirs(install_dir, exist_ok=True)\n","        install_file = os.path.join(install_dir, file_name)\n","        if not os.path.isfile(install_file):\n","            print('Downloading HuBERT base model')\n","            urllib.request.urlretrieve(download_url, install_file)\n","            print('Downloaded HuBERT')\n","        return install_file\n","\n","\n","    @staticmethod\n","    def make_sure_tokenizer_installed(model: str = 'quantifier_hubert_base_ls960_14.pth', repo: str = 'GitMylo/bark-voice-cloning', local_file: str = 'tokenizer.pth'):\n","        install_dir = os.path.join('data', 'models', 'hubert')\n","        if not os.path.isdir(install_dir):\n","            os.makedirs(install_dir, exist_ok=True)\n","        install_file = os.path.join(install_dir, local_file)\n","        if not os.path.isfile(install_file):\n","            print('Downloading HuBERT custom tokenizer')\n","            huggingface_hub.hf_hub_download(repo, model, local_dir=install_dir, local_dir_use_symlinks=False)\n","            shutil.move(os.path.join(install_dir, model), install_file)\n","            print('Downloaded tokenizer')\n","        return install_file\n","\n","from pathlib import Path\n","\n","import torch\n","from torch import nn\n","from einops import pack, unpack\n","\n","import fairseq\n","\n","from torchaudio.functional import resample\n","\n","from audiolm_pytorch.utils import curtail_to_multiple\n","\n","import logging\n","logging.root.setLevel(logging.ERROR)\n","\n","\n","def exists(val):\n","    return val is not None\n","\n","\n","def default(val, d):\n","    return val if exists(val) else d\n","\n","\n","class CustomHubert(nn.Module):\n","    \"\"\"\n","    checkpoint and kmeans can be downloaded at https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n","    or you can train your own\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        checkpoint_path,\n","        target_sample_hz=16000,\n","        seq_len_multiple_of=None,\n","        output_layer=9,\n","        device=None\n","    ):\n","        super().__init__()\n","        self.target_sample_hz = target_sample_hz\n","        self.seq_len_multiple_of = seq_len_multiple_of\n","        self.output_layer = output_layer\n","\n","        if device is not None:\n","            self.to(device)\n","\n","        model_path = Path(checkpoint_path)\n","\n","        assert model_path.exists(), f'path {checkpoint_path} does not exist'\n","\n","        checkpoint = torch.load(checkpoint_path)\n","        load_model_input = {checkpoint_path: checkpoint}\n","        model, *_ = fairseq.checkpoint_utils.load_model_ensemble_and_task(load_model_input)\n","\n","        if device is not None:\n","            model[0].to(device)\n","\n","        self.model = model[0]\n","        self.model.eval()\n","\n","    @property\n","    def groups(self):\n","        return 1\n","\n","    @torch.no_grad()\n","    def forward(\n","        self,\n","        wav_input,\n","        flatten=True,\n","        input_sample_hz=None\n","    ):\n","        device = wav_input.device\n","\n","        if exists(input_sample_hz):\n","            wav_input = resample(wav_input, input_sample_hz, self.target_sample_hz)\n","\n","        if exists(self.seq_len_multiple_of):\n","            wav_input = curtail_to_multiple(wav_input, self.seq_len_multiple_of)\n","\n","        embed = self.model(\n","            wav_input,\n","            features_only=True,\n","            mask=False,  # thanks to @maitycyrus for noticing that mask is defaulted to True in the fairseq code\n","            output_layer=self.output_layer\n","        )\n","\n","        embed, packed_shape = pack([embed['x']], '* d')\n","\n","        # codebook_indices = self.kmeans.predict(embed.cpu().detach().numpy())\n","\n","        codebook_indices = torch.from_numpy(embed.cpu().detach().numpy()).to(device)  # .long()\n","\n","        if flatten:\n","            return codebook_indices\n","\n","        codebook_indices, = unpack(codebook_indices, packed_shape, '*')\n","        return codebook_indices\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g7x4hpdZ1U8-"},"source":["### import things"]},{"cell_type":"code","source":[],"metadata":{"id":"ZDuVmUvNLuBt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-KEiFN9Di65","outputId":"2a98b1c2-039c-4a27-9905-fa8e72797d53","executionInfo":{"status":"ok","timestamp":1701593817959,"user_tz":-540,"elapsed":2,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/bark/bark\n"]}],"source":["\n","import torch\n","import torch.nn as nn\n","import os\n","import re\n","import gc\n","import json\n","import math\n","import hashlib\n","import numpy as np\n","import logging\n","import torchaudio\n","from tqdm.auto import tqdm\n","import torch.nn.functional as F\n","from encodec.utils import convert_audio\n","from accelerate import Accelerator\n","from accelerate.utils import set_seed\n","from transformers import BertTokenizer\n","from huggingface_hub import hf_hub_download\n","from packaging import version\n","from diffusers.optimization import get_scheduler\n","%cd /content/bark/bark\n","from model import GPTConfig, GPT\n","from model_fine import FineGPT, FineGPTConfig #model 앞 . 없애"]},{"cell_type":"markdown","metadata":{"id":"1JkPRFa4IfUD"},"source":["### 없는 모듈 설치 및 드라이브 접근"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x8VRKfbI8znI","outputId":"d706213f-d64f-4f37-cfed-572d651e17d1","executionInfo":{"status":"ok","timestamp":1701593837208,"user_tz":-540,"elapsed":16420,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tbrJr9DWHM2L","outputId":"7bdf2096-3085-47e7-ee7f-7766d56a7b59","executionInfo":{"status":"ok","timestamp":1701593837208,"user_tz":-540,"elapsed":3,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["%cd /content\n","\n","import warnings\n","import sys\n","import importlib.util\n","from copy import deepcopy\n","import copy\n","import json\n","import os\n","from dataclasses import dataclass\n","\n","from typing import Any, Tuple, Union, Dict\n","\n","from packaging import version\n","\n","if sys.version_info < (3, 8):\n","    import importlib_metadata\n","else:\n","    import importlib.metadata as importlib_metadata\n","\n","\n","def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[Tuple[bool, str], bool]:\n","    # Check we're not importing a \"pkg_name\" directory somewhere but the actual library by trying to grab the version\n","    package_exists = importlib.util.find_spec(pkg_name) is not None\n","    package_version = \"N/A\"\n","    if package_exists:\n","        try:\n","            package_version = importlib_metadata.version(pkg_name)\n","            package_exists = True\n","        except importlib_metadata.PackageNotFoundError:\n","            package_exists = False\n","    if return_version:\n","        return package_exists, package_version\n","    else:\n","        return package_exists\n","\n","_accelerate_available, _accelerate_version = _is_package_available(\"accelerate\", return_version=True)\n","_bitsandbytes_available = _is_package_available(\"bitsandbytes\")\n","_torch_available, _torch_version = _is_package_available(\"torch\", return_version=True)\n","\n","def is_accelerate_available(check_partial_state=False):\n","    if check_partial_state:\n","        return _accelerate_available and version.parse(_accelerate_version) >= version.parse(\"0.19.0\")\n","    return _accelerate_available\n","\n","def is_bitsandbytes_available():\n","    return _bitsandbytes_available\n","\n","def is_torch_available():\n","    return _torch_available\n","\n","if is_bitsandbytes_available():\n","    import bitsandbytes as bnb\n","    import torch\n","    import torch.nn as nn\n","\n","if is_accelerate_available():\n","    from accelerate import init_empty_weights\n","    from accelerate.utils import find_tied_parameters\n","\n","\n","def set_module_quantized_tensor_to_device(module, tensor_name, device, value=None, fp16_statistics=None):\n","    \"\"\"\n","    A helper function to set a given tensor (parameter of buffer) of a module on a specific device (note that doing\n","    `param.to(device)` creates a new tensor not linked to the parameter, which is why we need this function). The\n","    function is adapted from `set_module_tensor_to_device` function from accelerate that is adapted to support the\n","    class `Int8Params` from `bitsandbytes`.\n","\n","    Args:\n","        module (`torch.nn.Module`):\n","            The module in which the tensor we want to move lives.\n","        tensor_name (`str`):\n","            The full name of the parameter/buffer.\n","        device (`int`, `str` or `torch.device`):\n","            The device on which to set the tensor.\n","        value (`torch.Tensor`, *optional*):\n","            The value of the tensor (useful when going from the meta device to any other device).\n","        fp16_statistics (`torch.HalfTensor`, *optional*):\n","            The list of fp16 statistics to set on the module, used for serialization.\n","    \"\"\"\n","    # Recurse if needed\n","    if \".\" in tensor_name:\n","        splits = tensor_name.split(\".\")\n","        for split in splits[:-1]:\n","            new_module = getattr(module, split)\n","            if new_module is None:\n","                raise ValueError(f\"{module} has no attribute {split}.\")\n","            module = new_module\n","        tensor_name = splits[-1]\n","\n","    if tensor_name not in module._parameters and tensor_name not in module._buffers:\n","        raise ValueError(f\"{module} does not have a parameter or a buffer named {tensor_name}.\")\n","    is_buffer = tensor_name in module._buffers\n","    old_value = getattr(module, tensor_name)\n","\n","    if old_value.device == torch.device(\"meta\") and device not in [\"meta\", torch.device(\"meta\")] and value is None:\n","        raise ValueError(f\"{tensor_name} is on the meta device, we need a `value` to put in on {device}.\")\n","\n","    is_4bit = False\n","    is_8bit = False\n","    if is_buffer or not is_bitsandbytes_available():\n","        is_8bit = False\n","        is_4bit = False\n","    else:\n","        is_4bit = hasattr(bnb.nn, \"Params4bit\") and isinstance(module._parameters[tensor_name], bnb.nn.Params4bit)\n","        is_8bit = isinstance(module._parameters[tensor_name], bnb.nn.Int8Params)\n","\n","    if is_8bit or is_4bit:\n","        param = module._parameters[tensor_name]\n","        if param.device.type != \"cuda\":\n","            if value is None:\n","                new_value = old_value.to(device)\n","            elif isinstance(value, torch.Tensor):\n","                new_value = value.to(\"cpu\")\n","                if value.dtype == torch.int8:\n","                    is_8bit_serializable = version.parse(importlib_metadata.version(\"bitsandbytes\")) > version.parse(\n","                        \"0.37.2\"\n","                    )\n","                    if not is_8bit_serializable:\n","                        raise ValueError(\n","                            \"Detected int8 weights but the version of bitsandbytes is not compatible with int8 serialization. \"\n","                            \"Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`.\"\n","                        )\n","            else:\n","                new_value = torch.tensor(value, device=\"cpu\")\n","\n","            kwargs = old_value.__dict__\n","            if is_8bit:\n","                new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(device)\n","            elif is_4bit:\n","                new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(device)\n","\n","            module._parameters[tensor_name] = new_value\n","            if fp16_statistics is not None:\n","                setattr(module.weight, \"SCB\", fp16_statistics.to(device))\n","\n","    else:\n","        if value is None:\n","            new_value = old_value.to(device)\n","        elif isinstance(value, torch.Tensor):\n","            new_value = value.to(device)\n","        else:\n","            new_value = torch.tensor(value, device=device)\n","\n","        if is_buffer:\n","            module._buffers[tensor_name] = new_value\n","        else:\n","            new_value = nn.Parameter(new_value, requires_grad=old_value.requires_grad)\n","            module._parameters[tensor_name] = new_value\n","\n","\n","def replace_with_bnb_linear(model, modules_to_not_convert=None, current_key_name=None, quantization_config=None):\n","    \"\"\"\n","    A helper function to replace all `torch.nn.Linear` modules by `bnb.nn.Linear8bit` modules from the `bitsandbytes`\n","    library. This will enable running your models using mixed int8 precision as described by the paper `LLM.int8():\n","    8-bit Matrix Multiplication for Transformers at Scale`. Make sure `bitsandbytes` compiled with the correct CUDA\n","    version of your hardware is installed before running this function. `pip install -i https://test.pypi.org/simple/\n","    bitsandbytes`\n","\n","    The function will be run recursively and replace all `torch.nn.Linear` modules except for the `lm_head` that should\n","    be kept as a `torch.nn.Linear` module. The replacement is done under `init_empty_weights` context manager so no\n","    CPU/GPU memory is required to run this function. Int8 mixed-precision matrix decomposition works by separating a\n","    matrix multiplication into two streams: (1) and systematic feature outlier stream matrix multiplied in fp16\n","    (0.01%), (2) a regular stream of int8 matrix multiplication (99.9%). With this method, int8 inference with no\n","    predictive degradation is possible for very large models (>=176B parameters).\n","\n","    Parameters:\n","        model (`torch.nn.Module`):\n","            Input model or `torch.nn.Module` as the function is run recursively.\n","        modules_to_not_convert (`List[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n","            Names of the modules to not convert in `Linear8bitLt`. In practice we keep the `lm_head` in full precision\n","            for numerical stability reasons.\n","        current_key_name (`List[`str`]`, *optional*):\n","            An array to track the current key of the recursion. This is used to check whether the current key (part of\n","            it) is not in the list of modules to not convert (for instances modules that are offloaded to `cpu` or\n","            `disk`).\n","    \"\"\"\n","    modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n","    for name, module in model.named_children():\n","        if current_key_name is None:\n","            current_key_name = []\n","\n","        if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n","            # Check if the current key is not in the `modules_to_not_convert`\n","            if not any(key in \".\".join(current_key_name) for key in modules_to_not_convert):\n","                with init_empty_weights():\n","                    if quantization_config.quantization_method() == \"llm_int8\":\n","                        model._modules[name] = bnb.nn.Linear8bitLt(\n","                            module.in_features,\n","                            module.out_features,\n","                            module.bias is not None,\n","                            has_fp16_weights=quantization_config.llm_int8_has_fp16_weight,\n","                            threshold=quantization_config.llm_int8_threshold,\n","                        )\n","                    else:\n","                        if (\n","                            quantization_config.llm_int8_skip_modules is not None\n","                            and name in quantization_config.llm_int8_skip_modules\n","                        ):\n","                            pass\n","                        else:\n","                            model._modules[name] = bnb.nn.Linear4bit(\n","                                module.in_features,\n","                                module.out_features,\n","                                module.bias is not None,\n","                                quantization_config.bnb_4bit_compute_dtype,\n","                                compress_statistics=quantization_config.bnb_4bit_use_double_quant,\n","                                quant_type=quantization_config.bnb_4bit_quant_type,\n","                            )\n","                    # Force requires grad to False to avoid unexpected errors\n","                    model._modules[name].requires_grad_(False)\n","        # Remove the last key for recursion\n","        if len(list(module.children())) > 0:\n","            replace_with_bnb_linear(\n","                module,\n","                modules_to_not_convert,\n","                current_key_name,\n","                quantization_config,\n","            )\n","    return model\n","\n","\n","# For backward compatibility\n","def replace_8bit_linear(*args, **kwargs):\n","    warnings.warn(\n","        \"`replace_8bit_linear` will be deprecated in a future version, please use `replace_with_bnb_linear` instead\",\n","        FutureWarning,\n","    )\n","    return replace_with_bnb_linear(*args, **kwargs)\n","\n","\n","# For backward compatiblity\n","def set_module_8bit_tensor_to_device(*args, **kwargs):\n","    warnings.warn(\n","        \"`set_module_8bit_tensor_to_device` will be deprecated in a future version, please use `set_module_quantized_tensor_to_device` instead\",\n","        FutureWarning,\n","    )\n","    return set_module_quantized_tensor_to_device(*args, **kwargs)\n","\n","\n","def get_keys_to_not_convert(model):\n","    r\"\"\"\n","    An utility function to get the key of the module to keep in full precision if any For example for CausalLM modules\n","    we may want to keep the lm_head in full precision for numerical stability reasons. For other architectures, we want\n","    to keep the tied weights of the model. The function will return a list of the keys of the modules to not convert in\n","    int8.\n","\n","    Parameters:\n","    model (`torch.nn.Module`):\n","        Input model\n","    \"\"\"\n","    # Create a copy of the model and tie the weights, then\n","    # check if it contains tied weights\n","    tied_model = deepcopy(model)  # this has 0 cost since it is done inside `init_empty_weights` context manager`\n","    tied_model.tie_weights()\n","\n","    tied_params = find_tied_parameters(tied_model)\n","    # For compatibility with Accelerate < 0.18\n","    if isinstance(tied_params, dict):\n","        tied_keys = list(tied_params.values())\n","    else:\n","        tied_keys = sum([x[1:] for x in tied_params], [])\n","    has_tied_params = len(tied_keys) > 0\n","\n","    # Check if it is a base model\n","    is_base_model = not hasattr(model, model.base_model_prefix)\n","\n","    # Ignore this for base models (BertModel, GPT2Model, etc.)\n","    if (not has_tied_params) and is_base_model:\n","        return []\n","\n","    # otherwise they have an attached head\n","    list_modules = list(model.named_parameters())\n","    list_last_module = [list_modules[-1][0]]\n","\n","    # add last module together with tied weights\n","    intersection = set(list_last_module) - set(tied_keys)\n","    list_untouched = tied_keys + list(intersection)\n","\n","    # remove \".weight\" from the keys\n","    names_to_remove = [\".weight\", \".bias\"]\n","    filtered_module_names = []\n","    for name in list_untouched:\n","        for name_to_remove in names_to_remove:\n","            if name_to_remove in name:\n","                name = name.replace(name_to_remove, \"\")\n","        filtered_module_names.append(name)\n","\n","    return filtered_module_names\n","\n","#!/usr/bin/env python\n","# coding=utf-8\n","\n","# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","\n","\n","if is_torch_available():\n","    import torch\n","\n","\n","@dataclass\n","class BitsAndBytesConfig:\n","    \"\"\"\n","    This is a wrapper class about all possible attributes and features that you can play with a model that has been\n","    loaded using `bitsandbytes`.\n","\n","    This replaces `load_in_8bit` or `load_in_4bit`therefore both options are mutually exclusive.\n","\n","    Currently only supports `LLM.int8()`, `FP4`, and `NF4` quantization. If more methods are added to `bitsandbytes`,\n","    then more arguments will be added to this class.\n","\n","    Args:\n","        load_in_8bit (`bool`, *optional*, defaults to `False`):\n","            This flag is used to enable 8-bit quantization with LLM.int8().\n","        load_in_4bit (`bool`, *optional*, defaults to `False`):\n","            This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from\n","            `bitsandbytes`.\n","        llm_int8_threshold (`float`, *optional*, defaults to 6):\n","            This corresponds to the outlier threshold for outlier detection as described in `LLM.int8() : 8-bit Matrix\n","            Multiplication for Transformers at Scale` paper: https://arxiv.org/abs/2208.07339 Any hidden states value\n","            that is above this threshold will be considered an outlier and the operation on those values will be done\n","            in fp16. Values are usually normally distributed, that is, most values are in the range [-3.5, 3.5], but\n","            there are some exceptional systematic outliers that are very differently distributed for large models.\n","            These outliers are often in the interval [-60, -6] or [6, 60]. Int8 quantization works well for values of\n","            magnitude ~5, but beyond that, there is a significant performance penalty. A good default threshold is 6,\n","            but a lower threshold might be needed for more unstable models (small models, fine-tuning).\n","        llm_int8_skip_modules (`List[str]`, *optional*):\n","            An explicit list of the modules that we do not want to convert in 8-bit. This is useful for models such as\n","            Jukebox that has several heads in different places and not necessarily at the last position. For example\n","            for `CausalLM` models, the last `lm_head` is kept in its original `dtype`.\n","        llm_int8_enable_fp32_cpu_offload (`bool`, *optional*, defaults to `False`):\n","            This flag is used for advanced use cases and users that are aware of this feature. If you want to split\n","            your model in different parts and run some parts in int8 on GPU and some parts in fp32 on CPU, you can use\n","            this flag. This is useful for offloading large models such as `google/flan-t5-xxl`. Note that the int8\n","            operations will not be run on CPU.\n","        llm_int8_has_fp16_weight (`bool`, *optional*, defaults to `False`):\n","            This flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning as the weights do not\n","            have to be converted back and forth for the backward pass.\n","        bnb_4bit_compute_dtype (`torch.dtype` or str, *optional*, defaults to `torch.float32`):\n","            This sets the computational type which might be different than the input time. For example, inputs might be\n","            fp32, but computation can be set to bf16 for speedups.\n","        bnb_4bit_quant_type (`str`, {fp4, fn4}, defaults to `fp4`):\n","            This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and NF4 data types\n","            which are specified by `fp4` or `fn4`.\n","        bnb_4bit_use_double_quant (`bool`, *optional*, defaults to `False`):\n","            This flag is used for nested quantization where the quantization constants from the first quantization are\n","            quantized again.\n","        kwargs (`Dict[str, Any]`, *optional*):\n","            Additional parameters from which to initialize the configuration object.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        load_in_8bit=False,\n","        load_in_4bit=False,\n","        llm_int8_threshold=6.0,\n","        llm_int8_skip_modules=None,\n","        llm_int8_enable_fp32_cpu_offload=False,\n","        llm_int8_has_fp16_weight=False,\n","        bnb_4bit_compute_dtype=None,\n","        bnb_4bit_quant_type=\"fp4\",\n","        bnb_4bit_use_double_quant=False,\n","        **kwargs,\n","    ):\n","        self.load_in_8bit = load_in_8bit\n","        self.load_in_4bit = load_in_4bit\n","        self.llm_int8_threshold = llm_int8_threshold\n","        self.llm_int8_skip_modules = llm_int8_skip_modules\n","        self.llm_int8_enable_fp32_cpu_offload = llm_int8_enable_fp32_cpu_offload\n","        self.llm_int8_has_fp16_weight = llm_int8_has_fp16_weight\n","        self.bnb_4bit_quant_type = bnb_4bit_quant_type\n","        self.bnb_4bit_use_double_quant = bnb_4bit_use_double_quant\n","\n","        if bnb_4bit_compute_dtype is None:\n","            self.bnb_4bit_compute_dtype = torch.float32\n","        elif isinstance(bnb_4bit_compute_dtype, str):\n","            self.bnb_4bit_compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","        elif isinstance(bnb_4bit_compute_dtype, torch.dtype):\n","            self.bnb_4bit_compute_dtype = bnb_4bit_compute_dtype\n","        else:\n","            raise ValueError(\"bnb_4bit_compute_dtype must be a string or a torch.dtype\")\n","\n","        self.post_init()\n","\n","    def post_init(self):\n","        r\"\"\"\n","        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n","        \"\"\"\n","        if not isinstance(self.llm_int8_threshold, float):\n","            raise ValueError(\"llm_int8_threshold must be a float\")\n","\n","        if self.llm_int8_skip_modules is not None and not isinstance(self.llm_int8_skip_modules, list):\n","            raise ValueError(\"llm_int8_skip_modules must be a list of strings\")\n","        if not isinstance(self.llm_int8_enable_fp32_cpu_offload, bool):\n","            raise ValueError(\"llm_int8_enable_fp32_cpu_offload must be a boolean\")\n","\n","        if not isinstance(self.llm_int8_has_fp16_weight, bool):\n","            raise ValueError(\"llm_int8_has_fp16_weight must be a boolean\")\n","\n","        if self.bnb_4bit_compute_dtype is not None and not isinstance(self.bnb_4bit_compute_dtype, torch.dtype):\n","            raise ValueError(\"bnb_4bit_compute_dtype must be torch.dtype\")\n","\n","        if not isinstance(self.bnb_4bit_quant_type, str):\n","            raise ValueError(\"bnb_4bit_quant_type must be a string\")\n","\n","        if not isinstance(self.bnb_4bit_use_double_quant, bool):\n","            raise ValueError(\"bnb_4bit_use_double_quant must be a boolean\")\n","\n","        if self.load_in_4bit and not version.parse(importlib_metadata.version(\"bitsandbytes\")) >= version.parse(\n","            \"0.39.0\"\n","        ):\n","            raise ValueError(\n","                \"4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\"\n","            )\n","\n","    def is_quantizable(self):\n","        r\"\"\"\n","        Returns `True` if the model is quantizable, `False` otherwise.\n","        \"\"\"\n","        return self.load_in_8bit or self.load_in_4bit\n","\n","    def quantization_method(self):\n","        r\"\"\"\n","        This method returns the quantization method used for the model. If the model is not quantizable, it returns\n","        `None`.\n","        \"\"\"\n","        if self.load_in_8bit:\n","            return \"llm_int8\"\n","        elif self.load_in_4bit and self.bnb_4bit_quant_type == \"fp4\":\n","            return \"fp4\"\n","        elif self.load_in_4bit and self.bnb_4bit_quant_type == \"nf4\":\n","            return \"nf4\"\n","        else:\n","            return None\n","\n","    @classmethod\n","    def from_dict(cls, config_dict, return_unused_kwargs, **kwargs):\n","        \"\"\"\n","        Instantiates a [`BitsAndBytesConfig`] from a Python dictionary of parameters.\n","\n","        Args:\n","            config_dict (`Dict[str, Any]`):\n","                Dictionary that will be used to instantiate the configuration object.\n","            return_unused_kwargs (`bool`):\n","                Whether or not to return a list of unused keyword arguments. Used for `from_pretrained` method in\n","                `PreTrainedModel`.\n","            kwargs (`Dict[str, Any]`):\n","                Additional parameters from which to initialize the configuration object.\n","\n","        Returns:\n","            [`BitsAndBytesConfig`]: The configuration object instantiated from those parameters.\n","        \"\"\"\n","\n","        config = cls(**config_dict)\n","\n","        to_remove = []\n","        for key, value in kwargs.items():\n","            if hasattr(config, key):\n","                setattr(config, key, value)\n","                to_remove.append(key)\n","        for key in to_remove:\n","            kwargs.pop(key, None)\n","\n","        if return_unused_kwargs:\n","            return config, kwargs\n","        else:\n","            return config\n","\n","    def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n","        \"\"\"\n","        Save this instance to a JSON file.\n","\n","        Args:\n","            json_file_path (`str` or `os.PathLike`):\n","                Path to the JSON file in which this configuration instance's parameters will be saved.\n","            use_diff (`bool`, *optional*, defaults to `True`):\n","                If set to `True`, only the difference between the config instance and the default\n","                `BitsAndBytesConfig()` is serialized to JSON file.\n","        \"\"\"\n","        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n","            config_dict = self.to_dict()\n","            json_string = json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n","\n","            writer.write(json_string)\n","\n","    def to_dict(self) -> Dict[str, Any]:\n","        \"\"\"\n","        Serializes this instance to a Python dictionary. Returns:\n","            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n","        \"\"\"\n","\n","        output = copy.deepcopy(self.__dict__)\n","        output[\"bnb_4bit_compute_dtype\"] = str(output[\"bnb_4bit_compute_dtype\"]).split(\".\")[1]\n","\n","        return output\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niL_JcuZIaHY"},"outputs":[],"source":["# Adapted from https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/utils/module/lora.py\n","\n","import math\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","class LinearLayer_LoRA(nn.Module):\n","    # a simple implementation of LoRA\n","    def __init__(self,\n","                 weight,\n","                 lora_dim=0,\n","                 lora_scaling=1,\n","                 lora_dropout=0,\n","                 bias=None):\n","        super(LinearLayer_LoRA, self).__init__()\n","        self.weight = weight\n","        self.bias = bias\n","\n","        if lora_dim <= 0:\n","            raise ValueError(\n","                \"You are training to use LoRA, whose reduced dim should be larger than 1\"\n","            )\n","\n","        rows, columns = weight.shape\n","        self.lora_right_weight = nn.Parameter(torch.zeros(\n","            columns,\n","            lora_dim))  # apply transpose so in forward we do not need to\n","        self.lora_left_weight = nn.Parameter(torch.zeros(lora_dim, rows))\n","        self.lora_scaling = lora_scaling / lora_dim\n","\n","        if lora_dropout > 0:\n","            self.lora_dropout = nn.Dropout(lora_dropout)\n","        else:\n","            self.lora_dropout = nn.Identity()\n","\n","        self.reset_parameters()\n","        # disable the original weight gradient\n","        self.weight.requires_grad = False\n","        # fuse LoRA to the original weight\n","        self.fuse_lora = False\n","\n","    def eval(self):\n","        self.lora_dropout.eval()\n","\n","    #   self.fuse_lora_weight()\n","\n","    def train(self, mode=True):\n","        self.lora_dropout.train(mode)\n","        # self.unfuse_lora_weight()\n","\n","    def reset_parameters(self):\n","        nn.init.kaiming_uniform_(self.lora_right_weight, a=math.sqrt(5))\n","        nn.init.zeros_(self.lora_left_weight)\n","\n","    def fuse_lora_weight(self):\n","        if not self.fuse_lora:\n","            self.weight.data += self.lora_scaling * torch.matmul(\n","                self.lora_left_weight.t(), self.lora_right_weight.t())\n","        self.fuse_lora = True\n","\n","    def unfuse_lora_weight(self):\n","        if self.fuse_lora:\n","            self.weight.data -= self.lora_scaling * torch.matmul(\n","                self.lora_left_weight.t(), self.lora_right_weight.t())\n","        self.fuse_lora = False\n","\n","    def forward(self, input):\n","        if self.fuse_lora:\n","            return F.linear(input, self.weight, self.bias)\n","        else:\n","            return F.linear(\n","                input, self.weight,\n","                self.bias) + (self.lora_dropout(input) @ self.lora_right_weight\n","                              @ self.lora_left_weight) * self.lora_scaling\n","\n","\n","def recursive_getattr(model, module_name):\n","    \"\"\"\n","    From https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/compression/helper.py\n","    Recursively get the attribute of a module.\n","    Args:\n","        model (`torch.nn.Module`)\n","            The model to get the attribute from.\n","        module_name (`str`)\n","            The name of the module to get the attribute from.\n","    \"\"\"\n","    split_list = module_name.split('.')\n","    output = model\n","    for name in split_list:\n","        output = getattr(output, name)\n","    return output\n","\n","\n","def recursive_setattr(model, module_name, module):\n","    \"\"\"\n","    From https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/compression/helper.py\n","    Recursively set the attribute of a module.\n","    Args:\n","        model (`torch.nn.Module`)\n","            The model to set the attribute in.\n","        module_name (`str`)\n","            The name of the module to set the attribute in.\n","        module (`torch.nn.Module`)\n","            The module to set the attribute to.\n","    \"\"\"\n","    split_list = module_name.split('.')\n","    output = model\n","    for name in split_list[:-1]:\n","        output = getattr(output, name)\n","    output.__setattr__(split_list[-1], module)\n","\n","\n","# convert the linear layer to LoRA\n","def convert_linear_layer_to_lora(model,\n","                                 part_module_name,\n","                                 lora_dim=0,\n","                                 lora_scaling=1,\n","                                 lora_dropout=0):\n","    replace_name = []\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear) and part_module_name in name:\n","            replace_name.append(name)\n","    for name in replace_name:\n","        module = recursive_getattr(model, name)\n","        tmp = LinearLayer_LoRA(\n","            module.weight, lora_dim, lora_scaling, lora_dropout,\n","            module.bias).to(module.weight.device).to(module.weight.dtype)\n","        recursive_setattr(model, name, tmp)\n","    return model\n","\n","\n","# convert the LoRA layer to linear layer\n","def convert_lora_to_linear_layer(model):\n","    replace_name = []\n","    for name, module in model.named_modules():\n","        if isinstance(module, LinearLayer_LoRA):\n","            replace_name.append(name)\n","    for name in replace_name:\n","        module = recursive_getattr(model, name)\n","        module.fuse_lora_weight()\n","    return model\n","\n","\n","def only_optimize_lora_parameters(model):\n","    # turn off the gradient of all the parameters except the LoRA parameters\n","    for name, param in model.named_parameters():\n","        if \"lora_right_weight\" in name or \"lora_left_weight\" in name:\n","            param.requires_grad = True\n","        else:\n","            param.requires_grad = False\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"tOkIRR1zIoi1"},"source":["### set up- fine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2UYlaS5I1so"},"outputs":[],"source":["train_batch_size = 2\n","eval_batch_size = 2\n","grad_accum = 2\n","ckpt_path = 'models/fine_2.pt'\n","model_type = \"fine\"\n","dataset_path = '/content/drive/MyDrive/datasets_yoo/' #만들어줘\n","logging_dir = 'logs/'\n","log_with = 'wandb'\n","hubert_path = 'data/models/hubert/hubert.pt'\n","hubert_tokenizer_path = 'data/models/hubert/tokenizer.pth'\n","\n","output_dir = '/content/drive/MyDrive/datasets_yoo/fine_output' #만들어줘\n","resume_from_checkpoint = None\n","\n","checkpointing_steps = 100\n","\n","mixed_precision = 'fp16'\n","bits = 16 #4 4 and 8 bit are a work in progress\n","compute_dtype = torch.float16\n","double_quant = True\n","quant_type = 'nf4'\n","\n","lora_dim = 64\n","lora_scaling = 1\n","lora_dropout = 0.1\n","lora_module_name = 'transformer.h'\n","optimize_lora_params_only = False\n","\n","learning_rate = 1e-4\n","scale_lr = False\n","use_8bit_adam = False\n","adam_beta1 = 0.9\n","adam_beta2 = 0.999\n","adam_epsilon = 1e-8\n","weight_decay = 0.01\n","\n","llm_int8_skip_modules = None\n","keep_in_fp32_modules = ['lm_head']\n","\n","lr_scheduler_type = 'linear'\n","lr_warmup_steps = 50\n","num_train_epochs = 300\n","max_train_steps = None\n","max_grad_norm = 1.0\n","\n","seed = 741"]},{"cell_type":"markdown","source":["임시"],"metadata":{"id":"qSpSZiulTmR7"}},{"cell_type":"code","source":["train_batch_size = 2\n","eval_batch_size = 2\n","grad_accum = 2\n","ckpt_path = 'models/fine_2.pt'\n","model_type = \"fine\"\n","dataset_path = '/content/drive/MyDrive/datasets_yoo/' #만들어줘\n","logging_dir = 'logs/'\n","log_with = 'wandb'\n","hubert_path = 'data/models/hubert/hubert.pt'\n","hubert_tokenizer_path = 'data/models/hubert/tokenizer.pth'\n","\n","output_dir = '/content/drive/MyDrive/datasets_yoo/fine_output' #만들어줘\n","resume_from_checkpoint = None\n","\n","checkpointing_steps = 100\n","\n","mixed_precision = 'fp16'\n","bits = 16 #4 4 and 8 bit are a work in progress\n","compute_dtype = torch.float16\n","double_quant = True\n","quant_type = 'nf4'\n","\n","lora_dim = 64\n","lora_scaling = 1\n","lora_dropout = 0.1\n","lora_module_name = 'transformer.h'\n","optimize_lora_params_only = False\n","\n","learning_rate = 1e-4\n","scale_lr = False\n","use_8bit_adam = False\n","adam_beta1 = 0.9\n","adam_beta2 = 0.999\n","adam_epsilon = 1e-8\n","weight_decay = 0.01\n","\n","llm_int8_skip_modules = None\n","keep_in_fp32_modules = ['lm_head']\n","\n","lr_scheduler_type = 'linear'\n","lr_warmup_steps = 10\n","num_train_epochs = 100\n","max_train_steps = None\n","max_grad_norm = 1.0\n","\n","seed = 741"],"metadata":{"id":"dRmjxhXDTlmP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZAYy1erMLFs"},"outputs":[],"source":["CONTEXT_WINDOW_SIZE = 1024 #모델이 한 번에 고려할 데이터의 최대 길이를 의미합니다.\n","\n","MAX_SEMANTIC_LEN = 256 #의미론적 분석을 위한 최대 길이를 256으로 설정합니다\n","\n","SEMANTIC_RATE_HZ = 49.9\n","SEMANTIC_VOCAB_SIZE = 10_000 #조절..?\n","\n","TEXT_ENCODING_OFFSET = 10_048\n","SEMANTIC_PAD_TOKEN = 10_000\n","TEXT_PAD_TOKEN = 129_595\n","SEMANTIC_INFER_TOKEN = 129_599\n","\n","MAX_COARSE_LEN = 768\n","\n","SAMPLE_RATE = 24_000\n","CHANNELS = 1\n","\n","COARSE_SEMANTIC_PAD_TOKEN = 12_048\n","COARSE_INFER_TOKEN = 12_050\n","\n","CODEBOOK_SIZE = 1024\n","N_COARSE_CODEBOOKS = 2\n","N_FINE_CODEBOOKS = 8\n","COARSE_RATE_HZ = 75\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","USE_SMALL_MODELS = os.environ.get(\"SERP_USE_SMALL_MODELS\", False)\n","\n","default_cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\n","CACHE_DIR = os.path.join(os.getenv(\"XDG_CACHE_HOME\", default_cache_dir), \"serp\", \"bark_v0\")\n","\n","\n","def _clear_cuda_cache():\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","        torch.cuda.synchronize()\n","\n","\n","def _md5(fname):\n","    hash_md5 = hashlib.md5()\n","    with open(fname, \"rb\") as f:\n","        for chunk in iter(lambda: f.read(4096), b\"\"):\n","            hash_md5.update(chunk)\n","    return hash_md5.hexdigest()\n","\n","\n","def _download(from_hf_path, file_name, to_local_path):\n","    to_local_path = to_local_path.replace(\"\\\\\", \"/\")\n","    path = '/'.join(to_local_path.split(\"/\")[:-1])\n","    os.makedirs(path, exist_ok=True)\n","    hf_hub_download(repo_id=from_hf_path, filename=file_name, local_dir=path)\n","    os.replace(os.path.join(path, file_name), to_local_path)\n","\n","\n","def _tokenize(tokenizer, text):\n","    return tokenizer.encode(text, add_special_tokens=False)\n","\n","\n","def _detokenize(tokenizer, enc_text):\n","    return tokenizer.decode(enc_text)\n","\n","\n","def _normalize_whitespace(text):\n","    return re.sub(r\"\\s+\", \" \", text).strip()\n","\n","\n","REMOTE_MODEL_PATHS = {\n","    \"text_small\": {\n","        \"repo_id\": \"suno/bark\",\n","        \"file_name\": \"text.pt\",\n","        \"checksum\": \"b3e42bcbab23b688355cd44128c4cdd3\",\n","    },\n","    \"coarse_small\": {\n","        \"repo_id\": \"suno/bark\",\n","        \"file_name\": \"coarse.pt\",\n","        \"checksum\": \"5fe964825e3b0321f9d5f3857b89194d\",\n","    },\n","    \"fine_small\": {\n","        \"repo_id\": \"suno/bark\",\n","        \"file_name\": \"fine.pt\",\n","        \"checksum\": \"5428d1befe05be2ba32195496e58dc90\",\n","    },\n","    \"text\": {\n","        \"repo_id\": \"suno/bark\",\n","        \"file_name\": \"text_2.pt\",\n","        \"checksum\": \"54afa89d65e318d4f5f80e8e8799026a\",\n","    },\n","    \"coarse\": {\n","        \"repo_id\": \"suno/bark\",\n","        \"file_name\": \"coarse_2.pt\",\n","        \"checksum\": \"8a98094e5e3a255a5c9c0ab7efe8fd28\",\n","    },\n","    \"fine\": {\n","        \"repo_id\": \"suno/bark\",\n","        \"file_name\": \"fine_2.pt\",\n","        \"checksum\": \"59d184ed44e3650774a2f0503a48a97b\",\n","    },\n","}\n","\n","\n","def _load_model(ckpt_path, device, use_small=False, model_type=\"text\"):\n","    if model_type == \"text\":\n","        ConfigClass = GPTConfig\n","        ModelClass = GPT\n","    elif model_type == \"coarse\":\n","        ConfigClass = GPTConfig\n","        ModelClass = GPT\n","    elif model_type == \"fine\":\n","        ConfigClass = FineGPTConfig\n","        ModelClass = FineGPT\n","    else:\n","        raise NotImplementedError()\n","    model_key = f\"{model_type}_small\" if use_small or USE_SMALL_MODELS else model_type\n","    model_info = REMOTE_MODEL_PATHS[model_key]\n","    if ckpt_path in [None, '']:\n","        ckpt_path = os.path.join(CACHE_DIR, model_info[\"file_name\"])\n","    if not os.path.exists(ckpt_path):\n","        logger.info(f\"{model_type} model not found, downloading into `{CACHE_DIR}`.\")\n","        _download(model_info[\"repo_id\"], model_info[\"file_name\"], ckpt_path)\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    # this is a hack\n","    model_args = checkpoint[\"model_args\"]\n","    if \"input_vocab_size\" not in model_args:\n","        model_args[\"input_vocab_size\"] = model_args[\"vocab_size\"]\n","        model_args[\"output_vocab_size\"] = model_args[\"vocab_size\"]\n","        del model_args[\"vocab_size\"]\n","    gptconf = ConfigClass(**checkpoint[\"model_args\"])\n","    model = ModelClass(gptconf)\n","    state_dict = checkpoint[\"model\"]\n","    # fixup checkpoint\n","    unwanted_prefix = \"_orig_mod.\"\n","    for k, v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n","    extra_keys = set(state_dict.keys()) - set(model.state_dict().keys())\n","    extra_keys = set([k for k in extra_keys if not k.endswith(\".attn.bias\")])\n","    missing_keys = set(model.state_dict().keys()) - set(state_dict.keys())\n","    missing_keys = set([k for k in missing_keys if not k.endswith(\".attn.bias\")])\n","    if len(extra_keys) != 0:\n","        raise ValueError(f\"extra keys found: {extra_keys}\")\n","    if len(missing_keys) != 0:\n","        raise ValueError(f\"missing keys: {missing_keys}\")\n","    model.load_state_dict(state_dict, strict=False)\n","    n_params = model.get_num_params()\n","    val_loss = checkpoint[\"best_val_loss\"].item()\n","    print(f\"Loaded {model_type} model with {n_params} params, val_loss={val_loss:.4f}.\")\n","    del checkpoint, state_dict\n","    _clear_cuda_cache()\n","    if model_type == \"text\":\n","        tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","        return model, tokenizer\n","    return model\n","\n","\n","def _flatten_codebooks(arr, offset_size=CODEBOOK_SIZE):\n","    assert len(arr.shape) == 2\n","    arr = arr.copy()\n","    if offset_size is not None:\n","        for n in range(1, arr.shape[0]):\n","            arr[n, :] += offset_size * n\n","    flat_arr = arr.ravel(\"F\")\n","    return flat_arr\n","\n","\n","def load_filepaths_and_text(filename, split=\"|\"):\n","    with open(filename, encoding='utf-8', errors='ignore') as f:\n","        filepaths_and_text = [line.strip().replace('\"', '').split(split) for line in f]\n","        #print(filepaths_and_text)\n","        base = os.path.dirname(filename)\n","        for j in range(len(filepaths_and_text)):\n","            filepaths_and_text[j][0] = os.path.join(base, filepaths_and_text[j][0])\n","    return filepaths_and_text\n","\n","\n","class TtsDataset(torch.utils.data.Dataset):\n","    def __init__(self, opt):\n","        self.path = os.path.dirname(opt['path'])\n","        self.mode = opt['mode']\n","        self.audiopaths_and_text = load_filepaths_and_text(os.path.join(opt['path'] , opt['mode'] + '.txt'))\n","\n","    def __getitem__(self, index):\n","        audiopath_and_text = self.audiopaths_and_text[index]\n","        audiopath = audiopath_and_text[0]\n","\n","        tokens_path = audiopath.replace('.wav', '.npz').replace('/content/drive/MyDrive/datasets_yoo/', '/content/drive/MyDrive/datasets_yoo/tokens/')\n","        with np.load(tokens_path) as data:\n","             fine_tokens = data['fine']\n","\n","        return torch.from_numpy(fine_tokens)\n","\n","    def __len__(self):\n","        return len(self.audiopaths_and_text)\n","\n","\n","class TtsCollater():\n","    def __init__(self):\n","        pass\n","    def __call__(self, batch):\n","        max_len = 1024\n","        fine_tokens = []\n","\n","        for fine_tokens_ in batch:\n","            if fine_tokens_.shape[1] > max_len:\n","                start_idx = np.random.randint(0, fine_tokens_.shape[1] - max_len + 1)\n","                fine_tokens_ = fine_tokens_[:, start_idx : start_idx + max_len]\n","\n","            pad_size = max_len - fine_tokens_.shape[1]\n","            fine_tokens_ = F.pad(fine_tokens_, (0, pad_size), value=CODEBOOK_SIZE)\n","\n","            fine_tokens_ = fine_tokens_.T\n","\n","            fine_tokens.append(fine_tokens_)\n","\n","        return {'fine_tokens': torch.stack(fine_tokens).contiguous()}\n","\n","import torch.cuda.amp\n","accelerator = Accelerator(\n","    gradient_accumulation_steps=grad_accum,\n","    mixed_precision=mixed_precision,\n","    log_with=log_with,\n","    project_dir=logging_dir,\n",")\n","device = accelerator.device\n","\n","os.makedirs(output_dir, exist_ok=True)\n","\n","set_seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMKcSaX3W-ty","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["f9df4ae4229441caaf51d58ec7323e52","75f84abd7e3c42f6950a5dd1ed37b742","ccf3357142b645e0967c2e72b7610364","85a3695193394f028e3648b909ce3ea2","8cc90bcf7fe94eba89ad2e1a344d442e","1e2d5e26ddec46ea90ad9cd98a1f3355","0da33fcb3b414cccbfb918fe3e4bd71f","ed9c064fdb494c90b99cda6ed706f7b7","8d68bf889caa4044a4c53df9bcc8dad7","203ecfd79f5f4787b0fbd3be12c55345","f621c57f0c6745789b0a88544abce226"]},"outputId":"980b6f74-b0b7-4e67-e1c2-875288f199e6","executionInfo":{"status":"ok","timestamp":1701593889996,"user_tz":-540,"elapsed":27428,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading HuBERT base model\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloading HuBERT base model\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloaded HuBERT\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloaded HuBERT\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading HuBERT custom tokenizer\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloading HuBERT custom tokenizer\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["quantifier_hubert_base_ls960_14.pth:   0%|          | 0.00/104M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9df4ae4229441caaf51d58ec7323e52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloaded tokenizer\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloaded tokenizer\n","</pre>\n"]},"metadata":{}}],"source":["max_duration_sec = 15.12 # the maximum allowed duration in seconds\n","\n","path = dataset_path\n","device = 'cuda' # or 'cpu'\n","\n","# # From https://github.com/gitmylo/bark-voice-cloning-HuBERT-quantizer\n","\n","hubert_manager = HuBERTManager()\n","hubert_manager.make_sure_hubert_installed()\n","hubert_manager.make_sure_tokenizer_installed()\n","\n","# # Load the HuBERT model\n","hubert_model = CustomHubert(checkpoint_path=hubert_path).to(device)\n","hubert_model.eval()\n","for param in hubert_model.parameters():\n","     param.requires_grad = False\n","\n","# # Load the CustomTokenizer model\n","hubert_tokenizer = CustomTokenizer.load_from_checkpoint(hubert_tokenizer_path).to(device)  # Automatically uses the right layers\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B9EoHln9fedZ","outputId":"7559fb6e-b333-4049-b528-967fcc19b1c0","executionInfo":{"status":"ok","timestamp":1701593916250,"user_tz":-540,"elapsed":14995,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/bark/bark\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://dl.fbaipublicfiles.com/encodec/v0/encodec_24khz-d7cc33bc.th\" to /root/.cache/torch/hub/checkpoints/encodec_24khz-d7cc33bc.th\n","100%|██████████| 88.9M/88.9M [00:00<00:00, 94.8MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["%cd /content/bark/bark\n","\n","from bark.generation import _load_codec_model\n","codec_model = _load_codec_model(device)\n","codec_model.eval()\n","for param in codec_model.parameters():\n","     param.requires_grad = False\n","%cd /content\n","\n","def get_duration(wav, sr):\n","    return wav.shape[1] / sr\n","\n","valid_lines_train = []\n","\n","# convert wavs to semantic tokens\n","for wav_path, txt in load_filepaths_and_text('/content/drive/MyDrive/datasets_yoo/yoojs_train.csv'):\n","     wav, sr = torchaudio.load(wav_path)\n","     if not get_duration(wav, sr) > max_duration_sec:\n","         valid_lines_train.append((wav_path, txt))\n","     wav = convert_audio(wav, sr, SAMPLE_RATE, CHANNELS).to(device)\n","\n","     semantic_vectors = hubert_model.forward(wav, input_sample_hz=SAMPLE_RATE)\n","     semantic_tokens = hubert_tokenizer.get_token(semantic_vectors)\n","\n","     # save semantic tokens\n","     os.makedirs(os.path.join(path, 'tokens'), exist_ok=True)\n","     semantic_tokens = semantic_tokens.cpu().numpy()\n","\n","     # Extract discrete codes from EnCodec\n","     with torch.no_grad():\n","         encoded_frames = codec_model.encode(wav.unsqueeze(0))\n","     codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()  # [n_q, T]\n","\n","     # move codes to cpu\n","     codes = codes.cpu().numpy()\n","\n","     # save tokens\n","     np.savez_compressed(os.path.join(path, 'tokens', os.path.basename(wav_path).replace('.wav', '.npz')), fine=codes, coarse=codes[:2, :], semantic=semantic_tokens)\n","\n","# rewrite train.txt with valid lines(유효한 것들 모아..)\n","with open(path + 'train_valid.txt', 'w', encoding='utf-8') as f:\n","     for wav_path, txt in valid_lines_train:\n","         wav_path = os.path.relpath(wav_path, dataset_path).replace('\\\\', '/')\n","         f.write(f'{wav_path}|{txt}\\n')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqzCXwqxG7hX"},"outputs":[],"source":["valid_lines_valid = []\n","for wav_path, txt in load_filepaths_and_text(path + 'yoojs_valid.csv'):\n","     wav, sr = torchaudio.load(wav_path)\n","     if not get_duration(wav, sr) > max_duration_sec:\n","         valid_lines_valid.append((wav_path, txt))\n","     wav = convert_audio(wav, sr, SAMPLE_RATE, CHANNELS).to(device)\n","\n","     semantic_vectors = hubert_model.forward(wav, input_sample_hz=SAMPLE_RATE)\n","     semantic_tokens = hubert_tokenizer.get_token(semantic_vectors)\n","\n","     # save semantic tokens\n","     os.makedirs(os.path.join(path, 'tokens'), exist_ok=True)\n","     semantic_tokens = semantic_tokens.cpu().numpy()\n","\n","     # Extract discrete codes from EnCodec\n","     with torch.no_grad():\n","         encoded_frames = codec_model.encode(wav.unsqueeze(0))\n","     codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()  # [n_q, T]\n","\n","     # move codes to cpu\n","     codes = codes.cpu().numpy()\n","\n","     # save tokens\n","     np.savez_compressed(os.path.join(path, 'tokens', os.path.basename(wav_path).replace('.wav', '.npz')), fine=codes, coarse=codes[:2, :], semantic=semantic_tokens)\n","\n"," # rewrite valid.txt with valid lines\n","with open(path + 'valid_valid.txt', 'w', encoding='utf-8') as f:\n","     for wav_path, txt in valid_lines_valid:\n","         wav_path = os.path.relpath(wav_path, dataset_path).replace('\\\\', '/')\n","         f.write(f'{wav_path}|{txt}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NiAnHOIJHh0j"},"outputs":[],"source":["\n","del hubert_model\n","del hubert_tokenizer\n","del codec_model\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"GxIo1kaMQ5th"},"source":["### train, val- fine"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":65,"referenced_widgets":["a0719ea69b1145e0924695b886966dcb","14825cc5726246bba9aed08f61e829ad","1f10c7b46dc9406497c86e2c0ad0cdbc","7cde3bd6b98645a097ca730a2e53e9ff","965dfde15ff24c33995a186b27d5fd8b","985b0ff53f2b4313a2940b9ece0885e0","ce19f01a4fee48baa7ebde64464065ba","a39a45e84fc9464488c8af9f8ffdd979","98f057745f5a4bdfb28492e46e44a8d4","a5d9b18f376c4220b25476d048e879da","6b1b97844f9b4a1ab2994694ff940de5"]},"id":"XqkepMPfCjtu","outputId":"67dec395-e190-4ff5-8fae-e09c3e462889","executionInfo":{"status":"ok","timestamp":1701593981499,"user_tz":-540,"elapsed":45633,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["fine_2.pt:   0%|          | 0.00/3.74G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0719ea69b1145e0924695b886966dcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loaded fine model with \u001b[1;36m302090240\u001b[0m params, \u001b[33mval_loss\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1;36m.0786\u001b[0m.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded fine model with <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">302090240</span> params, <span style=\"color: #808000; text-decoration-color: #808000\">val_loss</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0786</span>.\n","</pre>\n"]},"metadata":{}}],"source":["model = _load_model(ckpt_path, device, use_small=False, model_type=model_type)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YoONAyyaCpBQ"},"outputs":[],"source":["if scale_lr:\n","    learning_rate = (\n","        learning_rate * grad_accum * train_batch_size * accelerator.num_processes\n","    )\n","\n","if use_8bit_adam:\n","    try:\n","        import bitsandbytes as bnb\n","    except ImportError:\n","        raise ImportError(\n","            \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n","        )\n","\n","    optimizer_class = bnb.optim.AdamW8bit\n","else:\n","    optimizer_class = torch.optim.AdamW\n","\n","quantization_config=BitsAndBytesConfig(\n","    load_in_4bit=bits == 4,\n","    load_in_8bit=bits == 8,\n","    llm_int8_threshold=6.0,\n","    llm_int8_has_fp16_weight=False,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=double_quant,\n","    bnb_4bit_quant_type=quant_type # {'fp4', 'nf4'}\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEEm7B3mDbk2"},"outputs":[],"source":["if bits == 4:\n","    from accelerate.utils import CustomDtype\n","    target_dtype = CustomDtype.INT4\n","elif bits == 8:\n","    target_dtype = torch.int8\n","\n","if lora_dim > 0:\n","    for param in model.parameters():\n","        if param.ndim == 1:\n","            # cast the small parameters (e.g. layernorm) to fp32 for stability\n","            param.data = param.data.to(torch.float32)\n","\n","    class CastOutputToFloat(nn.Sequential):\n","        def forward(self, x):\n","            return super().forward(x).to(torch.float32)\n","\n","    # model.lm_head = CastOutputToFloat(model.lm_head)\n","    for i, lm_head in enumerate(model.lm_heads):\n","        model.lm_heads[i] = CastOutputToFloat(lm_head)\n","\n","    model = convert_linear_layer_to_lora(model, lora_module_name,\n","                                            lora_dim=lora_dim, lora_scaling=lora_scaling,\n","                                            lora_dropout=lora_dropout)\n","    if optimize_lora_params_only:\n","        model = only_optimize_lora_parameters(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXNs4_i6DgGJ"},"outputs":[],"source":["params_to_optimize = (\n","        param for param in model.parameters() if param.requires_grad\n","    )\n","\n","optimizer = optimizer_class(\n","    params_to_optimize,\n","    lr=learning_rate,\n","    betas=(adam_beta1, adam_beta2),\n","    weight_decay=weight_decay,\n","    eps=adam_epsilon,\n",")\n","opt_train = {\n","    'path': dataset_path,\n","    'mode': 'train_valid',\n","}\n","\n","opt_val = {\n","    'path': dataset_path,\n","    'mode': 'valid_valid',\n","}\n","\n","train_dataset = TtsDataset(opt_train)\n","validation_dataset = TtsDataset(opt_val)\n","\n","train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset,\n","    batch_size=train_batch_size,\n","    collate_fn=TtsCollater(),\n",")\n","\n","validation_dataloader = torch.utils.data.DataLoader(\n","    validation_dataset,\n","    batch_size=eval_batch_size,\n","    collate_fn=TtsCollater(),\n",")\n","\n","criterion = torch.nn.CrossEntropyLoss(ignore_index=COARSE_SEMANTIC_PAD_TOKEN)\n","\n","# Scheduler and math around the number of training steps.\n","overrode_max_train_steps = False\n","num_update_steps_per_epoch = math.ceil(len(train_dataloader) / grad_accum)\n","if max_train_steps is None:\n","    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n","    overrode_max_train_steps = True\n","\n","lr_scheduler = get_scheduler(\n","    lr_scheduler_type,\n","    optimizer=optimizer,\n","    num_warmup_steps=lr_warmup_steps * grad_accum,\n","    num_training_steps=max_train_steps * grad_accum,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3jY8YmUD4r1"},"outputs":[],"source":["model, optimizer, train_dataloader, validation_dataloader, lr_scheduler = accelerator.prepare(\n","    model, optimizer, train_dataloader, validation_dataloader, lr_scheduler\n",")\n","accelerator.register_for_checkpointing(lr_scheduler)\n","\n","weight_dtype = torch.float32\n","if accelerator.mixed_precision == \"fp16\":\n","    weight_dtype = torch.float16\n","elif accelerator.mixed_precision == \"bf16\":\n","    weight_dtype = torch.bfloat16"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":33},"id":"16N1DzroD9Ko","outputId":"214abd93-3be8-4b1d-847e-a606c46eabf1","executionInfo":{"status":"ok","timestamp":1701593997891,"user_tz":-540,"elapsed":380,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Validation Loss: \u001b[1;36m13.094276428222656\u001b[0m over \u001b[1;36m1\u001b[0m samples and \u001b[1;36m1\u001b[0m batches.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Validation Loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.094276428222656</span> over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> samples and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> batches.\n","</pre>\n"]},"metadata":{}}],"source":["# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n","num_update_steps_per_epoch = math.ceil(len(train_dataloader) / grad_accum)\n","max_train_steps = num_train_epochs * num_update_steps_per_epoch\n","# Afterwards we recalculate our number of training epochs\n","num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n","\n","# We need to initialize the trackers we use, and also store our configuration.\n","# The trackers initializes automatically on the main process.\n","if accelerator.is_main_process:\n","    accelerator.init_trackers(\"bark_coarse\", config={})\n","\n","total_batch_size = train_batch_size * accelerator.num_processes * grad_accum\n","logger.info(\"***** Running training *****\")\n","logger.info(f\"  Num examples = {len(train_dataset)}\")\n","logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n","logger.info(f\"  Num Epochs = {num_train_epochs}\")\n","logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n","logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n","logger.info(f\"  Gradient Accumulation steps = {grad_accum}\")\n","logger.info(f\"  Total optimization steps = {max_train_steps}\")\n","global_step = 0\n","first_epoch = 0\n","\n","if resume_from_checkpoint:\n","    if resume_from_checkpoint != \"latest\":\n","        path = os.path.basename(resume_from_checkpoint)\n","    else:\n","        # Get the most recent checkpoint\n","        dirs = os.listdir(output_dir)\n","        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n","        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n","        path = dirs[-1]\n","    accelerator.print(f\"Resuming from checkpoint {path}\")\n","    accelerator.load_state(os.path.join(output_dir, path))\n","    global_step = int(path.split(\"-\")[1])\n","\n","    resume_global_step = global_step * grad_accum\n","    first_epoch = resume_global_step // num_update_steps_per_epoch\n","    resume_step = resume_global_step % num_update_steps_per_epoch\n","\n","if accelerator.is_main_process:\n","    model.eval()\n","    validation_loss = 0.0\n","    num_batches = 0\n","    num_samples = 0\n","    with torch.no_grad():\n","        for val_step, val_batch in enumerate(validation_dataloader):\n","            # Similar to training, process the validation batch\n","            fine_targets_7 = val_batch['fine_tokens'][:, :, 6]\n","            fine_tokens_input_7 = torch.cat([val_batch['fine_tokens'][:, :, :6], torch.zeros_like(val_batch['fine_tokens'][:, :, 6:])], dim=2)\n","            fine_targets_8 = val_batch['fine_tokens'][:, :, 7]\n","            fine_tokens_input_8 = torch.cat([val_batch['fine_tokens'][:, :, :7], torch.zeros_like(val_batch['fine_tokens'][:, :, 7:])], dim=2)\n","\n","            # Forward pass for validation\n","            logits_7 = model(6, fine_tokens_input_7)\n","            logits_8 = model(7, fine_tokens_input_8)\n","\n","            # Calculate the validation loss\n","            loss_7 = criterion(logits_7.view(-1, model.config.output_vocab_size), fine_targets_7.view(-1))\n","            loss_8 = criterion(logits_8.view(-1, model.config.output_vocab_size), fine_targets_8.view(-1))\n","\n","            loss = (loss_7 + loss_8) / 2\n","            validation_loss += loss.item()\n","            num_batches += 1\n","            num_samples += val_batch['fine_tokens'].size(0)\n","\n","    average_validation_loss = validation_loss / num_batches\n","    logger.info(f\"Validation Loss: {average_validation_loss} over {num_samples} samples and {num_batches} batches.\")\n","    print(f\"Validation Loss: {average_validation_loss} over {num_samples} samples and {num_batches} batches.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DXeDsX7kQ1on","outputId":"0a683bac-c671-4432-980f-453d0d9d6d8d","executionInfo":{"status":"ok","timestamp":1701594334356,"user_tz":-540,"elapsed":333777,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Steps: 100%|██████████| 200/200 [05:09<00:00,  1.35s/steps, loss=1.52, lr=5.26e-5]"]}],"source":["# Only show the progress bar once on each machine.\n","progress_bar = tqdm(range(global_step, max_train_steps), disable=not accelerator.is_local_main_process)\n","progress_bar.set_description(\"Steps\")\n","\n","for epoch in range(first_epoch, num_train_epochs):\n","    model.train()\n","    for step, batch in enumerate(train_dataloader):\n","        # Skip steps until we reach the resumed step\n","        if resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n","            if step % grad_accum == 0:\n","                progress_bar.update(1)\n","            continue\n","\n","        with accelerator.accumulate(model):\n","            fine_targets_7 = batch['fine_tokens'][:, :, 6]\n","            fine_tokens_input_7 = torch.cat([batch['fine_tokens'][:, :, :6], torch.zeros_like(batch['fine_tokens'][:, :, 6:])], dim=2)\n","            fine_targets_8 = batch['fine_tokens'][:, :, 7]\n","            fine_tokens_input_8 = torch.cat([batch['fine_tokens'][:, :, :7], torch.zeros_like(batch['fine_tokens'][:, :, 7:])], dim=2)\n","\n","            # Forward pass\n","            logits_7 = model(6, fine_tokens_input_7)\n","            logits_8 = model(7, fine_tokens_input_8)\n","\n","            # Calculate the loss\n","            loss_7 = criterion(logits_7.view(-1, model.config.output_vocab_size), fine_targets_7.view(-1))\n","            loss_8 = criterion(logits_8.view(-1, model.config.output_vocab_size), fine_targets_8.view(-1))\n","\n","            loss = (loss_7 + loss_8) / 2\n","\n","            accelerator.backward(loss)\n","            if accelerator.sync_gradients:\n","                params_to_clip = (\n","                    param for param in model.parameters() if param.requires_grad\n","                )\n","                accelerator.clip_grad_norm_(params_to_clip, max_grad_norm)\n","            optimizer.step()\n","            lr_scheduler.step()\n","            optimizer.zero_grad()\n","\n","        # Checks if the accelerator has performed an optimization step behind the scenes\n","        if accelerator.sync_gradients:\n","            progress_bar.update(1)\n","            global_step += 1\n","\n","            if global_step % checkpointing_steps == 0:\n","                if accelerator.is_main_process:\n","                    save_path = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n","                    accelerator.save_state(save_path)\n","                    logger.info(f\"Saved state to {save_path}\")\n","\n","        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n","        progress_bar.set_postfix(**logs)\n","        accelerator.log(logs, step=global_step)\n","\n","        if global_step >= max_train_steps:\n","            break\n","\n","    accelerator.wait_for_everyone()\n","\n","if accelerator.is_main_process:\n","    if lora_dim > 0:\n","        model = convert_lora_to_linear_layer(model)\n","    # save model\n","    accelerator.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n","\n","    config = model.config.__dict__\n","    # save config\n","    with open(os.path.join(output_dir, \"config.json\"), \"w\") as f:\n","        json.dump(config, f, indent=2)\n","\n","accelerator.end_training()"]},{"cell_type":"code","source":["\n","if accelerator.is_main_process:\n","    if lora_dim > 0:\n","        model = convert_lora_to_linear_layer(model)\n","    # save model\n","    accelerator.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n","\n","    config = model.config.__dict__\n","    # save config\n","    with open(os.path.join(output_dir, \"config.json\"), \"w\") as f:\n","        json.dump(config, f, indent=2)\n","\n","accelerator.end_training()"],"metadata":{"id":"IXll-6vBemjk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":33},"id":"jvrREez5TYNN","outputId":"52e05f6b-16fc-4935-a6a1-2332685dba1c","executionInfo":{"status":"ok","timestamp":1701594377215,"user_tz":-540,"elapsed":443,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Validation Loss: \u001b[1;36m1.1932625770568848\u001b[0m over \u001b[1;36m1\u001b[0m samples and \u001b[1;36m1\u001b[0m batches.\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Validation Loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1932625770568848</span> over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> samples and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> batches.\n","</pre>\n"]},"metadata":{}}],"source":["if accelerator.is_main_process:\n","    model.eval()\n","    validation_loss = 0.0\n","    num_batches = 0\n","    num_samples = 0\n","    with torch.no_grad():\n","        for val_step, val_batch in enumerate(validation_dataloader):\n","            # Similar to training, process the validation batch\n","            fine_targets_7 = val_batch['fine_tokens'][:, :, 6]\n","            fine_tokens_input_7 = torch.cat([val_batch['fine_tokens'][:, :, :6], torch.zeros_like(val_batch['fine_tokens'][:, :, 6:])], dim=2)\n","            fine_targets_8 = val_batch['fine_tokens'][:, :, 7]\n","            fine_tokens_input_8 = torch.cat([val_batch['fine_tokens'][:, :, :7], torch.zeros_like(val_batch['fine_tokens'][:, :, 7:])], dim=2)\n","\n","            # Forward pass for validation\n","            logits_7 = model(6, fine_tokens_input_7)\n","            logits_8 = model(7, fine_tokens_input_8)\n","\n","            # Calculate the validation loss\n","            loss_7 = criterion(logits_7.view(-1, model.config.output_vocab_size), fine_targets_7.view(-1))\n","            loss_8 = criterion(logits_8.view(-1, model.config.output_vocab_size), fine_targets_8.view(-1))\n","\n","            loss = (loss_7 + loss_8) / 2\n","            validation_loss += loss.item()\n","            num_batches += 1\n","            num_samples += val_batch['fine_tokens'].size(0)\n","\n","    average_validation_loss = validation_loss / num_batches\n","    logger.info(f\"Validation Loss: {average_validation_loss} over {num_samples} samples and {num_batches} batches.\")\n","    print(f\"Validation Loss: {average_validation_loss} over {num_samples} samples and {num_batches} batches.\")"]},{"cell_type":"markdown","metadata":{"id":"BS41g7rz_kqh"},"source":["### train- semaintic"]},{"cell_type":"markdown","metadata":{"id":"4GziFiTBUqee"},"source":["## TEST & CREATE WAV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgk16VuVWUP8"},"outputs":[],"source":["from IPython.display import Audio\n","from scipy.io.wavfile import write as write_wav\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7F2PAbfnHmB"},"outputs":[],"source":["#generation\n","import contextlib\n","import gc\n","import hashlib\n","import os\n","import re\n","import json\n","\n","from encodec import EncodecModel\n","import funcy\n","import logging\n","import numpy as np\n","from scipy.special import softmax\n","import torch\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from transformers import BertTokenizer\n","from huggingface_hub import hf_hub_download\n","\n","#import GPTConfig, GPT\n","#import FineGPT, FineGPTConfig\n","\n","if (\n","    torch.cuda.is_available() and\n","    hasattr(torch.cuda, \"amp\") and\n","    hasattr(torch.cuda.amp, \"autocast\") and\n","    hasattr(torch.cuda, \"is_bf16_supported\") and\n","    torch.cuda.is_bf16_supported()\n","):\n","    autocast = funcy.partial(torch.cuda.amp.autocast, dtype=torch.bfloat16)\n","else:\n","    @contextlib.contextmanager\n","    def autocast():\n","        yield\n","\n","\n","# hold models in global scope to lazy load\n","global models\n","models = {}\n","\n","global models_devices\n","models_devices = {}\n","\n","\n","CONTEXT_WINDOW_SIZE = 1024\n","\n","SEMANTIC_RATE_HZ = 49.9\n","SEMANTIC_VOCAB_SIZE = 10_000\n","\n","CODEBOOK_SIZE = 1024\n","N_COARSE_CODEBOOKS = 2\n","N_FINE_CODEBOOKS = 8\n","COARSE_RATE_HZ = 75\n","\n","SAMPLE_RATE = 24_000\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","#CUR_PATH = os.path.dirname(os.path.abspath(__file__))\n","\n","\n","default_cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\n","CACHE_DIR = os.path.join(os.getenv(\"XDG_CACHE_HOME\", default_cache_dir), \"serp\", \"bark_v0\")\n","\n","\n","USE_SMALL_MODELS = os.environ.get(\"SERP_USE_SMALL_MODELS\", False)\n","GLOBAL_ENABLE_MPS = os.environ.get(\"SERP_ENABLE_MPS\", False)\n","OFFLOAD_CPU = os.environ.get(\"SERP_OFFLOAD_CPU\", False)\n","\n","\n","REMOTE_MODEL_PATHS = {\n","    \"text_small\": {\n","        \"repo_id\": \"suno/bark\",\n","        \"file_name\": \"text.pt\",\n","        \"checksum\": \"b3e42bcbab23b688355cd44128c4cdd3\",\n","    },\n","    \"coarse_small\": {\n","        \"repo_id\": \"suno/bark\",\n","        \"file_name\": \"coarse.pt\",\n","        \"checksum\": \"5fe964825e3b0321f9d5f3857b89194d\",\n","    },\n","    \"fine_small\": {\n","        \"repo_id\": \"suno/bark\",\n","        \"file_name\": \"fine.pt\",\n","        \"checksum\": \"5428d1befe05be2ba32195496e58dc90\",\n","    },\n","    \"text\": {\n","        \"repo_id\": \"suno/bark\",\n","        \"file_name\": \"text_2.pt\",\n","        \"checksum\": \"54afa89d65e318d4f5f80e8e8799026a\",\n","    },\n","    \"coarse\": {\n","        \"repo_id\": \"suno/bark\",\n","        \"file_name\": \"coarse_2.pt\",\n","        \"checksum\": \"8a98094e5e3a255a5c9c0ab7efe8fd28\",\n","    },\n","    \"fine\": {\n","        \"repo_id\": \"suno/bark\",\n","        \"file_name\": \"fine_2.pt\",\n","        \"checksum\": \"59d184ed44e3650774a2f0503a48a97b\",\n","    },\n","}\n","\n","\n","if not hasattr(torch.nn.functional, 'scaled_dot_product_attention') and torch.cuda.is_available():\n","    logger.warning(\n","        \"torch version does not support flash attention. You will get faster\" +\n","        \" inference speed by upgrade torch to newest nightly version.\"\n","    )\n","\n","\n","def _string_md5(s):\n","    m = hashlib.md5()\n","    m.update(s.encode(\"utf-8\"))\n","    return m.hexdigest()\n","\n","\n","def _md5(fname):\n","    hash_md5 = hashlib.md5()\n","    with open(fname, \"rb\") as f:\n","        for chunk in iter(lambda: f.read(4096), b\"\"):\n","            hash_md5.update(chunk)\n","    return hash_md5.hexdigest()\n","\n","\n","def _get_ckpt_path(model_type, use_small=False, path=None):\n","    model_key = f\"{model_type}_small\" if use_small or USE_SMALL_MODELS else model_type\n","    model_name = REMOTE_MODEL_PATHS[model_key][\"file_name\"]\n","    if path is None:\n","        path = CACHE_DIR\n","    return os.path.join(path, f\"{model_name}\")\n","\n","\n","def _grab_best_device(use_gpu=True):\n","    if torch.cuda.device_count() > 0 and use_gpu:\n","        device = \"cuda\"\n","    elif torch.backends.mps.is_available() and use_gpu and GLOBAL_ENABLE_MPS:\n","        device = \"mps\"\n","    else:\n","        device = \"cpu\"\n","    return device\n","\n","\n","def _download(from_hf_path, file_name, to_local_path):\n","    to_local_path = to_local_path.replace(\"\\\\\", \"/\")\n","    path = '/'.join(to_local_path.split(\"/\")[:-1])\n","    os.makedirs(path, exist_ok=True)\n","    hf_hub_download(repo_id=from_hf_path, filename=file_name, local_dir=path)\n","    os.replace(os.path.join(path, file_name), to_local_path)\n","\n","class InferenceContext:\n","    def __init__(self, benchmark=False):\n","        # we can't expect inputs to be the same length, so disable benchmarking by default\n","        self._chosen_cudnn_benchmark = benchmark\n","        self._cudnn_benchmark = None\n","\n","    def __enter__(self):\n","        self._cudnn_benchmark = torch.backends.cudnn.benchmark\n","        torch.backends.cudnn.benchmark = self._chosen_cudnn_benchmark\n","\n","    def __exit__(self, exc_type, exc_value, exc_traceback):\n","        torch.backends.cudnn.benchmark = self._cudnn_benchmark\n","\n","\n","if torch.cuda.is_available():\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","\n","\n","@contextlib.contextmanager\n","def _inference_mode():\n","    with InferenceContext(), torch.inference_mode(), torch.no_grad(), autocast():\n","        yield\n","\n","\n","def _clear_cuda_cache():\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","        torch.cuda.synchronize()\n","\n","\n","def clean_models(model_key=None):\n","    global models\n","    model_keys = [model_key] if model_key is not None else models.keys()\n","    for k in model_keys:\n","        if k in models:\n","            del models[k]\n","    _clear_cuda_cache()\n","    gc.collect()\n","\n","\n","def _load_model(ckpt_path, device, use_small=False, model_type=\"text\"):\n","    if model_type == \"text\":\n","        ConfigClass = GPTConfig\n","        ModelClass = GPT\n","    elif model_type == \"coarse\":\n","        ConfigClass = GPTConfig\n","        ModelClass = GPT\n","    elif model_type == \"fine\":\n","        ConfigClass = FineGPTConfig\n","        ModelClass = FineGPT\n","    else:\n","        raise NotImplementedError()\n","    model_key = f\"{model_type}_small\" if use_small or USE_SMALL_MODELS else model_type\n","    model_info = REMOTE_MODEL_PATHS[model_key]\n","    # if (\n","    #     os.path.exists(ckpt_path) and\n","    #     _md5(ckpt_path) != model_info[\"checksum\"]\n","    # ):\n","    #     logger.warning(f\"found outdated {model_type} model, removing.\")\n","    #     os.remove(ckpt_path)\n","    if not os.path.exists(ckpt_path):\n","        logger.info(f\"{model_type} model not found, downloading into `{CACHE_DIR}`.\")\n","        _download(model_info[\"repo_id\"], model_info[\"file_name\"], ckpt_path)\n","    checkpoint = torch.load(ckpt_path, map_location=device)\n","    # this is a hack\n","    # check if config.json is in the same directory as the checkpoint\n","    # if so, load it\n","    # otherwise, assume it's in the checkpoint\n","    config_path = os.path.join(os.path.dirname(ckpt_path), \"config.json\")\n","    if os.path.exists(config_path):\n","        with open(config_path, \"r\") as f:\n","            model_args = json.load(f)\n","    else:\n","        model_args = checkpoint[\"model_args\"]\n","    if \"input_vocab_size\" not in model_args:\n","        model_args[\"input_vocab_size\"] = model_args[\"vocab_size\"]\n","        model_args[\"output_vocab_size\"] = model_args[\"vocab_size\"]\n","        del model_args[\"vocab_size\"]\n","    gptconf = ConfigClass(**model_args)\n","    model = ModelClass(gptconf)\n","    if checkpoint.get(\"model\", None) is not None:\n","        state_dict = checkpoint[\"model\"]\n","    else:\n","        state_dict = checkpoint\n","    # fixup checkpoint\n","    unwanted_prefix = \"_orig_mod.\"\n","    for k, v in list(state_dict.items()):\n","        if k.startswith(unwanted_prefix):\n","            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n","    unwanted_suffixes = [\n","        \"lora_right_weight\",\n","        \"lora_left_weight\",\n","        \"lora_right_bias\",\n","        \"lora_left_bias\",\n","    ]\n","    for k, v in list(state_dict.items()):\n","        for suffix in unwanted_suffixes:\n","            if k.endswith(suffix):\n","                state_dict.pop(k)\n","    # super hacky - should probably refactor this\n","    if state_dict.get('lm_head.0.weight', None) is not None:\n","        state_dict['lm_head.weight'] = state_dict.pop('lm_head.0.weight')\n","    if state_dict.get('lm_heads.0.0.weight', None) is not None:\n","        state_dict['lm_heads.0.weight'] = state_dict.pop('lm_heads.0.0.weight')\n","    if state_dict.get('lm_heads.1.0.weight', None) is not None:\n","        state_dict['lm_heads.1.weight'] = state_dict.pop('lm_heads.1.0.weight')\n","    if state_dict.get('lm_heads.2.0.weight', None) is not None:\n","        state_dict['lm_heads.2.weight'] = state_dict.pop('lm_heads.2.0.weight')\n","    if state_dict.get('lm_heads.3.0.weight', None) is not None:\n","        state_dict['lm_heads.3.weight'] = state_dict.pop('lm_heads.3.0.weight')\n","    if state_dict.get('lm_heads.4.0.weight', None) is not None:\n","        state_dict['lm_heads.4.weight'] = state_dict.pop('lm_heads.4.0.weight')\n","    if state_dict.get('lm_heads.5.0.weight', None) is not None:\n","        state_dict['lm_heads.5.weight'] = state_dict.pop('lm_heads.5.0.weight')\n","    if state_dict.get('lm_heads.6.0.weight', None) is not None:\n","        state_dict['lm_heads.6.weight'] = state_dict.pop('lm_heads.6.0.weight')\n","    extra_keys = set(state_dict.keys()) - set(model.state_dict().keys())\n","    extra_keys = set([k for k in extra_keys if not k.endswith(\".attn.bias\")])\n","    missing_keys = set(model.state_dict().keys()) - set(state_dict.keys())\n","    missing_keys = set([k for k in missing_keys if not k.endswith(\".attn.bias\")])\n","    if len(extra_keys) != 0:\n","        print(f\"extra keys found: {extra_keys}\")\n","    if len(missing_keys) != 0:\n","        raise ValueError(f\"missing keys: {missing_keys}\")\n","    model.load_state_dict(state_dict, strict=False)\n","    n_params = model.get_num_params()\n","    if checkpoint.get(\"best_val_loss\", None) is not None:\n","        val_loss = checkpoint[\"best_val_loss\"].item()\n","        logger.info(f\"model loaded: {round(n_params/1e6,1)}M params, {round(val_loss,3)} loss\")\n","    model.eval()\n","    model.to(device)\n","    del checkpoint, state_dict\n","    _clear_cuda_cache()\n","    if model_type == \"text\":\n","        tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","        return {\n","            \"model\": model,\n","            \"tokenizer\": tokenizer,\n","        }\n","    return model\n","\n","\n","def _load_codec_model(device):\n","    model = EncodecModel.encodec_model_24khz()\n","    model.set_target_bandwidth(6.0)\n","    model.eval()\n","    model.to(device)\n","    _clear_cuda_cache()\n","    return model\n","\n","\n","def load_model(use_gpu=True, use_small=False, force_reload=False, model_type=\"text\", path=None):\n","    _load_model_f = funcy.partial(_load_model, model_type=model_type, use_small=use_small)\n","    if model_type not in (\"text\", \"coarse\", \"fine\"):\n","        raise NotImplementedError()\n","    global models\n","    global models_devices\n","    device = _grab_best_device(use_gpu=use_gpu)\n","    model_key = f\"{model_type}\"\n","    if OFFLOAD_CPU:\n","        models_devices[model_key] = device\n","        device = \"cpu\"\n","    if model_key not in models or force_reload:\n","        if path.endswith(\".ckpt\") or path.endswith(\".pt\") or path.endswith(\".bin\"):\n","            ckpt_path = path\n","        else:\n","            ckpt_path = _get_ckpt_path(model_type, use_small=use_small, path=path)\n","        # clean_models(model_key=model_key)\n","        model = _load_model_f(ckpt_path, device)\n","        models[model_key] = model\n","    if model_type == \"text\":\n","        models[model_key][\"model\"].to(device)\n","    else:\n","        models[model_key].to(device)\n","    return models[model_key]\n","\n","\n","def load_codec_model(use_gpu=True, force_reload=False):\n","    global models\n","    global models_devices\n","    device = _grab_best_device(use_gpu=use_gpu)\n","    if device == \"mps\":\n","        # encodec doesn't support mps\n","        device = \"cpu\"\n","    model_key = \"codec\"\n","    if OFFLOAD_CPU:\n","        models_devices[model_key] = device\n","        device = \"cpu\"\n","    if model_key not in models or force_reload:\n","        clean_models(model_key=model_key)\n","        model = _load_codec_model(device)\n","        models[model_key] = model\n","    models[model_key].to(device)\n","    return models[model_key]\n","\n","\n","def preload_models_vc(\n","    text_use_gpu=True,\n","    text_use_small=False,\n","    text_model_path=None,\n","    coarse_use_gpu=True,\n","    coarse_use_small=False,\n","    coarse_model_path=None,\n","    fine_use_gpu=True,\n","    fine_use_small=False,\n","    fine_model_path=None,\n","    codec_use_gpu=True,\n","    force_reload=False,\n","    path=None,\n","):\n","    \"\"\"Load all the necessary models for the pipeline.\"\"\"\n","    if _grab_best_device() == \"cpu\" and (\n","        text_use_gpu or coarse_use_gpu or fine_use_gpu or codec_use_gpu\n","    ):\n","        logger.warning(\"No GPU being used. Careful, inference might be very slow!\")\n","    _ = load_model(\n","        model_type=\"text\", use_gpu=text_use_gpu, use_small=text_use_small, force_reload=force_reload, path=path if text_model_path is None else text_model_path\n","    )\n","    _ = load_model(\n","        model_type=\"coarse\",\n","        use_gpu=coarse_use_gpu,\n","        use_small=coarse_use_small,\n","        force_reload=force_reload,\n","        path=path if coarse_model_path is None else coarse_model_path,\n","    )\n","    _ = load_model(\n","        model_type=\"fine\", use_gpu=fine_use_gpu, use_small=fine_use_small, force_reload=force_reload, path=path if fine_model_path is None else fine_model_path\n","    )\n","    _ = load_codec_model(use_gpu=codec_use_gpu, force_reload=force_reload)\n","\n","\n","####\n","# Generation Functionality\n","####\n","\n","def _tokenize(tokenizer, text):\n","    return tokenizer.encode(text, add_special_tokens=False)\n","\n","\n","def _detokenize(tokenizer, enc_text):\n","    return tokenizer.decode(enc_text)\n","\n","\n","def _normalize_whitespace(text):\n","    return re.sub(r\"\\s+\", \" \", text).strip()\n","\n","TEXT_ENCODING_OFFSET = 10_048\n","SEMANTIC_PAD_TOKEN = 10_000\n","TEXT_PAD_TOKEN = 129_595\n","SEMANTIC_INFER_TOKEN = 129_599\n","\n","\n","def generate_text_semantic(\n","    text,\n","    history_prompt=None,\n","    temp=0.7,\n","    top_k=None,\n","    top_p=None,\n","    silent=False,\n","    min_eos_p=0.2,\n","    max_gen_duration_s=None,\n","    allow_early_stop=True,\n","    use_kv_caching=False,\n","):\n","    \"\"\"Generate semantic tokens from text.\"\"\"\n","    assert isinstance(text, str)\n","    text = _normalize_whitespace(text)\n","    assert len(text.strip()) > 0\n","    if history_prompt is not None:\n","        if history_prompt.endswith(\".npz\"):\n","            try:\n","                semantic_history = np.load(history_prompt)[\"semantic_prompt\"]\n","            except:\n","                semantic_history = np.load(history_prompt)[\"semantic\"]\n","        else:\n","            semantic_history = np.load(\n","                os.path.join(CUR_PATH, \"assets\", \"prompts\", f\"{history_prompt}.npz\")\n","            )[\"semantic_prompt\"]\n","        assert (\n","            isinstance(semantic_history, np.ndarray)\n","            and len(semantic_history.shape) == 1\n","            and len(semantic_history) > 0\n","            and semantic_history.min() >= 0\n","            and semantic_history.max() <= SEMANTIC_VOCAB_SIZE - 1\n","        )\n","    else:\n","        semantic_history = None\n","    # load models if not yet exist\n","    global models\n","    global models_devices\n","    if \"text\" not in models:\n","        preload_models_vc()\n","    model_container = models[\"text\"]\n","    model = model_container[\"model\"]\n","    tokenizer = model_container[\"tokenizer\"]\n","    encoded_text = np.array(_tokenize(tokenizer, text)) + TEXT_ENCODING_OFFSET\n","    if OFFLOAD_CPU:\n","        model.to(models_devices[\"text\"])\n","    device = next(model.parameters()).device\n","    if len(encoded_text) > 256:\n","        p = round((len(encoded_text) - 256) / len(encoded_text) * 100, 1)\n","        logger.warning(f\"warning, text too long, lopping of last {p}%\")\n","        encoded_text = encoded_text[:256]\n","    encoded_text = np.pad(\n","        encoded_text,\n","        (0, 256 - len(encoded_text)),\n","        constant_values=TEXT_PAD_TOKEN,\n","        mode=\"constant\",\n","    )\n","    if semantic_history is not None:\n","        semantic_history = semantic_history.astype(np.int64)\n","        # lop off if history is too long, pad if needed\n","        semantic_history = semantic_history[-256:]\n","        semantic_history = np.pad(\n","            semantic_history,\n","            (0, 256 - len(semantic_history)),\n","            constant_values=SEMANTIC_PAD_TOKEN,\n","            mode=\"constant\",\n","        )\n","    else:\n","        semantic_history = np.array([SEMANTIC_PAD_TOKEN] * 256)\n","    x = torch.from_numpy(\n","        np.hstack([\n","            encoded_text, semantic_history, np.array([SEMANTIC_INFER_TOKEN])\n","        ]).astype(np.int64)\n","    )[None]\n","    assert x.shape[1] == 256 + 256 + 1\n","    with _inference_mode():\n","        x = x.to(device)\n","        n_tot_steps = 768\n","        # custom tqdm updates since we don't know when eos will occur\n","        pbar = tqdm(disable=silent, total=100)\n","        pbar_state = 0\n","        tot_generated_duration_s = 0\n","        kv_cache = None\n","        for n in range(n_tot_steps):\n","            if use_kv_caching and kv_cache is not None:\n","                x_input = x[:, [-1]]\n","            else:\n","                x_input = x\n","            logits, kv_cache = model(\n","                x_input, merge_context=True, use_cache=use_kv_caching, past_kv=kv_cache\n","            )\n","            relevant_logits = logits[0, 0, :SEMANTIC_VOCAB_SIZE]\n","            if allow_early_stop:\n","                relevant_logits = torch.hstack(\n","                    (relevant_logits, logits[0, 0, [SEMANTIC_PAD_TOKEN]])  # eos\n","                )\n","            if top_p is not None:\n","                # faster to convert to numpy\n","                original_device = relevant_logits.device\n","                relevant_logits = relevant_logits.detach().cpu().type(torch.float32).numpy()\n","                sorted_indices = np.argsort(relevant_logits)[::-1]\n","                sorted_logits = relevant_logits[sorted_indices]\n","                cumulative_probs = np.cumsum(softmax(sorted_logits))\n","                sorted_indices_to_remove = cumulative_probs > top_p\n","                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].copy()\n","                sorted_indices_to_remove[0] = False\n","                relevant_logits[sorted_indices[sorted_indices_to_remove]] = -np.inf\n","                relevant_logits = torch.from_numpy(relevant_logits)\n","                relevant_logits = relevant_logits.to(original_device)\n","            if top_k is not None:\n","                v, _ = torch.topk(relevant_logits, min(top_k, relevant_logits.size(-1)))\n","                relevant_logits[relevant_logits < v[-1]] = -float(\"Inf\")\n","            probs = F.softmax(relevant_logits / temp, dim=-1)\n","            # multinomial bugged on mps: shuttle to cpu if necessary\n","            inf_device = probs.device\n","            if probs.device.type == \"mps\":\n","                probs = probs.to(\"cpu\")\n","            item_next = torch.multinomial(probs, num_samples=1)\n","            probs = probs.to(inf_device)\n","            item_next = item_next.to(inf_device)\n","            if allow_early_stop and (\n","                item_next == SEMANTIC_VOCAB_SIZE\n","                or (min_eos_p is not None and probs[-1] >= min_eos_p)\n","            ):\n","                # eos found, so break\n","                pbar.update(100 - pbar_state)\n","                break\n","            x = torch.cat((x, item_next[None]), dim=1)\n","            tot_generated_duration_s += 1 / SEMANTIC_RATE_HZ\n","            if max_gen_duration_s is not None and tot_generated_duration_s > max_gen_duration_s:\n","                pbar.update(100 - pbar_state)\n","                break\n","            if n == n_tot_steps - 1:\n","                pbar.update(100 - pbar_state)\n","                break\n","            del logits, relevant_logits, probs, item_next\n","            req_pbar_state = np.min([100, int(round(100 * n / n_tot_steps))])\n","            if req_pbar_state > pbar_state:\n","                pbar.update(req_pbar_state - pbar_state)\n","            pbar_state = req_pbar_state\n","        pbar.close()\n","        out = x.detach().cpu().numpy().squeeze()[256 + 256 + 1 :]\n","    if OFFLOAD_CPU:\n","        model.to(\"cpu\")\n","    assert all(0 <= out) and all(out < SEMANTIC_VOCAB_SIZE)\n","    _clear_cuda_cache()\n","    return out\n","\n","\n","def _flatten_codebooks(arr, offset_size=CODEBOOK_SIZE):\n","    assert len(arr.shape) == 2\n","    arr = arr.copy()\n","    if offset_size is not None:\n","        for n in range(1, arr.shape[0]):\n","            arr[n, :] += offset_size * n\n","    flat_arr = arr.ravel(\"F\")\n","    return flat_arr\n","\n","\n","COARSE_SEMANTIC_PAD_TOKEN = 12_048\n","COARSE_INFER_TOKEN = 12_050\n","\n","\n","def generate_coarse(\n","    x_semantic,\n","    history_prompt=None,\n","    temp=0.7,\n","    top_k=None,\n","    top_p=None,\n","    silent=False,\n","    max_coarse_history=630,  # min 60 (faster), max 630 (more context)\n","    sliding_window_len=60,\n","    use_kv_caching=False,\n","):\n","    \"\"\"Generate coarse audio codes from semantic tokens.\"\"\"\n","    assert (\n","        isinstance(x_semantic, np.ndarray)\n","        and len(x_semantic.shape) == 1\n","        and len(x_semantic) > 0\n","        and x_semantic.min() >= 0\n","        and x_semantic.max() <= SEMANTIC_VOCAB_SIZE - 1\n","    )\n","    assert 60 <= max_coarse_history <= 630\n","    assert max_coarse_history + sliding_window_len <= 1024 - 256\n","    semantic_to_coarse_ratio = COARSE_RATE_HZ / SEMANTIC_RATE_HZ * N_COARSE_CODEBOOKS\n","    max_semantic_history = int(np.floor(max_coarse_history / semantic_to_coarse_ratio))\n","    if history_prompt is not None:\n","        if history_prompt.endswith(\".npz\"):\n","            x_history = np.load(history_prompt)\n","        else:\n","            x_history = np.load(\n","                os.path.join(CUR_PATH, \"assets\", \"prompts\", f\"{history_prompt}.npz\")\n","            )\n","        try:\n","            x_semantic_history = x_history[\"semantic_prompt\"]\n","            x_coarse_history = x_history[\"coarse_prompt\"]\n","        except:\n","            x_semantic_history = x_history[\"semantic\"]\n","            x_coarse_history = x_history[\"coarse\"]\n","        assert (\n","            isinstance(x_semantic_history, np.ndarray)\n","            and len(x_semantic_history.shape) == 1\n","            and len(x_semantic_history) > 0\n","            and x_semantic_history.min() >= 0\n","            and x_semantic_history.max() <= SEMANTIC_VOCAB_SIZE - 1\n","            and isinstance(x_coarse_history, np.ndarray)\n","            and len(x_coarse_history.shape) == 2\n","            and x_coarse_history.shape[0] == N_COARSE_CODEBOOKS\n","            and x_coarse_history.shape[-1] >= 0\n","            and x_coarse_history.min() >= 0\n","            and x_coarse_history.max() <= CODEBOOK_SIZE - 1\n","            and (\n","                round(x_coarse_history.shape[-1] / len(x_semantic_history), 1)\n","                == round(semantic_to_coarse_ratio / N_COARSE_CODEBOOKS, 1)\n","            )\n","        )\n","        x_coarse_history = _flatten_codebooks(x_coarse_history) + SEMANTIC_VOCAB_SIZE\n","        # trim histories correctly\n","        n_semantic_hist_provided = np.min(\n","            [\n","                max_semantic_history,\n","                len(x_semantic_history) - len(x_semantic_history) % 2,\n","                int(np.floor(len(x_coarse_history) / semantic_to_coarse_ratio)),\n","            ]\n","        )\n","        n_coarse_hist_provided = int(round(n_semantic_hist_provided * semantic_to_coarse_ratio))\n","        x_semantic_history = x_semantic_history[-n_semantic_hist_provided:].astype(np.int32)\n","        x_coarse_history = x_coarse_history[-n_coarse_hist_provided:].astype(np.int32)\n","        # TODO: bit of a hack for time alignment (sounds better)\n","        x_coarse_history = x_coarse_history[:-2]\n","    else:\n","        x_semantic_history = np.array([], dtype=np.int32)\n","        x_coarse_history = np.array([], dtype=np.int32)\n","    # load models if not yet exist\n","    global models\n","    global models_devices\n","    if \"coarse\" not in models:\n","        preload_models_vc()\n","    model = models[\"coarse\"]\n","    if OFFLOAD_CPU:\n","        model.to(models_devices[\"coarse\"])\n","    device = next(model.parameters()).device\n","    # start loop\n","    n_steps = int(\n","        round(\n","            np.floor(len(x_semantic) * semantic_to_coarse_ratio / N_COARSE_CODEBOOKS)\n","            * N_COARSE_CODEBOOKS\n","        )\n","    )\n","    assert n_steps > 0 and n_steps % N_COARSE_CODEBOOKS == 0\n","    x_semantic = np.hstack([x_semantic_history, x_semantic]).astype(np.int32)\n","    x_coarse = x_coarse_history.astype(np.int32)\n","    base_semantic_idx = len(x_semantic_history)\n","    with _inference_mode():\n","        x_semantic_in = torch.from_numpy(x_semantic)[None].to(device)\n","        x_coarse_in = torch.from_numpy(x_coarse)[None].to(device)\n","        n_window_steps = int(np.ceil(n_steps / sliding_window_len))\n","        n_step = 0\n","        for _ in tqdm(range(n_window_steps), total=n_window_steps, disable=silent):\n","            semantic_idx = base_semantic_idx + int(round(n_step / semantic_to_coarse_ratio))\n","            # pad from right side\n","            x_in = x_semantic_in[:, np.max([0, semantic_idx - max_semantic_history]) :]\n","            x_in = x_in[:, :256]\n","            x_in = F.pad(\n","                x_in,\n","                (0, 256 - x_in.shape[-1]),\n","                \"constant\",\n","                COARSE_SEMANTIC_PAD_TOKEN,\n","            )\n","            x_in = torch.hstack(\n","                [\n","                    x_in,\n","                    torch.tensor([COARSE_INFER_TOKEN])[None].to(device),\n","                    x_coarse_in[:, -max_coarse_history:],\n","                ]\n","            )\n","            kv_cache = None\n","            for _ in range(sliding_window_len):\n","                if n_step >= n_steps:\n","                    continue\n","                is_major_step = n_step % N_COARSE_CODEBOOKS == 0\n","\n","                if use_kv_caching and kv_cache is not None:\n","                    x_input = x_in[:, [-1]]\n","                else:\n","                    x_input = x_in\n","\n","                logits, kv_cache = model(x_input, use_cache=use_kv_caching, past_kv=kv_cache)\n","                logit_start_idx = (\n","                    SEMANTIC_VOCAB_SIZE + (1 - int(is_major_step)) * CODEBOOK_SIZE\n","                )\n","                logit_end_idx = (\n","                    SEMANTIC_VOCAB_SIZE + (2 - int(is_major_step)) * CODEBOOK_SIZE\n","                )\n","                relevant_logits = logits[0, 0, logit_start_idx:logit_end_idx]\n","                if top_p is not None:\n","                    # faster to convert to numpy\n","                    original_device = relevant_logits.device\n","                    relevant_logits = relevant_logits.detach().cpu().type(torch.float32).numpy()\n","                    sorted_indices = np.argsort(relevant_logits)[::-1]\n","                    sorted_logits = relevant_logits[sorted_indices]\n","                    cumulative_probs = np.cumsum(softmax(sorted_logits))\n","                    sorted_indices_to_remove = cumulative_probs > top_p\n","                    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].copy()\n","                    sorted_indices_to_remove[0] = False\n","                    relevant_logits[sorted_indices[sorted_indices_to_remove]] = -np.inf\n","                    relevant_logits = torch.from_numpy(relevant_logits)\n","                    relevant_logits = relevant_logits.to(original_device)\n","                if top_k is not None:\n","                    v, _ = torch.topk(relevant_logits, min(top_k, relevant_logits.size(-1)))\n","                    relevant_logits[relevant_logits < v[-1]] = -float(\"Inf\")\n","                probs = F.softmax(relevant_logits / temp, dim=-1)\n","                # multinomial bugged on mps: shuttle to cpu if necessary\n","                inf_device = probs.device\n","                if probs.device.type == \"mps\":\n","                    probs = probs.to(\"cpu\")\n","                item_next = torch.multinomial(probs, num_samples=1)\n","                probs = probs.to(inf_device)\n","                item_next = item_next.to(inf_device)\n","                item_next += logit_start_idx\n","                x_coarse_in = torch.cat((x_coarse_in, item_next[None]), dim=1)\n","                x_in = torch.cat((x_in, item_next[None]), dim=1)\n","                del logits, relevant_logits, probs, item_next\n","                n_step += 1\n","            del x_in\n","        del x_semantic_in\n","    if OFFLOAD_CPU:\n","        model.to(\"cpu\")\n","    gen_coarse_arr = x_coarse_in.detach().cpu().numpy().squeeze()[len(x_coarse_history) :]\n","    del x_coarse_in\n","    assert len(gen_coarse_arr) == n_steps\n","    gen_coarse_audio_arr = gen_coarse_arr.reshape(-1, N_COARSE_CODEBOOKS).T - SEMANTIC_VOCAB_SIZE\n","    for n in range(1, N_COARSE_CODEBOOKS):\n","        gen_coarse_audio_arr[n, :] -= n * CODEBOOK_SIZE\n","    _clear_cuda_cache()\n","    return gen_coarse_audio_arr\n","\n","\n","def generate_fine(\n","    x_coarse_gen,\n","    history_prompt=None,\n","    temp=0.5,\n","    silent=True,\n","):\n","    \"\"\"Generate full audio codes from coarse audio codes.\"\"\"\n","    assert (\n","        isinstance(x_coarse_gen, np.ndarray)\n","        and len(x_coarse_gen.shape) == 2\n","        and 1 <= x_coarse_gen.shape[0] <= N_FINE_CODEBOOKS - 1\n","        and x_coarse_gen.shape[1] > 0\n","        and x_coarse_gen.min() >= 0\n","        and x_coarse_gen.max() <= CODEBOOK_SIZE - 1\n","    )\n","    if history_prompt is not None:\n","        if history_prompt.endswith(\".npz\"):\n","            try:\n","                x_fine_history = np.load(history_prompt)[\"fine_prompt\"]\n","            except:\n","                x_fine_history = np.load(history_prompt)[\"fine\"]\n","        else:\n","            x_fine_history = np.load(\n","                os.path.join(CUR_PATH, \"assets\", \"prompts\", f\"{history_prompt}.npz\")\n","            )[\"fine_prompt\"]\n","        assert (\n","            isinstance(x_fine_history, np.ndarray)\n","            and len(x_fine_history.shape) == 2\n","            and x_fine_history.shape[0] == N_FINE_CODEBOOKS\n","            and x_fine_history.shape[1] >= 0\n","            and x_fine_history.min() >= 0\n","            and x_fine_history.max() <= CODEBOOK_SIZE - 1\n","        )\n","    else:\n","        x_fine_history = None\n","    n_coarse = x_coarse_gen.shape[0]\n","    # load models if not yet exist\n","    global models\n","    global models_devices\n","    if \"fine\" not in models:\n","        preload_models_vc()\n","    model = models[\"fine\"]\n","    if OFFLOAD_CPU:\n","        model.to(models_devices[\"fine\"])\n","    device = next(model.parameters()).device\n","    # make input arr\n","    in_arr = np.vstack(\n","        [\n","            x_coarse_gen,\n","            np.zeros((N_FINE_CODEBOOKS - n_coarse, x_coarse_gen.shape[1]))\n","            + CODEBOOK_SIZE,  # padding\n","        ]\n","    ).astype(np.int32)\n","    # prepend history if available (max 512)\n","    if x_fine_history is not None:\n","        x_fine_history = x_fine_history.astype(np.int32)\n","        in_arr = np.hstack(\n","            [\n","                x_fine_history[:, -512:].astype(np.int32),\n","                in_arr,\n","            ]\n","        )\n","        n_history = x_fine_history[:, -512:].shape[1]\n","    else:\n","        n_history = 0\n","    n_remove_from_end = 0\n","    # need to pad if too short (since non-causal model)\n","    if in_arr.shape[1] < 1024:\n","        n_remove_from_end = 1024 - in_arr.shape[1]\n","        in_arr = np.hstack(\n","            [\n","                in_arr,\n","                np.zeros((N_FINE_CODEBOOKS, n_remove_from_end), dtype=np.int32) + CODEBOOK_SIZE,\n","            ]\n","        )\n","    # we can be lazy about fractional loop and just keep overwriting codebooks\n","    n_loops = np.max([0, int(np.ceil((x_coarse_gen.shape[1] - (1024 - n_history)) / 512))]) + 1\n","    with _inference_mode():\n","        in_arr = torch.tensor(in_arr.T).to(device)\n","        for n in tqdm(range(n_loops), disable=silent):\n","            start_idx = np.min([n * 512, in_arr.shape[0] - 1024])\n","            start_fill_idx = np.min([n_history + n * 512, in_arr.shape[0] - 512])\n","            rel_start_fill_idx = start_fill_idx - start_idx\n","            in_buffer = in_arr[start_idx : start_idx + 1024, :][None]\n","            for nn in range(n_coarse, N_FINE_CODEBOOKS):\n","                logits = model(nn, in_buffer)\n","                if temp is None:\n","                    relevant_logits = logits[0, rel_start_fill_idx:, :CODEBOOK_SIZE]\n","                    codebook_preds = torch.argmax(relevant_logits, -1)\n","                else:\n","                    relevant_logits = logits[0, :, :CODEBOOK_SIZE] / temp\n","                    probs = F.softmax(relevant_logits, dim=-1)\n","                    # multinomial bugged on mps: shuttle to cpu if necessary\n","                    inf_device = probs.device\n","                    if probs.device.type == \"mps\":\n","                        probs = probs.to(\"cpu\")\n","                    codebook_preds = torch.hstack(\n","                        [\n","                            torch.multinomial(probs[nnn], num_samples=1).to(inf_device)\n","                            for nnn in range(rel_start_fill_idx, 1024)\n","                        ]\n","                    )\n","                in_buffer[0, rel_start_fill_idx:, nn] = codebook_preds\n","                del logits, codebook_preds\n","            # transfer over info into model_in and convert to numpy\n","            for nn in range(n_coarse, N_FINE_CODEBOOKS):\n","                in_arr[\n","                    start_fill_idx : start_fill_idx + (1024 - rel_start_fill_idx), nn\n","                ] = in_buffer[0, rel_start_fill_idx:, nn]\n","            del in_buffer\n","        gen_fine_arr = in_arr.detach().cpu().numpy().squeeze().T\n","        del in_arr\n","    if OFFLOAD_CPU:\n","        model.to(\"cpu\")\n","    gen_fine_arr = gen_fine_arr[:, n_history:]\n","    if n_remove_from_end > 0:\n","        gen_fine_arr = gen_fine_arr[:, :-n_remove_from_end]\n","    assert gen_fine_arr.shape[-1] == x_coarse_gen.shape[-1]\n","    _clear_cuda_cache()\n","    return gen_fine_arr\n","\n","\n","def codec_decode(fine_tokens):\n","    \"\"\"Turn quantized audio codes into audio array using encodec.\"\"\"\n","    # load models if not yet exist\n","    global models\n","    global models_devices\n","    if \"codec\" not in models:\n","        preload_models_vc()\n","    model = models[\"codec\"]\n","    if OFFLOAD_CPU:\n","        model.to(models_devices[\"codec\"])\n","    device = next(model.parameters()).device\n","    arr = torch.from_numpy(fine_tokens)[None]\n","    arr = arr.to(device)\n","    arr = arr.transpose(0, 1)\n","    emb = model.quantizer.decode(arr)\n","    out = model.decoder(emb)\n","    audio_arr = out.detach().cpu().numpy().squeeze()\n","    del arr, emb, out\n","    if OFFLOAD_CPU:\n","        model.to(\"cpu\")\n","    return audio_arr\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Atsnm7V761Pv"},"outputs":[],"source":["semantic_path = None # set to None if you don't want to use finetuned semantic\n","coarse_path = None # set to None if you don't want to use finetuned coarse\n","fine_path = \"/content/drive/MyDrive/datasets_yoo/fine_output/pytorch_model.bin\" # set to None if you don't want to use finetuned fine\n","use_rvc = False # Set to False to use bark without RVC\n","#rvc_name = 'mi-test'\n","#rvc_path = f\"Retrieval-based-Voice-Conversion-WebUI/weights/{rvc_name}.pth\"\n","#index_path = f\"Retrieval-based-Voice-Conversion-WebUI/logs/{rvc_name}/added_IVF256_Flat_nprobe_1_{rvc_name}_v2.index\"\n","device=\"cuda:0\"\n","is_half=True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209,"referenced_widgets":["f06abfb80f6348b488c39db0e8181c88","5d13ac3f7c8a4d49b9068e4135379762","77f51f7eab774ccca0abd9575cc23db2","36838a41fbb14cf9a9acbd1e12581877","38e7ae2e9c0841d1a8ec44f2b058ebff","584257fcc1ce40d6bd2dee36ed414a33","1aa7ebc3f6a74028a241cdc04aa93531","a43ea8ff9d034c2f917c4b07c91f119f","cfa2eecc06d649179b077fcb2d1530c7","5fc400b19e284ec4b1ef83d9fca6e33f","628bdde2399242ffa5e01363681d0354","0c50f21416824f4a9d41bbe85151a525","cf78bd25730b4ebca6361c4c0e0ab25f","da7a482174184c60bb76966326eeff15","b8d99184176e4f2dabfb4dcfb91444a9","1da7802aec5343d1a1d8f1257e3750f8","350632b3b4a548c38120fef8f9636969","d601b9cdcc6d4f0bbcd0950d4f88e2d7","b1b88a3e474841d3b73f58b6ed92744f","064d18c7a38646288c17157fe60ab489","34f5508ea0e647509868e9c124cc91ff","ad2f9710e07d46bc99941c36afd9d92a","f28e36c24285493eb1128c5d257d268d","3f635e62e29f476a81f6eaaad4c92a16","4969cc3dcc3c467ba6ab24d8edcedc63","ffee722f2dce47c49ffcdcad4a1f747a","1a4b4f81124442fd868fdb237723579b","d40d4f0880384f9982816a4d0b9ce424","ec895a1d515b4012b98e7b2b2eeb6442","f33d5623643243e29c8ce030aaa15cfa","b6f34ab99217498487a2fb5b90b16cd7","9ddddf94cfda49f1b1c788844e82177c","fd4b77a7c8ae4ea6906ff4be17d0325c","06d075c5924942c5a0a35384c3bc98a7","ae30cc1dfb304027abe3ab03bf2d306b","2b29a42ac3e24110a76574ca87cf56a7","fd30fef30ab8465a9553f55ec159be11","2dfe3a63022342c487bba3978168aefc","557448b8ab3445a698360da9e44fc892","1bb40b06b62c4661aa0658e2ec56de1b","29c3ae275777428881d491e7fce37f65","dc8aebd2d7944fc8b7d8bc752faa1ad6","60805d4359bb4d70b4391fd3128c8e6a","96748e18b2a449188a60fa74d084f521","aa1249e668524bf69a8f86da7a904af0","1684b179d01949f5899195653a4a6ea8","713c8a8aa6844e78bfc560dd00bcc8a1","50b7608632e84c3a85a3c1e8f82e46ce","7f52dc454d6a4ac088cb8fefa4694dd6","901e94cfb5464e3eb4d1deb37e9c112c","b7d43ed325da477c92944900d6f2f020","d780aa537f4a46698e3f4c74fd74a8e7","3ace7d758f634d30b9b7ecbac089fd9e","e913552248bc41c0ab0f71ac9e6567f4","cdce501f31f3418f9b46fe0c6c253bdb","3114cab3a43e48cb8594357eb27e36df","77fdd3c8cfbf430fbaa0eb9cce1df088","d117ef24284a420bbcbea8ea43eb9eda","5f142d5ba6784929880cc2015b7780f5","6d58d72266de498cb9211966ac5758a0","d6c43620793f497aaf5680544e7f8891","9cc730a13dfd4cefa02b4af56514df1a","a2238d724e9d4d1c898374cca9f865d7","aa16ca04b6e3472c8037fbc8deb7a5d2","78db1265d6664de19cfabdef7c00db0c","940b5b645a0e46af89198faa5ed83504"]},"id":"aSgbFqty7Yxo","outputId":"ccd426d8-cfaf-4ec1-d11f-0930218b86f9","executionInfo":{"status":"ok","timestamp":1701594708982,"user_tz":-540,"elapsed":285060,"user":{"displayName":"serah K","userId":"08070710945457856845"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["text_2.pt:   0%|          | 0.00/5.35G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f06abfb80f6348b488c39db0e8181c88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c50f21416824f4a9d41bbe85151a525"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f28e36c24285493eb1128c5d257d268d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06d075c5924942c5a0a35384c3bc98a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa1249e668524bf69a8f86da7a904af0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["coarse_2.pt:   0%|          | 0.00/3.93G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3114cab3a43e48cb8594357eb27e36df"}},"metadata":{}}],"source":["preload_models_vc(\n","    text_use_gpu=True,\n","    text_use_small=False,\n","    text_model_path=semantic_path,\n","    coarse_use_gpu=True,\n","    coarse_use_small=False,\n","    coarse_model_path=coarse_path,\n","    fine_use_gpu=True,\n","    fine_use_small=False,\n","    fine_model_path=fine_path,\n","    codec_use_gpu=True,\n","    force_reload=False,\n","    path=\"models\"\n",")\n","\n","if use_rvc:\n","    from rvc_infer import get_vc, vc_single\n","    get_vc(rvc_path, device, is_half)\n","# simple generation"]},{"cell_type":"markdown","metadata":{"id":"ZAgUcUPc8NLD"},"source":["### 긴문구"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZqGbyLl8Lr-"},"outputs":[],"source":["import re\n","def split_and_recombine_text(text, desired_length=100, max_length=150):\n","    # from https://github.com/neonbjb/tortoise-tts\n","    \"\"\"Split text it into chunks of a desired length trying to keep sentences intact.\"\"\"\n","    # normalize text, remove redundant whitespace and convert non-ascii quotes to ascii\n","    text = re.sub(r\"\\n\\n+\", \"\\n\", text)\n","    text = re.sub(r\"\\s+\", \" \", text)\n","    text = re.sub(r\"[“”]\", '\"', text)\n","\n","    rv = []\n","    in_quote = False\n","    current = \"\"\n","    split_pos = []\n","    pos = -1\n","    end_pos = len(text) - 1\n","\n","    def seek(delta):\n","        nonlocal pos, in_quote, current\n","        is_neg = delta < 0\n","        for _ in range(abs(delta)):\n","            if is_neg:\n","                pos -= 1\n","                current = current[:-1]\n","            else:\n","                pos += 1\n","                current += text[pos]\n","            if text[pos] == '\"':\n","                in_quote = not in_quote\n","        return text[pos]\n","\n","    def peek(delta):\n","        p = pos + delta\n","        return text[p] if p < end_pos and p >= 0 else \"\"\n","\n","    def commit():\n","        nonlocal rv, current, split_pos\n","        rv.append(current)\n","        current = \"\"\n","        split_pos = []\n","\n","    while pos < end_pos:\n","        c = seek(1)\n","        # do we need to force a split?\n","        if len(current) >= max_length:\n","            if len(split_pos) > 0 and len(current) > (desired_length / 2):\n","                # we have at least one sentence and we are over half the desired length, seek back to the last split\n","                d = pos - split_pos[-1]\n","                seek(-d)\n","            else:\n","                # no full sentences, seek back until we are not in the middle of a word and split there\n","                while c not in \"!?.\\n \" and pos > 0 and len(current) > desired_length:\n","                    c = seek(-1)\n","            commit()\n","        # check for sentence boundaries\n","        elif not in_quote and (c in \"!?\\n\" or (c == \".\" and peek(1) in \"\\n \")):\n","            # seek forward if we have consecutive boundary markers but still within the max length\n","            while (\n","                pos < len(text) - 1 and len(current) < max_length and peek(1) in \"!?.\"\n","            ):\n","                c = seek(1)\n","            split_pos.append(pos)\n","            if len(current) >= desired_length:\n","                commit()\n","        # treat end of quote as a boundary if its followed by a space or newline\n","        elif in_quote and peek(1) == '\"' and peek(2) in \"\\n \":\n","            seek(2)\n","            split_pos.append(pos)\n","    rv.append(current)\n","\n","    # clean up, remove lines with only whitespace or punctuation\n","    rv = [s.strip() for s in rv]\n","    rv = [s for s in rv if len(s) > 0 and not re.match(r\"^[\\s\\.,;:!?]*$\", s)]\n","\n","    return rv\n","\n","\n","def generate_with_settings(text_prompt, semantic_temp=0.7, semantic_top_k=50, semantic_top_p=0.95, coarse_temp=0.7, coarse_top_k=50, coarse_top_p=0.95, fine_temp=0.5, voice_name_1=None,voice_name_2=None, use_semantic_history_prompt=True, use_coarse_history_prompt=True, use_fine_history_prompt=True, output_full=False):\n","    # generation with more control\n","    x_semantic = generate_text_semantic(\n","        text_prompt,\n","        history_prompt=voice_name_1 if use_semantic_history_prompt else None,\n","        temp=semantic_temp,\n","        top_k=semantic_top_k,\n","        top_p=semantic_top_p,\n","    )\n","\n","    x_coarse_gen = generate_coarse(\n","        x_semantic,\n","        history_prompt=voice_name if use_coarse_history_prompt else None,\n","        temp=coarse_temp,\n","        top_k=coarse_top_k,\n","        top_p=coarse_top_p,\n","    )\n","    x_fine_gen = generate_fine(\n","        x_coarse_gen,\n","        history_prompt=voice_name_2 if use_fine_history_prompt else None,\n","        temp=fine_temp,\n","    )\n","\n","    if output_full:\n","        full_generation = {\n","            'semantic_prompt': x_semantic,\n","            'coarse_prompt': x_coarse_gen,\n","            'fine_prompt': x_fine_gen,\n","        }\n","        return full_generation, codec_decode(x_fine_gen)\n","    return codec_decode(x_fine_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KOEOnOJC8f2r"},"outputs":[],"source":["text = \"\"\"당뇨병은 혈액 중의 포도당을 세포내로 운반하도록 조절하는\n","호르몬인 인슐린이 부족하거나 그 기능을 제대로 발휘하지 못하여 높아진 혈당\"\"\"\n","#이 소변으로 배출되는 당대사 장애 질병이다.\"\"\"\n","#인슐린 분비 부족이나 인슐린에 대한 세포반응성 저하로 인해 음식물이 소화되어 얻어지는 글루코즈가 인체 내\n","#혈액에 축적되면 동맥경화증 고혈압 뇌혈관 경색증과 같은 심혈관계 질환, 당뇨병성 신증과 같은 신장 질환,\n","#당뇨병성 망막증이나 백내장과 같은 안질환, 농피증이나 괴저와 같은 피부질환,\n","#치주농루와 같은 구강질환 등의 합병증을 유발한다. \"\"\"\n","#사회 경제적인 발전이 이루어지면서 과식, 운동부족, 스트레스 증가 등으로\n","#인하여 당뇨병 인구는 현저히 증가하는 추세이며, 선진화되어 있는 국가의 경우\n","#전체인구 대비 5∼10%가 당뇨병 환자로 추정되고 있다. 2005년 당뇨병 전국 표\n","#본조사 결과, 우리 국민 중 성인의 약 8%가 당뇨병을 앓고 있고, 1인당 평균 진료비는 일반인의 4.6배이며,\n","#치료 중 1년이내 사망할 확률은 일반인보다 3배 이상으로 OECD 국가 중 가장 높게 나타났다\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYI293NrBq_x"},"outputs":[],"source":["import numpy as np\n","from scipy.io import wavfile\n","\n","# 샘플레이트 설정 (예: 44100Hz)\n","sample_rate = 24000\n","\n","# 기본 오디오 데이터 생성 (예: 1초 길이의 무음)\n","# np.zeros를 사용하여 모든 샘플이 0인 배열 생성\n","duration = 1  # 1초\n","audio_data = np.zeros(sample_rate * duration)\n","\n","# 데이터 타입을 32비트 부동소수점 형식으로 변환\n","audio_data = audio_data.astype(np.float32)\n","\n","# WAV 파일로 저장\n","wavfile.write('/content/audio/audio.wav', sample_rate, audio_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tzPoQTo8iW6"},"outputs":[],"source":["# Chunk the text into smaller pieces then combine the generated audio\n","#같은 대사 여러번 하는게.?\n","from time import time\n","from tqdm import tqdm\n","from IPython.display import Audio\n","from scipy.io.wavfile import write as write_wav\n","import os\n","import numpy as np\n","#/content/datasets_yoo/tokens/yujaeseog17.npz\n","# generation settings\n","voice_name_1= \"/content/bark/bark/assets/prompts/ko_speaker_4.npz\"\n","voice_name_2 = \"/content/drive/MyDrive/datasets_yoo/tokens/yujaeseog61.npz\"\n","out_filepath = '/content/audio/audio.wav'\n","\n","semantic_temp = 0.7 #높은 값 (예: 1 이상)은 더 다양하고 예측 불가능한 텍스트를 생성하는 데 기여합니다.\n","semantic_top_k = 100 #모델이 다음 단어를 선택할 때 고려하는 후보 단어의 집합을 k개의 가장 높은 확률을 가진 단어로 제한합니다.\n","semantic_top_p = 0.99\n","\n","coarse_temp = 0.7\n","coarse_top_k = 100\n","coarse_top_p = 0.95\n","\n","fine_temp = 0.7\n","\n","use_semantic_history_prompt = True\n","use_coarse_history_prompt = False\n","use_fine_history_prompt = True\n","\n","use_last_generation_as_history = False #지난 루프가..\n","\n","if use_rvc:\n","    index_rate = 0.75\n","    f0up_key = -6\n","    filter_radius = 3\n","    rms_mix_rate = 0.25\n","    protect = 0.33\n","    resample_sr = SAMPLE_RATE\n","    f0method = \"harvest\" #harvest or pm\n","\n","texts = split_and_recombine_text(text)\n","\n","all_parts = []\n","for i, text in tqdm(enumerate(texts), total=len(texts)):\n","    full_generation, audio_array = generate_with_settings(\n","        text,\n","        semantic_temp=semantic_temp,\n","        semantic_top_k=semantic_top_k,\n","        semantic_top_p=semantic_top_p,\n","        coarse_temp=coarse_temp,\n","        coarse_top_k=coarse_top_k,\n","        coarse_top_p=coarse_top_p,\n","        fine_temp=fine_temp,\n","        voice_name_1=voice_name_1,\n","        voice_name_2=voice_name_2,\n","        use_semantic_history_prompt=use_semantic_history_prompt,\n","        use_coarse_history_prompt=use_coarse_history_prompt,\n","        use_fine_history_prompt=use_fine_history_prompt,\n","        output_full=True\n","    )\n","    if use_last_generation_as_history:\n","        # save to npz\n","        os.makedirs('_temp', exist_ok=True)\n","        np.savez_compressed(\n","            '_temp/history.npz',\n","            semantic_prompt=full_generation['semantic_prompt'],\n","            coarse_prompt=full_generation['coarse_prompt'],\n","            fine_prompt=full_generation['fine_prompt'],\n","        )\n","        voice_name = '_temp/history.npz'\n","    write_wav(out_filepath.replace('.wav', f'_{i}') + '.wav', SAMPLE_RATE, audio_array)\n","\n","    if use_rvc:\n","        try:\n","            audio_array = vc_single(0,out_filepath.replace('.wav', f'_{i}') + '.wav',f0up_key,None,f0method,index_path,index_rate, filter_radius=filter_radius, resample_sr=resample_sr, rms_mix_rate=rms_mix_rate, protect=protect)\n","        except:\n","            audio_array = vc_single(0,out_filepath.replace('.wav', f'_{i}') + '.wav',f0up_key,None,'pm',index_path,index_rate, filter_radius=filter_radius, resample_sr=resample_sr, rms_mix_rate=rms_mix_rate, protect=protect)\n","        write_wav(out_filepath.replace('.wav', f'_{i}') + '.wav', SAMPLE_RATE, audio_array)\n","    all_parts.append(audio_array)\n","\n","audio_array = np.concatenate(all_parts, axis=-1)\n","\n","# save audio\n","write_wav(out_filepath, SAMPLE_RATE, audio_array)\n","\n","# play audio\n","Audio(audio_array, rate=SAMPLE_RATE)"]},{"cell_type":"markdown","metadata":{"id":"9d6knlB7zH9h"},"source":["# Import"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":34914,"status":"ok","timestamp":1701547530061,"user":{"displayName":"serah K","userId":"08070710945457856845"},"user_tz":-540},"id":"2-woboyU7wNI","outputId":"4b0fa5d3-c38d-4c87-836a-9aa0f20ca326"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting librosa\n","  Using cached librosa-0.10.1-py3-none-any.whl (253 kB)\n","Collecting audioread>=2.1.9 (from librosa)\n","  Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n","Collecting numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 (from librosa)\n","  Using cached numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","Collecting scipy>=1.2.0 (from librosa)\n","  Using cached scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n","Collecting scikit-learn>=0.20.0 (from librosa)\n","  Using cached scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n","Collecting joblib>=0.14 (from librosa)\n","  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n","Collecting decorator>=4.3.0 (from librosa)\n","  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n","Collecting numba>=0.51.0 (from librosa)\n","  Using cached numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n","Collecting soundfile>=0.12.1 (from librosa)\n","  Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n","Collecting pooch>=1.0 (from librosa)\n","  Using cached pooch-1.8.0-py3-none-any.whl (62 kB)\n","Collecting soxr>=0.3.2 (from librosa)\n","  Using cached soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","Collecting typing-extensions>=4.1.1 (from librosa)\n","  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Collecting lazy-loader>=0.1 (from librosa)\n","  Using cached lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n","Collecting msgpack>=1.0 (from librosa)\n","  Using cached msgpack-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (530 kB)\n","Collecting llvmlite<0.42,>=0.41.0dev0 (from numba>=0.51.0->librosa)\n","  Using cached llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n","Collecting platformdirs>=2.5.0 (from pooch>=1.0->librosa)\n","  Using cached platformdirs-4.0.0-py3-none-any.whl (17 kB)\n","Collecting packaging>=20.0 (from pooch>=1.0->librosa)\n","  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n","Collecting requests>=2.19.0 (from pooch>=1.0->librosa)\n","  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n","Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.20.0->librosa)\n","  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n","Collecting cffi>=1.0 (from soundfile>=0.12.1->librosa)\n","  Using cached cffi-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n","Collecting pycparser (from cffi>=1.0->soundfile>=0.12.1->librosa)\n","  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n","Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->pooch>=1.0->librosa)\n","  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n","Collecting idna<4,>=2.5 (from requests>=2.19.0->pooch>=1.0->librosa)\n","  Using cached idna-3.6-py3-none-any.whl (61 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->pooch>=1.0->librosa)\n","  Using cached urllib3-2.1.0-py3-none-any.whl (104 kB)\n","Collecting certifi>=2017.4.17 (from requests>=2.19.0->pooch>=1.0->librosa)\n","  Using cached certifi-2023.11.17-py3-none-any.whl (162 kB)\n","Installing collected packages: urllib3, typing-extensions, threadpoolctl, pycparser, platformdirs, packaging, numpy, msgpack, llvmlite, lazy-loader, joblib, idna, decorator, charset-normalizer, certifi, audioread, soxr, scipy, requests, numba, cffi, soundfile, scikit-learn, pooch, librosa\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.2 which is incompatible.\n","moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed audioread-3.0.1 certifi-2023.11.17 cffi-1.16.0 charset-normalizer-3.3.2 decorator-4.4.2 idna-3.6 joblib-1.3.2 lazy-loader-0.3 librosa-0.10.1 llvmlite-0.41.1 msgpack-1.0.7 numba-0.58.1 numpy-1.23.5 packaging-23.2 platformdirs-4.0.0 pooch-1.8.0 pycparser-2.21 requests-2.31.0 scikit-learn-1.2.2 scipy-1.11.4 soundfile-0.12.1 soxr-0.3.7 threadpoolctl-3.2.0 typing-extensions-4.5.0 urllib3-2.0.7\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["_cffi_backend","_soundfile","_soundfile_data","certifi","cffi","charset_normalizer","decorator","joblib","librosa","llvmlite","msgpack","numba","numpy","pycparser","requests","sklearn","soundfile","soxr"]}}},"metadata":{}}],"source":["!pip install --ignore-installed librosa"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1EHksGhJOAM"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import librosa, librosa.display\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2670,"status":"ok","timestamp":1701547545107,"user":{"displayName":"serah K","userId":"08070710945457856845"},"user_tz":-540},"id":"2iOTHNZmI8NI","outputId":"e65d5ff1-6ee3-4391-a83e-aaa76f5e01d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"OBCUViZLtJNr"},"source":["# Y dataset 행 표준화"]},{"cell_type":"markdown","source":["이런 식으로 Y dataset 생성"],"metadata":{"id":"WXjLsWl6AGb2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kue3yx1_5u94"},"outputs":[],"source":["'''\n","import cv2\n","import dlib\n","import pandas as pd\n","\n","# Dlib의 얼굴 검출기와 랜드마크 예측기 초기화\n","detector = dlib.get_frontal_face_detector()\n","predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n","\n","# 동영상 파일 불러오기\n","video_path = \"/content/gdrive//MyDrive/Wav2Lip/mp4/lip_J_2_M_05_C221_A_003_1.mp4\"\n","video_capture = cv2.VideoCapture(video_path)\n","\n","# 총 프레임 수 확인\n","total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","\n","# 데이터프레임 초기화\n","columns = [f\"Landmark_{i}_{j}\" for i in range(1, 21) for j in [\"X\", \"Y\"]]\n","df03 = pd.DataFrame(columns=columns)\n","\n","# 현재 프레임 초기화\n","current_frame = 0\n","\n","while current_frame < total_frames:\n","    # 현재 프레임에서 1000개의 프레임 수집\n","    for i in range(total_frames):\n","        ret, frame = video_capture.read()\n","        if not ret:\n","            break\n","\n","        # 얼굴 검출\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        faces = detector(gray)\n","\n","        for face in faces:\n","            # 얼굴에서 랜드마크(특징점) 찾기\n","            landmarks = predictor(gray, face)\n","\n","            # 입술 좌표 추출 (랜드마크 48-68은 입술에 해당)\n","            lips_points = landmarks.parts()[48:68]\n","\n","            # 좌표를 리스트에 추가\n","            coordinates = [point.x for point in lips_points] + [point.y for point in lips_points]\n","            df03 = df03.append(pd.Series(coordinates, index=df02.columns), ignore_index=True)\n","\n","        current_frame += 1\n","\n","# 사용이 끝난 객체들을 해제\n","video_capture.release()\n","\n","# 데이터프레임 확인\n","#print(df.head())\n","'''"]},{"cell_type":"code","source":["df1 = pd.read_csv(\"/content/gdrive//MyDrive/Wav2Lip/csv/lip_J_1_F_02_C032_A_010.csv\",index_col=0)\n","# x 좌표와 y 좌표 구분\n","x_cols = df1.iloc[:, :20]  # x 열\n","y_cols = df1.iloc[:, 20:]  # y 열\n","\n","x_cols['mean_x']=x_cols.mean(axis=1)\n","x_cols['std_x']=x_cols.std(axis=1)\n","df1.iloc[:, :20]=(x_cols.iloc[:, :20].sub(x_cols['mean_x'],axis=0)).div(x_cols['std_x'],axis=0)\n","\n","y_cols['mean_y']=y_cols.mean(axis=1)\n","y_cols['std_y']=y_cols.std(axis=1)\n","df1.iloc[:, 20:]=(y_cols.iloc[:, :20].sub(y_cols['mean_y'],axis=0)).div(y_cols['std_y'],axis=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701547548854,"user_tz":-540,"elapsed":5,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"025c4d68-8d64-48d6-b765-c84dab6e7a86","id":"12EXrkWP142M"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-d0681a3bf214>:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  x_cols['mean_x']=x_cols.mean(axis=1)\n","<ipython-input-4-d0681a3bf214>:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  x_cols['std_x']=x_cols.std(axis=1)\n","<ipython-input-4-d0681a3bf214>:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  y_cols['mean_y']=y_cols.mean(axis=1)\n","<ipython-input-4-d0681a3bf214>:11: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  y_cols['std_y']=y_cols.std(axis=1)\n"]}]},{"cell_type":"code","source":["df2 = pd.read_csv(\"/content/gdrive//MyDrive/Wav2Lip/csv/lip_J_1_F_02_C032_A_011.csv\",index_col=0)\n","# x 좌표와 y 좌표 구분\n","x_cols = df2.iloc[:, :20]  # x 열\n","y_cols = df2.iloc[:, 20:]  # y 열\n","\n","x_cols['mean_x']=x_cols.mean(axis=1)\n","x_cols['std_x']=x_cols.std(axis=1)\n","df2.iloc[:, :20]=(x_cols.iloc[:, :20].sub(x_cols['mean_x'],axis=0)).div(x_cols['std_x'],axis=0)\n","\n","y_cols['mean_y']=y_cols.mean(axis=1)\n","y_cols['std_y']=y_cols.std(axis=1)\n","df2.iloc[:, 20:]=(y_cols.iloc[:, :20].sub(y_cols['mean_y'],axis=0)).div(y_cols['std_y'],axis=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701547553347,"user_tz":-540,"elapsed":889,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"57d36b86-e894-4ebb-a859-7ea482df8a91","id":"opI2K58q15G0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-bd55252f8f7f>:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  x_cols['mean_x']=x_cols.mean(axis=1)\n","<ipython-input-5-bd55252f8f7f>:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  x_cols['std_x']=x_cols.std(axis=1)\n","<ipython-input-5-bd55252f8f7f>:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  y_cols['mean_y']=y_cols.mean(axis=1)\n","<ipython-input-5-bd55252f8f7f>:11: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  y_cols['std_y']=y_cols.std(axis=1)\n"]}]},{"cell_type":"code","source":["df3 = pd.read_csv(\"/content/gdrive//MyDrive/Wav2Lip/csv/lip_J_2_M_05_C221_A_002.csv\",index_col=0)\n","# x 좌표와 y 좌표 구분\n","x_cols = df3.iloc[:, :20]  # x 열\n","y_cols = df3.iloc[:, 20:]  # y 열\n","\n","x_cols['mean_x']=x_cols.mean(axis=1)\n","x_cols['std_x']=x_cols.std(axis=1)\n","df3.iloc[:, :20]=(x_cols.iloc[:, :20].sub(x_cols['mean_x'],axis=0)).div(x_cols['std_x'],axis=0)\n","\n","y_cols['mean_y']=y_cols.mean(axis=1)\n","y_cols['std_y']=y_cols.std(axis=1)\n","df3.iloc[:, 20:]=(y_cols.iloc[:, :20].sub(y_cols['mean_y'],axis=0)).div(y_cols['std_y'],axis=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AFKd6Z51wY5K","executionInfo":{"status":"ok","timestamp":1701547555054,"user_tz":-540,"elapsed":444,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"845fee7b-e5e4-430c-fc5c-34a319c33228"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-b4fe0a21b5a3>:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  x_cols['mean_x']=x_cols.mean(axis=1)\n","<ipython-input-6-b4fe0a21b5a3>:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  x_cols['std_x']=x_cols.std(axis=1)\n","<ipython-input-6-b4fe0a21b5a3>:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  y_cols['mean_y']=y_cols.mean(axis=1)\n","<ipython-input-6-b4fe0a21b5a3>:11: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  y_cols['std_y']=y_cols.std(axis=1)\n"]}]},{"cell_type":"code","source":["df4 = pd.read_csv(\"/content/gdrive//MyDrive/Wav2Lip/csv/lip_J_2_M_05_C221_A_003.csv\",index_col=0)\n","# x 좌표와 y 좌표 구분\n","x_cols = df4.iloc[:, :20]  # x 열\n","y_cols = df4.iloc[:, 20:]  # y 열\n","\n","x_cols['mean_x']=x_cols.mean(axis=1)\n","x_cols['std_x']=x_cols.std(axis=1)\n","df4.iloc[:, :20]=(x_cols.iloc[:, :20].sub(x_cols['mean_x'],axis=0)).div(x_cols['std_x'],axis=0)\n","\n","y_cols['mean_y']=y_cols.mean(axis=1)\n","y_cols['std_y']=y_cols.std(axis=1)\n","df4.iloc[:, 20:]=(y_cols.iloc[:, :20].sub(y_cols['mean_y'],axis=0)).div(y_cols['std_y'],axis=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701547557823,"user_tz":-540,"elapsed":948,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"d08595d7-5aa5-4e9d-a9e7-972e0fe3e7c1","id":"gEwSGseT16d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-c23fcc85a4e4>:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  x_cols['mean_x']=x_cols.mean(axis=1)\n","<ipython-input-7-c23fcc85a4e4>:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  x_cols['std_x']=x_cols.std(axis=1)\n","<ipython-input-7-c23fcc85a4e4>:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  y_cols['mean_y']=y_cols.mean(axis=1)\n","<ipython-input-7-c23fcc85a4e4>:11: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  y_cols['std_y']=y_cols.std(axis=1)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CsR0rn1q08gF"},"outputs":[],"source":["Y=np.concatenate([df1,df2,df3,df4], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701547561848,"user":{"displayName":"serah K","userId":"08070710945457856845"},"user_tz":-540},"outputId":"e0768d1e-2466-44bf-e279-647d9d24b0b4","id":"CFVJY0Co08gG"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(36523, 40)"]},"metadata":{},"execution_count":9}],"source":["Y.shape"]},{"cell_type":"markdown","source":["# X dataset 만들기"],"metadata":{"id":"zq2nkohK0Eb9"}},{"cell_type":"markdown","source":["wav -> mfcc"],"metadata":{"id":"nNvJAdER_qme"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TL9mMId5JOEd"},"outputs":[],"source":["file1 = \"/content/gdrive//MyDrive/Wav2Lip/wav/lip_J_1_F_02_C032_A_010.wav\"\n","file2 = \"/content/gdrive//MyDrive/Wav2Lip/wav/lip_J_1_F_02_C032_A_011.wav\"\n","file3 = \"/content/gdrive//MyDrive/Wav2Lip/wav/lip_J_2_M_05_C221_A_002.wav\"\n","file4 = \"/content/gdrive//MyDrive/Wav2Lip/wav/lip_J_2_M_05_C221_A_003.wav\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PskISnx8JOGs"},"outputs":[],"source":["# load audio file with Librosa\n","sig1, sr1 = librosa.load(file1, sr=16000)    # 16,000 Hz는 음성 신호에 대한 효과적인 표현을 제공하는 일반적인 값\n","sig2, sr2 = librosa.load(file2, sr=16000)\n","sig3, sr3 = librosa.load(file3, sr=16000)\n","sig4, sr4 = librosa.load(file4, sr=16000)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3459,"status":"ok","timestamp":1701547587112,"user":{"displayName":"serah K","userId":"08070710945457856845"},"user_tz":-540},"outputId":"01f9eed5-c5af-4b91-c65b-16e5b5202f9d","id":"oY7weH6P0e65"},"outputs":[{"output_type":"stream","name":"stdout","text":["2D 행렬의 형태: (9128, 20)\n"]}],"source":["# WAV 파일 읽기\n","y, sr = librosa.load(file1)\n","\n","# MFCCs 추출\n","n_mfcc = 20  # MFCCs의 차원 수\n","n_fft = 2048  # FFT 크기\n","hop_length = 512  # 프레임 이동 간격\n","\n","# MFCCs 추출\n","mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n","\n","# 원하는 행의 개수를 지정\n","num_rows = df1.shape[0]\n","\n","# MFCCs 데이터를 2차원 행렬로 변환\n","mfcc_matrix1 = np.empty((0, n_mfcc))\n","\n","# MFCCs 데이터를 행 단위로 추가\n","for i in range(num_rows):\n","    start_frame = i * (mfccs.shape[1] // num_rows)\n","    end_frame = (i + 1) * (mfccs.shape[1] // num_rows)\n","    mfcc_sequence = mfccs[:, start_frame:end_frame]\n","    mfcc_matrix1 = np.vstack((mfcc_matrix1, mfcc_sequence.T))  # .T를 통해 전치하여 (n_mfcc, 프레임 수) 형태로\n","\n","# 결과 확인\n","print(\"2D 행렬의 형태:\", mfcc_matrix1.shape)  # (9129, n_mfcc)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2061,"status":"ok","timestamp":1701547592299,"user":{"displayName":"serah K","userId":"08070710945457856845"},"user_tz":-540},"id":"agbBMKrf0e65","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eff02a1d-f88c-4516-a5a3-b2c9a9a4da18"},"outputs":[{"output_type":"stream","name":"stdout","text":["2D 행렬의 형태: (9195, 20)\n"]}],"source":["# WAV 파일 읽기\n","y, sr = librosa.load(file2)\n","\n","# MFCCs 추출\n","n_mfcc = 20  # MFCCs의 차원 수\n","n_fft = 2048  # FFT 크기\n","hop_length = 512  # 프레임 이동 간격\n","\n","# MFCCs 추출\n","mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n","\n","# 원하는 행의 개수를 지정\n","num_rows = df2.shape[0]\n","\n","# MFCCs 데이터를 2차원 행렬로 변환\n","mfcc_matrix2 = np.empty((0, n_mfcc))\n","\n","# MFCCs 데이터를 행 단위로 추가\n","for i in range(num_rows):\n","    start_frame = i * (mfccs.shape[1] // num_rows)\n","    end_frame = (i + 1) * (mfccs.shape[1] // num_rows)\n","    mfcc_sequence = mfccs[:, start_frame:end_frame]\n","    mfcc_matrix2 = np.vstack((mfcc_matrix2, mfcc_sequence.T))  # .T를 통해 전치하여 (n_mfcc, 프레임 수) 형태로\n","\n","# 결과 확인\n","print(\"2D 행렬의 형태:\", mfcc_matrix2.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2215,"status":"ok","timestamp":1701547595664,"user":{"displayName":"serah K","userId":"08070710945457856845"},"user_tz":-540},"id":"GoYVRFhN0e66","colab":{"base_uri":"https://localhost:8080/"},"outputId":"670310f0-b615-472e-a97f-fcbd8cdf476c"},"outputs":[{"output_type":"stream","name":"stdout","text":["2D 행렬의 형태: (9098, 20)\n"]}],"source":["# WAV 파일 읽기\n","y, sr = librosa.load(file3)\n","\n","# MFCCs 추출\n","n_mfcc = 20  # MFCCs의 차원 수\n","n_fft = 2048  # FFT 크기\n","hop_length = 512  # 프레임 이동 간격\n","\n","# MFCCs 추출\n","mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n","\n","# 원하는 행의 개수를 지정\n","num_rows = df3.shape[0]\n","\n","# MFCCs 데이터를 2차원 행렬로 변환\n","mfcc_matrix3 = np.empty((0, n_mfcc))\n","\n","# MFCCs 데이터를 행 단위로 추가\n","for i in range(num_rows):\n","    start_frame = i * (mfccs.shape[1] // num_rows)\n","    end_frame = (i + 1) * (mfccs.shape[1] // num_rows)\n","    mfcc_sequence = mfccs[:, start_frame:end_frame]\n","    mfcc_matrix3 = np.vstack((mfcc_matrix3, mfcc_sequence.T))  # .T를 통해 전치하여 (n_mfcc, 프레임 수) 형태로\n","\n","# 결과 확인\n","print(\"2D 행렬의 형태:\", mfcc_matrix3.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1698,"status":"ok","timestamp":1701547598713,"user":{"displayName":"serah K","userId":"08070710945457856845"},"user_tz":-540},"id":"cRy3hSfh0e66","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c271fda3-4995-42be-fd79-438a56c46564"},"outputs":[{"output_type":"stream","name":"stdout","text":["2D 행렬의 형태: (9102, 20)\n"]}],"source":["# WAV 파일 읽기\n","y, sr = librosa.load(file4)\n","\n","# MFCCs 추출\n","n_mfcc = 20  # MFCCs의 차원 수\n","n_fft = 2048  # FFT 크기\n","hop_length = 512  # 프레임 이동 간격\n","\n","# MFCCs 추출\n","mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n","\n","# 원하는 행의 개수를 지정\n","num_rows = df4.shape[0]\n","\n","# MFCCs 데이터를 2차원 행렬로 변환\n","mfcc_matrix4 = np.empty((0, n_mfcc))\n","\n","# MFCCs 데이터를 행 단위로 추가\n","for i in range(num_rows):\n","    start_frame = i * (mfccs.shape[1] // num_rows)\n","    end_frame = (i + 1) * (mfccs.shape[1] // num_rows)\n","    mfcc_sequence = mfccs[:, start_frame:end_frame]\n","    mfcc_matrix4 = np.vstack((mfcc_matrix4, mfcc_sequence.T))  # .T를 통해 전치하여 (n_mfcc, 프레임 수) 형태로\n","\n","# 결과 확인\n","print(\"2D 행렬의 형태:\", mfcc_matrix4.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAOBdslE0e66"},"outputs":[],"source":["X=np.concatenate([mfcc_matrix1,mfcc_matrix2,mfcc_matrix3,mfcc_matrix4], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701547602064,"user":{"displayName":"serah K","userId":"08070710945457856845"},"user_tz":-540},"id":"L8OXlKK30e67","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0c4e9f6a-cc6e-4c7f-cd14-e22a5d662b04"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(36523, 20)"]},"metadata":{},"execution_count":17}],"source":["X.shape"]},{"cell_type":"markdown","metadata":{"id":"7OKKllHeqheY"},"source":["# 모델 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xak0q3b9VQsg"},"outputs":[],"source":["import tensorflow"]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Bidirectional, LSTM\n","from sklearn.metrics import r2_score\n","\n","# 데이터를 훈련 세트와 테스트 세트로 나누기\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","# Reshape input data for LSTM layer\n","X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n","X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n","\n","# 모델 생성\n","model = Sequential()\n","\n","# LSTM Layer\n","model.add(LSTM(32, input_shape=(X_train.shape[1], 1), activation='relu'))\n","\n","# Fully Connected (FC) Layer\n","model.add(Dense(20, activation='relu'))\n","\n","# Output Layer\n","model.add(Dense(40))  # 출력 레이어의 노드 수는 출력의 차원과 일치\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n"],"metadata":{"id":"rKzEUl4kglX2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 훈련\n","history=model.fit(X_train_reshaped, Y_train, batch_size=64, epochs=30, validation_split=0.2, verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PmfxIfPtyPcN","executionInfo":{"status":"ok","timestamp":1701547761427,"user_tz":-540,"elapsed":151279,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"70e2216b-5db2-4412-d193-825767a26bc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","366/366 [==============================] - 7s 15ms/step - loss: 0.7197 - mse: 0.7197 - val_loss: 0.0153 - val_mse: 0.0153\n","Epoch 2/30\n","366/366 [==============================] - 5s 14ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.0134 - val_mse: 0.0134\n","Epoch 3/30\n","366/366 [==============================] - 4s 12ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.0129 - val_mse: 0.0129\n","Epoch 4/30\n","366/366 [==============================] - 4s 11ms/step - loss: 0.0127 - mse: 0.0127 - val_loss: 0.0124 - val_mse: 0.0124\n","Epoch 5/30\n","366/366 [==============================] - 6s 17ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.0110 - val_mse: 0.0110\n","Epoch 6/30\n","366/366 [==============================] - 5s 15ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0092 - val_mse: 0.0092\n","Epoch 7/30\n","366/366 [==============================] - 8s 22ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0086 - val_mse: 0.0086\n","Epoch 8/30\n","366/366 [==============================] - 8s 22ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0083 - val_mse: 0.0083\n","Epoch 9/30\n","366/366 [==============================] - 4s 11ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0082 - val_mse: 0.0082\n","Epoch 10/30\n","366/366 [==============================] - 4s 11ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0081 - val_mse: 0.0081\n","Epoch 11/30\n","366/366 [==============================] - 5s 15ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0081 - val_mse: 0.0081\n","Epoch 12/30\n","366/366 [==============================] - 5s 13ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.0081 - val_mse: 0.0081\n","Epoch 13/30\n","366/366 [==============================] - 4s 11ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0079 - val_mse: 0.0079\n","Epoch 14/30\n","366/366 [==============================] - 4s 11ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0079 - val_mse: 0.0079\n","Epoch 15/30\n","366/366 [==============================] - 6s 16ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0078 - val_mse: 0.0078\n","Epoch 16/30\n","366/366 [==============================] - 4s 11ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0078 - val_mse: 0.0078\n","Epoch 17/30\n","366/366 [==============================] - 4s 11ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0078 - val_mse: 0.0078\n","Epoch 18/30\n","366/366 [==============================] - 5s 13ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0079 - val_mse: 0.0079\n","Epoch 19/30\n","366/366 [==============================] - 6s 16ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0077 - val_mse: 0.0077\n","Epoch 20/30\n","366/366 [==============================] - 4s 12ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0077 - val_mse: 0.0077\n","Epoch 21/30\n","366/366 [==============================] - 4s 12ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0078 - val_mse: 0.0078\n","Epoch 22/30\n","366/366 [==============================] - 6s 16ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0078 - val_mse: 0.0078\n","Epoch 23/30\n","366/366 [==============================] - 4s 12ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0079 - val_mse: 0.0079\n","Epoch 24/30\n","366/366 [==============================] - 4s 12ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0078 - val_mse: 0.0078\n","Epoch 25/30\n","366/366 [==============================] - 5s 13ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0078 - val_mse: 0.0078\n","Epoch 26/30\n","366/366 [==============================] - 6s 15ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0077 - val_mse: 0.0077\n","Epoch 27/30\n","366/366 [==============================] - 4s 12ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0078 - val_mse: 0.0078\n","Epoch 28/30\n","366/366 [==============================] - 4s 11ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0078 - val_mse: 0.0078\n","Epoch 29/30\n","366/366 [==============================] - 6s 15ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0077 - val_mse: 0.0077\n","Epoch 30/30\n","366/366 [==============================] - 5s 13ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0077 - val_mse: 0.0077\n"]}]},{"cell_type":"markdown","source":["# 모델 평가"],"metadata":{"id":"l4mzd64m2nOE"}},{"cell_type":"code","source":["res=model.evaluate(X_test_reshaped, Y_test, verbose=1)\n","print(f'테스트 세트의 MSE : {round(res[1], 4)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uqxP9M_k2uEI","executionInfo":{"status":"ok","timestamp":1701547772930,"user_tz":-540,"elapsed":2110,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"5547d651-4b87-4c62-8398-893b5f7fe4f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["229/229 [==============================] - 1s 5ms/step - loss: 0.0078 - mse: 0.0078\n","테스트 세트의 MSE : 0.0078\n"]}]},{"cell_type":"code","source":["# R_square\n","from sklearn.metrics import r2_score\n","score = r2_score(Y_test, model.predict(X_test_reshaped))\n","score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pDKecEZ19JK","executionInfo":{"status":"ok","timestamp":1701547775509,"user_tz":-540,"elapsed":2587,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"6369c84f-ae82-42fe-9df6-d1bc323914fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["229/229 [==============================] - 1s 5ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["0.28855612970163635"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["# plot\n","import matplotlib.pyplot as plt\n","\n","plt.scatter(Y_test, model.predict(X_test_reshaped))\n","plt.xlabel(\"True Values $Y$\")\n","plt.ylabel(\"Predicted Values $\\hat{Y}$\")\n","plt.axis(\"equal\")\n","plt.axis(\"square\")\n","plt.plot(Y_test, Y_test)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"id":"ZCPNyy4n19Lq","executionInfo":{"status":"ok","timestamp":1701547780516,"user_tz":-540,"elapsed":2609,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"f56ae758-7390-4b31-907b-31bbb6f07db5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["229/229 [==============================] - 1s 3ms/step\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["# 적용시킬 목소리 파일 입력받기"],"metadata":{"id":"E9qKSvZ02lw6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TcO-H5uUsKQi"},"outputs":[],"source":["out_filepath = '/content/audio/audio.wav'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aGxsbWD0sKQv","executionInfo":{"status":"ok","timestamp":1701547789190,"user_tz":-540,"elapsed":4,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"274fe79c-28dd-4401-8d44-e7cbf982a60f"},"outputs":[{"output_type":"stream","name":"stdout","text":["2D 행렬의 형태: (329, 20)\n"]}],"source":["# WAV 파일 읽기\n","y, sr = librosa.load(out_filepath)\n","\n","# MFCCs 추출\n","n_mfcc = 20  # MFCCs의 차원 수\n","n_fft = 2048  # FFT 크기\n","hop_length = 512  # 프레임 이동 간격\n","\n","# MFCCs 추출\n","mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n","\n","# 원하는 행의 개수를 지정\n","num_rows = 329\n","# MFCCs 데이터를 2차원 행렬로 변환\n","mfcc_matrix_ = np.empty((0, n_mfcc))\n","\n","# MFCCs 데이터를 행 단위로 추가\n","for i in range(num_rows):\n","    start_frame = i * (mfccs.shape[1] // num_rows)\n","    end_frame = (i + 1) * (mfccs.shape[1] // num_rows)\n","    mfcc_sequence = mfccs[:, start_frame:end_frame]\n","    mfcc_matrix_ = np.vstack((mfcc_matrix_, mfcc_sequence.T))  # .T를 통해 전치하여 (n_mfcc, 프레임 수) 형태로\n","\n","# 결과 확인\n","print(\"2D 행렬의 형태:\", mfcc_matrix_.shape)"]},{"cell_type":"code","source":["mfcc_matrix_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24ewhKszDLh6","executionInfo":{"status":"ok","timestamp":1701547790440,"user_tz":-540,"elapsed":5,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"2cc5cc20-6354-40df-9587-e3618417393f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-3.89145142e+02,  9.92002411e+01,  2.76201859e+01, ...,\n","         4.91820574e+00,  7.26293278e+00,  1.33049905e+00],\n","       [-2.76436981e+02,  6.17430573e+01,  1.96106129e+01, ...,\n","         2.01745758e+01,  4.54727173e+00,  8.67536783e-01],\n","       [-1.93924576e+02,  4.58624725e+01,  1.37431660e+01, ...,\n","         2.15107250e+01,  6.24473453e-01, -3.28706980e+00],\n","       ...,\n","       [-2.04391617e+02,  1.11690598e+02, -1.71410294e+01, ...,\n","         1.22319403e+01, -7.06464481e+00, -1.35376811e+00],\n","       [-1.88997131e+02,  1.46041946e+02, -2.72662210e+00, ...,\n","         4.08183479e+00, -8.46849680e-01,  1.72464252e-02],\n","       [-1.35380219e+02,  1.44040054e+02, -8.58210564e+00, ...,\n","         4.54561234e+00, -2.09507537e+00,  1.15694189e+00]])"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHwIwiZ-TLxk"},"outputs":[],"source":["y=model(mfcc_matrix_)             # 예측된 표준화 좌표"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBJmQ9lFs4Oq","executionInfo":{"status":"ok","timestamp":1701547795504,"user_tz":-540,"elapsed":2,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"47648f21-2731-459e-e3ce-604cb8c3f678"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(329, 40), dtype=float32, numpy=\n","array([[-1.9588535 , -1.219673  , -0.5033467 , ...,  0.1194309 ,\n","         0.28040504,  0.23261008],\n","       [-2.0151355 , -1.2510682 , -0.5280951 , ...,  0.14859791,\n","         0.35406372,  0.30300468],\n","       [-2.0177135 , -1.2558832 , -0.523826  , ...,  0.15972678,\n","         0.3604304 ,  0.3072246 ],\n","       ...,\n","       [-1.9837039 , -1.2276936 , -0.51248914, ...,  0.13453455,\n","         0.33091655,  0.2936641 ],\n","       [-1.9892266 , -1.2328713 , -0.5183383 , ...,  0.13007206,\n","         0.32270828,  0.27839684],\n","       [-1.9834273 , -1.227984  , -0.51902807, ...,  0.12357557,\n","         0.31760716,  0.27532226]], dtype=float32)>"]},"metadata":{},"execution_count":28}],"source":["y"]},{"cell_type":"markdown","source":["# 적용할 동영상 프레임 단위로 랜드마크 데이터프레임화"],"metadata":{"id":"xHw_ky8a6DR2"}},{"cell_type":"code","source":["df = pd.read_csv(\"/content/gdrive//MyDrive/Wav2Lip/csv/KakaoTalk_20231119_220036247.csv\",index_col=0)    # 딥페이크 영상 프레임 단위로 랜드마크 저장"],"metadata":{"id":"1iKOVjgDykBU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 그 중 기준이 되는 프레임1\n","d=df.iloc[319,:]"],"metadata":{"id":"05rs__Ni4wh1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L0uP2YKLy1Ef"},"outputs":[],"source":["# x 좌표와 y 좌표 구분\n","x_cols = d[:20]  # x 열\n","y_cols = d[20:]  # y 열\n","\n","import numpy as np\n","mx=np.mean(x_cols)\n","my=np.mean(y_cols)\n","sx=np.std(x_cols)\n","sy=np.std(y_cols)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcI5AYzhy67I"},"outputs":[],"source":["y=np.copy(y)\n","y_df = pd.DataFrame(y, columns=df3.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXq84geGzIwK"},"outputs":[],"source":["# 기준 프레임에 역표준화하여 적용시킴\n","y_df.iloc[:, :20]=(y_df.iloc[:, :20].mul(sx)).add(mx)\n","y_df.iloc[:, 20:]=(y_df.iloc[:, 20:].mul(sy)).add(my)"]},{"cell_type":"code","source":["y_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"MjJOZYTnzPuG","executionInfo":{"status":"ok","timestamp":1701547816769,"user_tz":-540,"elapsed":556,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"676b2973-dc32-48cc-ff04-bec24ab946a2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Landmark_1_X  Landmark_2_X  Landmark_3_X  Landmark_4_X  Landmark_5_X  \\\n","0      610.283508    621.761292    632.884216    640.581848    648.343933   \n","1      609.409546    621.273804    632.499878    640.566040    648.535950   \n","2      609.369507    621.199036    632.566162    640.684326    648.460999   \n","3      609.906738    621.519897    632.727295    640.646729    648.327515   \n","4      609.749146    621.440735    632.648438    640.578003    648.394531   \n","..            ...           ...           ...           ...           ...   \n","324    611.022461    621.915833    633.452698    640.907898    648.139343   \n","325    610.946350    621.848877    633.436768    640.937134    648.150940   \n","326    609.897644    621.636719    632.742249    640.519226    648.278137   \n","327    609.811890    621.556335    632.651428    640.495605    648.436218   \n","328    609.901917    621.632202    632.640686    640.448242    648.472656   \n","\n","     Landmark_6_X  Landmark_7_X  Landmark_8_X  Landmark_9_X  Landmark_10_X  \\\n","0      659.051208    668.801758    659.906189    650.205933     641.899719   \n","1      659.192505    668.500610    660.436096    650.920471     642.177063   \n","2      659.223328    668.592346    660.432373    651.100769     642.158447   \n","3      658.964294    668.349487    660.098999    650.847229     642.088013   \n","4      658.992676    668.410034    660.196899    650.778625     642.025146   \n","..            ...           ...           ...           ...            ...   \n","324    659.169189    669.763000    659.402527    649.404053     641.106445   \n","325    659.192078    669.764160    659.443054    649.471008     641.089905   \n","326    659.000671    668.662781    660.109985    650.703186     642.176514   \n","327    659.088989    668.625122    660.195801    650.563171     642.110474   \n","328    659.077759    668.571594    660.162354    650.439819     642.131836   \n","\n","     ...  Landmark_11_Y  Landmark_12_Y  Landmark_13_Y  Landmark_14_Y  \\\n","0    ...     308.437622     302.822388     289.617523     285.055908   \n","1    ...     309.233521     303.600311     290.216400     285.401459   \n","2    ...     309.322327     303.610413     290.103088     285.410645   \n","3    ...     308.913910     303.289612     289.962982     285.395813   \n","4    ...     308.905426     303.313843     290.014465     285.349060   \n","..   ...            ...            ...            ...            ...   \n","324  ...     307.369171     301.561066     288.453735     284.328735   \n","325  ...     307.413727     301.587860     288.464264     284.337708   \n","326  ...     308.970886     303.364960     289.890320     285.320160   \n","327  ...     308.897095     303.311646     289.992737     285.266663   \n","328  ...     308.822296     303.271027     290.033020     285.256653   \n","\n","     Landmark_15_Y  Landmark_16_Y  Landmark_17_Y  Landmark_18_Y  \\\n","0       285.601013     283.890045     286.112213     290.374756   \n","1       285.760101     283.646149     284.102081     290.710480   \n","2       285.664948     283.755920     283.856293     290.838593   \n","3       285.692566     283.840820     284.394409     290.729156   \n","4       285.719727     283.752228     284.595642     290.670898   \n","..             ...            ...            ...            ...   \n","324     285.015686     284.435028     289.217896     290.145538   \n","325     284.995544     284.443848     289.095184     290.194061   \n","326     285.745758     283.758514     284.791077     290.548615   \n","327     285.754578     283.693634     285.020538     290.497253   \n","328     285.795349     283.659271     285.190918     290.422455   \n","\n","     Landmark_19_Y  Landmark_20_Y  \n","0       292.227692     291.677551  \n","1       293.075592     292.487854  \n","2       293.148865     292.536407  \n","3       292.901794     292.327271  \n","4       292.837250     292.223450  \n","..             ...            ...  \n","324     290.840546     290.015533  \n","325     290.881927     290.025818  \n","326     292.809143     292.380341  \n","327     292.714661     292.204590  \n","328     292.655945     292.169189  \n","\n","[329 rows x 40 columns]"],"text/html":["\n","  <div id=\"df-f08f3cfe-e6c0-4939-9d42-f4aa2144fe20\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Landmark_1_X</th>\n","      <th>Landmark_2_X</th>\n","      <th>Landmark_3_X</th>\n","      <th>Landmark_4_X</th>\n","      <th>Landmark_5_X</th>\n","      <th>Landmark_6_X</th>\n","      <th>Landmark_7_X</th>\n","      <th>Landmark_8_X</th>\n","      <th>Landmark_9_X</th>\n","      <th>Landmark_10_X</th>\n","      <th>...</th>\n","      <th>Landmark_11_Y</th>\n","      <th>Landmark_12_Y</th>\n","      <th>Landmark_13_Y</th>\n","      <th>Landmark_14_Y</th>\n","      <th>Landmark_15_Y</th>\n","      <th>Landmark_16_Y</th>\n","      <th>Landmark_17_Y</th>\n","      <th>Landmark_18_Y</th>\n","      <th>Landmark_19_Y</th>\n","      <th>Landmark_20_Y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>610.283508</td>\n","      <td>621.761292</td>\n","      <td>632.884216</td>\n","      <td>640.581848</td>\n","      <td>648.343933</td>\n","      <td>659.051208</td>\n","      <td>668.801758</td>\n","      <td>659.906189</td>\n","      <td>650.205933</td>\n","      <td>641.899719</td>\n","      <td>...</td>\n","      <td>308.437622</td>\n","      <td>302.822388</td>\n","      <td>289.617523</td>\n","      <td>285.055908</td>\n","      <td>285.601013</td>\n","      <td>283.890045</td>\n","      <td>286.112213</td>\n","      <td>290.374756</td>\n","      <td>292.227692</td>\n","      <td>291.677551</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>609.409546</td>\n","      <td>621.273804</td>\n","      <td>632.499878</td>\n","      <td>640.566040</td>\n","      <td>648.535950</td>\n","      <td>659.192505</td>\n","      <td>668.500610</td>\n","      <td>660.436096</td>\n","      <td>650.920471</td>\n","      <td>642.177063</td>\n","      <td>...</td>\n","      <td>309.233521</td>\n","      <td>303.600311</td>\n","      <td>290.216400</td>\n","      <td>285.401459</td>\n","      <td>285.760101</td>\n","      <td>283.646149</td>\n","      <td>284.102081</td>\n","      <td>290.710480</td>\n","      <td>293.075592</td>\n","      <td>292.487854</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>609.369507</td>\n","      <td>621.199036</td>\n","      <td>632.566162</td>\n","      <td>640.684326</td>\n","      <td>648.460999</td>\n","      <td>659.223328</td>\n","      <td>668.592346</td>\n","      <td>660.432373</td>\n","      <td>651.100769</td>\n","      <td>642.158447</td>\n","      <td>...</td>\n","      <td>309.322327</td>\n","      <td>303.610413</td>\n","      <td>290.103088</td>\n","      <td>285.410645</td>\n","      <td>285.664948</td>\n","      <td>283.755920</td>\n","      <td>283.856293</td>\n","      <td>290.838593</td>\n","      <td>293.148865</td>\n","      <td>292.536407</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>609.906738</td>\n","      <td>621.519897</td>\n","      <td>632.727295</td>\n","      <td>640.646729</td>\n","      <td>648.327515</td>\n","      <td>658.964294</td>\n","      <td>668.349487</td>\n","      <td>660.098999</td>\n","      <td>650.847229</td>\n","      <td>642.088013</td>\n","      <td>...</td>\n","      <td>308.913910</td>\n","      <td>303.289612</td>\n","      <td>289.962982</td>\n","      <td>285.395813</td>\n","      <td>285.692566</td>\n","      <td>283.840820</td>\n","      <td>284.394409</td>\n","      <td>290.729156</td>\n","      <td>292.901794</td>\n","      <td>292.327271</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>609.749146</td>\n","      <td>621.440735</td>\n","      <td>632.648438</td>\n","      <td>640.578003</td>\n","      <td>648.394531</td>\n","      <td>658.992676</td>\n","      <td>668.410034</td>\n","      <td>660.196899</td>\n","      <td>650.778625</td>\n","      <td>642.025146</td>\n","      <td>...</td>\n","      <td>308.905426</td>\n","      <td>303.313843</td>\n","      <td>290.014465</td>\n","      <td>285.349060</td>\n","      <td>285.719727</td>\n","      <td>283.752228</td>\n","      <td>284.595642</td>\n","      <td>290.670898</td>\n","      <td>292.837250</td>\n","      <td>292.223450</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>324</th>\n","      <td>611.022461</td>\n","      <td>621.915833</td>\n","      <td>633.452698</td>\n","      <td>640.907898</td>\n","      <td>648.139343</td>\n","      <td>659.169189</td>\n","      <td>669.763000</td>\n","      <td>659.402527</td>\n","      <td>649.404053</td>\n","      <td>641.106445</td>\n","      <td>...</td>\n","      <td>307.369171</td>\n","      <td>301.561066</td>\n","      <td>288.453735</td>\n","      <td>284.328735</td>\n","      <td>285.015686</td>\n","      <td>284.435028</td>\n","      <td>289.217896</td>\n","      <td>290.145538</td>\n","      <td>290.840546</td>\n","      <td>290.015533</td>\n","    </tr>\n","    <tr>\n","      <th>325</th>\n","      <td>610.946350</td>\n","      <td>621.848877</td>\n","      <td>633.436768</td>\n","      <td>640.937134</td>\n","      <td>648.150940</td>\n","      <td>659.192078</td>\n","      <td>669.764160</td>\n","      <td>659.443054</td>\n","      <td>649.471008</td>\n","      <td>641.089905</td>\n","      <td>...</td>\n","      <td>307.413727</td>\n","      <td>301.587860</td>\n","      <td>288.464264</td>\n","      <td>284.337708</td>\n","      <td>284.995544</td>\n","      <td>284.443848</td>\n","      <td>289.095184</td>\n","      <td>290.194061</td>\n","      <td>290.881927</td>\n","      <td>290.025818</td>\n","    </tr>\n","    <tr>\n","      <th>326</th>\n","      <td>609.897644</td>\n","      <td>621.636719</td>\n","      <td>632.742249</td>\n","      <td>640.519226</td>\n","      <td>648.278137</td>\n","      <td>659.000671</td>\n","      <td>668.662781</td>\n","      <td>660.109985</td>\n","      <td>650.703186</td>\n","      <td>642.176514</td>\n","      <td>...</td>\n","      <td>308.970886</td>\n","      <td>303.364960</td>\n","      <td>289.890320</td>\n","      <td>285.320160</td>\n","      <td>285.745758</td>\n","      <td>283.758514</td>\n","      <td>284.791077</td>\n","      <td>290.548615</td>\n","      <td>292.809143</td>\n","      <td>292.380341</td>\n","    </tr>\n","    <tr>\n","      <th>327</th>\n","      <td>609.811890</td>\n","      <td>621.556335</td>\n","      <td>632.651428</td>\n","      <td>640.495605</td>\n","      <td>648.436218</td>\n","      <td>659.088989</td>\n","      <td>668.625122</td>\n","      <td>660.195801</td>\n","      <td>650.563171</td>\n","      <td>642.110474</td>\n","      <td>...</td>\n","      <td>308.897095</td>\n","      <td>303.311646</td>\n","      <td>289.992737</td>\n","      <td>285.266663</td>\n","      <td>285.754578</td>\n","      <td>283.693634</td>\n","      <td>285.020538</td>\n","      <td>290.497253</td>\n","      <td>292.714661</td>\n","      <td>292.204590</td>\n","    </tr>\n","    <tr>\n","      <th>328</th>\n","      <td>609.901917</td>\n","      <td>621.632202</td>\n","      <td>632.640686</td>\n","      <td>640.448242</td>\n","      <td>648.472656</td>\n","      <td>659.077759</td>\n","      <td>668.571594</td>\n","      <td>660.162354</td>\n","      <td>650.439819</td>\n","      <td>642.131836</td>\n","      <td>...</td>\n","      <td>308.822296</td>\n","      <td>303.271027</td>\n","      <td>290.033020</td>\n","      <td>285.256653</td>\n","      <td>285.795349</td>\n","      <td>283.659271</td>\n","      <td>285.190918</td>\n","      <td>290.422455</td>\n","      <td>292.655945</td>\n","      <td>292.169189</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>329 rows × 40 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f08f3cfe-e6c0-4939-9d42-f4aa2144fe20')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f08f3cfe-e6c0-4939-9d42-f4aa2144fe20 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f08f3cfe-e6c0-4939-9d42-f4aa2144fe20');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-23432baa-52b4-4267-8fc1-d50a6379b084\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23432baa-52b4-4267-8fc1-d50a6379b084')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-23432baa-52b4-4267-8fc1-d50a6379b084 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["# 기준이 되는 프레임 내 x,y 좌표 min값 빼줌\n","y_df.iloc[:, :20]=y_df.iloc[:, :20].sub(y_df.iloc[:, :20].min(axis=1), axis=0)\n","y_df.iloc[:, 20:]=y_df.iloc[:, 20:].sub(y_df.iloc[:, 20:].min(axis=1), axis=0)"],"metadata":{"id":"39WHjWQk94i9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"7jqveIGj-EHW","executionInfo":{"status":"ok","timestamp":1701547822940,"user_tz":-540,"elapsed":374,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"5ef6b40c-6019-42c9-cd46-a39cc92cfd78"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Landmark_1_X  Landmark_2_X  Landmark_3_X  Landmark_4_X  Landmark_5_X  \\\n","0             0.0     11.477783     22.600708     30.298340     38.060425   \n","1             0.0     11.864258     23.090332     31.156494     39.126404   \n","2             0.0     11.829529     23.196655     31.314819     39.091492   \n","3             0.0     11.613159     22.820557     30.739990     38.420776   \n","4             0.0     11.691589     22.899292     30.828857     38.645386   \n","..            ...           ...           ...           ...           ...   \n","324           0.0     10.893372     22.430237     29.885437     37.116882   \n","325           0.0     10.902527     22.490417     29.990784     37.204590   \n","326           0.0     11.739075     22.844604     30.621582     38.380493   \n","327           0.0     11.744446     22.839539     30.683716     38.624329   \n","328           0.0     11.730286     22.738770     30.546326     38.570740   \n","\n","     Landmark_6_X  Landmark_7_X  Landmark_8_X  Landmark_9_X  Landmark_10_X  \\\n","0       48.767700     58.518250     49.622681     39.922424      31.616211   \n","1       49.782959     59.091064     51.026550     41.510925      32.767517   \n","2       49.853821     59.222839     51.062866     41.731262      32.788940   \n","3       49.057556     58.442749     50.192261     40.940491      32.181274   \n","4       49.243530     58.660889     50.447754     41.029480      32.276001   \n","..            ...           ...           ...           ...            ...   \n","324     48.146729     58.740540     48.380066     38.381592      30.083984   \n","325     48.245728     58.817810     48.496704     38.524658      30.143555   \n","326     49.103027     58.765137     50.212341     40.805542      32.278870   \n","327     49.277100     58.813232     50.383911     40.751282      32.298584   \n","328     49.175842     58.669678     50.260437     40.537903      32.229919   \n","\n","     ...  Landmark_11_Y  Landmark_12_Y  Landmark_13_Y  Landmark_14_Y  \\\n","0    ...      37.232727      31.617493      18.412628      13.851013   \n","1    ...      38.294037      32.660828      19.276917      14.461975   \n","2    ...      38.576111      32.864197      19.356873      14.664429   \n","3    ...      37.887909      32.263611      18.936981      14.369812   \n","4    ...      37.858765      32.267181      18.967804      14.302399   \n","..   ...            ...            ...            ...            ...   \n","324  ...      36.741821      30.933716      17.826385      13.701385   \n","325  ...      36.787354      30.961487      17.837891      13.711334   \n","326  ...      37.855835      32.249908      18.775269      14.205109   \n","327  ...      37.743164      32.157715      18.838806      14.112732   \n","328  ...      37.553680      32.002411      18.764404      13.988037   \n","\n","     Landmark_15_Y  Landmark_16_Y  Landmark_17_Y  Landmark_18_Y  \\\n","0        14.396118      12.685150      14.907318      19.169861   \n","1        14.820618      12.706665      13.162598      19.770996   \n","2        14.918732      13.009705      13.110077      20.092377   \n","3        14.666565      12.814819      13.368408      19.703156   \n","4        14.673065      12.705566      13.548981      19.624237   \n","..             ...            ...            ...            ...   \n","324      14.388336      13.807678      18.590546      19.518188   \n","325      14.369171      13.817474      18.468811      19.567688   \n","326      14.630707      12.643463      13.676025      19.433563   \n","327      14.600647      12.539703      13.866608      19.343323   \n","328      14.526733      12.390656      13.922302      19.153839   \n","\n","     Landmark_19_Y  Landmark_20_Y  \n","0        21.022797      20.472656  \n","1        22.136108      21.548370  \n","2        22.402649      21.790192  \n","3        21.875793      21.301270  \n","4        21.790588      21.176788  \n","..             ...            ...  \n","324      20.213196      19.388184  \n","325      20.255554      19.399445  \n","326      21.694092      21.265289  \n","327      21.560730      21.050659  \n","328      21.387329      20.900574  \n","\n","[329 rows x 40 columns]"],"text/html":["\n","  <div id=\"df-bbf8af62-7f2b-4e42-b462-1175123c4317\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Landmark_1_X</th>\n","      <th>Landmark_2_X</th>\n","      <th>Landmark_3_X</th>\n","      <th>Landmark_4_X</th>\n","      <th>Landmark_5_X</th>\n","      <th>Landmark_6_X</th>\n","      <th>Landmark_7_X</th>\n","      <th>Landmark_8_X</th>\n","      <th>Landmark_9_X</th>\n","      <th>Landmark_10_X</th>\n","      <th>...</th>\n","      <th>Landmark_11_Y</th>\n","      <th>Landmark_12_Y</th>\n","      <th>Landmark_13_Y</th>\n","      <th>Landmark_14_Y</th>\n","      <th>Landmark_15_Y</th>\n","      <th>Landmark_16_Y</th>\n","      <th>Landmark_17_Y</th>\n","      <th>Landmark_18_Y</th>\n","      <th>Landmark_19_Y</th>\n","      <th>Landmark_20_Y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>11.477783</td>\n","      <td>22.600708</td>\n","      <td>30.298340</td>\n","      <td>38.060425</td>\n","      <td>48.767700</td>\n","      <td>58.518250</td>\n","      <td>49.622681</td>\n","      <td>39.922424</td>\n","      <td>31.616211</td>\n","      <td>...</td>\n","      <td>37.232727</td>\n","      <td>31.617493</td>\n","      <td>18.412628</td>\n","      <td>13.851013</td>\n","      <td>14.396118</td>\n","      <td>12.685150</td>\n","      <td>14.907318</td>\n","      <td>19.169861</td>\n","      <td>21.022797</td>\n","      <td>20.472656</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>11.864258</td>\n","      <td>23.090332</td>\n","      <td>31.156494</td>\n","      <td>39.126404</td>\n","      <td>49.782959</td>\n","      <td>59.091064</td>\n","      <td>51.026550</td>\n","      <td>41.510925</td>\n","      <td>32.767517</td>\n","      <td>...</td>\n","      <td>38.294037</td>\n","      <td>32.660828</td>\n","      <td>19.276917</td>\n","      <td>14.461975</td>\n","      <td>14.820618</td>\n","      <td>12.706665</td>\n","      <td>13.162598</td>\n","      <td>19.770996</td>\n","      <td>22.136108</td>\n","      <td>21.548370</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>11.829529</td>\n","      <td>23.196655</td>\n","      <td>31.314819</td>\n","      <td>39.091492</td>\n","      <td>49.853821</td>\n","      <td>59.222839</td>\n","      <td>51.062866</td>\n","      <td>41.731262</td>\n","      <td>32.788940</td>\n","      <td>...</td>\n","      <td>38.576111</td>\n","      <td>32.864197</td>\n","      <td>19.356873</td>\n","      <td>14.664429</td>\n","      <td>14.918732</td>\n","      <td>13.009705</td>\n","      <td>13.110077</td>\n","      <td>20.092377</td>\n","      <td>22.402649</td>\n","      <td>21.790192</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>11.613159</td>\n","      <td>22.820557</td>\n","      <td>30.739990</td>\n","      <td>38.420776</td>\n","      <td>49.057556</td>\n","      <td>58.442749</td>\n","      <td>50.192261</td>\n","      <td>40.940491</td>\n","      <td>32.181274</td>\n","      <td>...</td>\n","      <td>37.887909</td>\n","      <td>32.263611</td>\n","      <td>18.936981</td>\n","      <td>14.369812</td>\n","      <td>14.666565</td>\n","      <td>12.814819</td>\n","      <td>13.368408</td>\n","      <td>19.703156</td>\n","      <td>21.875793</td>\n","      <td>21.301270</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>11.691589</td>\n","      <td>22.899292</td>\n","      <td>30.828857</td>\n","      <td>38.645386</td>\n","      <td>49.243530</td>\n","      <td>58.660889</td>\n","      <td>50.447754</td>\n","      <td>41.029480</td>\n","      <td>32.276001</td>\n","      <td>...</td>\n","      <td>37.858765</td>\n","      <td>32.267181</td>\n","      <td>18.967804</td>\n","      <td>14.302399</td>\n","      <td>14.673065</td>\n","      <td>12.705566</td>\n","      <td>13.548981</td>\n","      <td>19.624237</td>\n","      <td>21.790588</td>\n","      <td>21.176788</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>324</th>\n","      <td>0.0</td>\n","      <td>10.893372</td>\n","      <td>22.430237</td>\n","      <td>29.885437</td>\n","      <td>37.116882</td>\n","      <td>48.146729</td>\n","      <td>58.740540</td>\n","      <td>48.380066</td>\n","      <td>38.381592</td>\n","      <td>30.083984</td>\n","      <td>...</td>\n","      <td>36.741821</td>\n","      <td>30.933716</td>\n","      <td>17.826385</td>\n","      <td>13.701385</td>\n","      <td>14.388336</td>\n","      <td>13.807678</td>\n","      <td>18.590546</td>\n","      <td>19.518188</td>\n","      <td>20.213196</td>\n","      <td>19.388184</td>\n","    </tr>\n","    <tr>\n","      <th>325</th>\n","      <td>0.0</td>\n","      <td>10.902527</td>\n","      <td>22.490417</td>\n","      <td>29.990784</td>\n","      <td>37.204590</td>\n","      <td>48.245728</td>\n","      <td>58.817810</td>\n","      <td>48.496704</td>\n","      <td>38.524658</td>\n","      <td>30.143555</td>\n","      <td>...</td>\n","      <td>36.787354</td>\n","      <td>30.961487</td>\n","      <td>17.837891</td>\n","      <td>13.711334</td>\n","      <td>14.369171</td>\n","      <td>13.817474</td>\n","      <td>18.468811</td>\n","      <td>19.567688</td>\n","      <td>20.255554</td>\n","      <td>19.399445</td>\n","    </tr>\n","    <tr>\n","      <th>326</th>\n","      <td>0.0</td>\n","      <td>11.739075</td>\n","      <td>22.844604</td>\n","      <td>30.621582</td>\n","      <td>38.380493</td>\n","      <td>49.103027</td>\n","      <td>58.765137</td>\n","      <td>50.212341</td>\n","      <td>40.805542</td>\n","      <td>32.278870</td>\n","      <td>...</td>\n","      <td>37.855835</td>\n","      <td>32.249908</td>\n","      <td>18.775269</td>\n","      <td>14.205109</td>\n","      <td>14.630707</td>\n","      <td>12.643463</td>\n","      <td>13.676025</td>\n","      <td>19.433563</td>\n","      <td>21.694092</td>\n","      <td>21.265289</td>\n","    </tr>\n","    <tr>\n","      <th>327</th>\n","      <td>0.0</td>\n","      <td>11.744446</td>\n","      <td>22.839539</td>\n","      <td>30.683716</td>\n","      <td>38.624329</td>\n","      <td>49.277100</td>\n","      <td>58.813232</td>\n","      <td>50.383911</td>\n","      <td>40.751282</td>\n","      <td>32.298584</td>\n","      <td>...</td>\n","      <td>37.743164</td>\n","      <td>32.157715</td>\n","      <td>18.838806</td>\n","      <td>14.112732</td>\n","      <td>14.600647</td>\n","      <td>12.539703</td>\n","      <td>13.866608</td>\n","      <td>19.343323</td>\n","      <td>21.560730</td>\n","      <td>21.050659</td>\n","    </tr>\n","    <tr>\n","      <th>328</th>\n","      <td>0.0</td>\n","      <td>11.730286</td>\n","      <td>22.738770</td>\n","      <td>30.546326</td>\n","      <td>38.570740</td>\n","      <td>49.175842</td>\n","      <td>58.669678</td>\n","      <td>50.260437</td>\n","      <td>40.537903</td>\n","      <td>32.229919</td>\n","      <td>...</td>\n","      <td>37.553680</td>\n","      <td>32.002411</td>\n","      <td>18.764404</td>\n","      <td>13.988037</td>\n","      <td>14.526733</td>\n","      <td>12.390656</td>\n","      <td>13.922302</td>\n","      <td>19.153839</td>\n","      <td>21.387329</td>\n","      <td>20.900574</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>329 rows × 40 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbf8af62-7f2b-4e42-b462-1175123c4317')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-bbf8af62-7f2b-4e42-b462-1175123c4317 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-bbf8af62-7f2b-4e42-b462-1175123c4317');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0c9592d3-5686-4c41-b2c3-8b1fe7051ec1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c9592d3-5686-4c41-b2c3-8b1fe7051ec1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0c9592d3-5686-4c41-b2c3-8b1fe7051ec1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["# 적용하려는 동영상 프레임 당 x,y좌표 min값 더해줌(x,y min값 기준으로 재조정)\n","y_df.iloc[:, :20]=y_df.iloc[:, :20].add(df.iloc[:, :20].min(axis=1), axis=0)\n","y_df.iloc[:, 20:]=y_df.iloc[:, 20:].add(df.iloc[:, 20:].min(axis=1), axis=0)"],"metadata":{"id":"jJ5J6-B6-Uxa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"F6xUtIhk-sZo","executionInfo":{"status":"ok","timestamp":1701547832518,"user_tz":-540,"elapsed":425,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"636e5de4-8d61-417a-8b93-a47fd0676320"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Landmark_1_X  Landmark_2_X  Landmark_3_X  Landmark_4_X  Landmark_5_X  \\\n","0           613.0    624.477783    635.600708    643.298340    651.060425   \n","1           613.0    624.864258    636.090332    644.156494    652.126404   \n","2           613.0    624.829529    636.196655    644.314819    652.091492   \n","3           612.0    623.613159    634.820557    642.739990    650.420776   \n","4           613.0    624.691589    635.899292    643.828857    651.645386   \n","..            ...           ...           ...           ...           ...   \n","324         614.0    624.893372    636.430237    643.885437    651.116882   \n","325         614.0    624.902527    636.490417    643.990784    651.204590   \n","326         614.0    625.739075    636.844604    644.621582    652.380493   \n","327         614.0    625.744446    636.839539    644.683716    652.624329   \n","328         614.0    625.730286    636.738770    644.546326    652.570740   \n","\n","     Landmark_6_X  Landmark_7_X  Landmark_8_X  Landmark_9_X  Landmark_10_X  \\\n","0      661.767700    671.518250    662.622681    652.922424     644.616211   \n","1      662.782959    672.091064    664.026550    654.510925     645.767517   \n","2      662.853821    672.222839    664.062866    654.731262     645.788940   \n","3      661.057556    670.442749    662.192261    652.940491     644.181274   \n","4      662.243530    671.660889    663.447754    654.029480     645.276001   \n","..            ...           ...           ...           ...            ...   \n","324    662.146729    672.740540    662.380066    652.381592     644.083984   \n","325    662.245728    672.817810    662.496704    652.524658     644.143555   \n","326    663.103027    672.765137    664.212341    654.805542     646.278870   \n","327    663.277100    672.813232    664.383911    654.751282     646.298584   \n","328    663.175842    672.669678    664.260437    654.537903     646.229919   \n","\n","     ...  Landmark_11_Y  Landmark_12_Y  Landmark_13_Y  Landmark_14_Y  \\\n","0    ...     314.232727     308.617493     295.412628     290.851013   \n","1    ...     315.294037     309.660828     296.276917     291.461975   \n","2    ...     316.576111     310.864197     297.356873     292.664429   \n","3    ...     315.887909     310.263611     296.936981     292.369812   \n","4    ...     314.858765     309.267181     295.967804     291.302399   \n","..   ...            ...            ...            ...            ...   \n","324  ...     316.741821     310.933716     297.826385     293.701385   \n","325  ...     321.787354     315.961487     302.837891     298.711334   \n","326  ...     325.855835     320.249908     306.775269     302.205109   \n","327  ...     329.743164     324.157715     310.838806     306.112732   \n","328  ...     331.553680     326.002411     312.764404     307.988037   \n","\n","     Landmark_15_Y  Landmark_16_Y  Landmark_17_Y  Landmark_18_Y  \\\n","0       291.396118     289.685150     291.907318     296.169861   \n","1       291.820618     289.706665     290.162598     296.770996   \n","2       292.918732     291.009705     291.110077     298.092377   \n","3       292.666565     290.814819     291.368408     297.703156   \n","4       291.673065     289.705566     290.548981     296.624237   \n","..             ...            ...            ...            ...   \n","324     294.388336     293.807678     298.590546     299.518188   \n","325     299.369171     298.817474     303.468811     304.567688   \n","326     302.630707     300.643463     301.676025     307.433563   \n","327     306.600647     304.539703     305.866608     311.343323   \n","328     308.526733     306.390656     307.922302     313.153839   \n","\n","     Landmark_19_Y  Landmark_20_Y  \n","0       298.022797     297.472656  \n","1       299.136108     298.548370  \n","2       300.402649     299.790192  \n","3       299.875793     299.301270  \n","4       298.790588     298.176788  \n","..             ...            ...  \n","324     300.213196     299.388184  \n","325     305.255554     304.399445  \n","326     309.694092     309.265289  \n","327     313.560730     313.050659  \n","328     315.387329     314.900574  \n","\n","[329 rows x 40 columns]"],"text/html":["\n","  <div id=\"df-a87a1f76-c254-42eb-8b24-bf6b40bc9310\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Landmark_1_X</th>\n","      <th>Landmark_2_X</th>\n","      <th>Landmark_3_X</th>\n","      <th>Landmark_4_X</th>\n","      <th>Landmark_5_X</th>\n","      <th>Landmark_6_X</th>\n","      <th>Landmark_7_X</th>\n","      <th>Landmark_8_X</th>\n","      <th>Landmark_9_X</th>\n","      <th>Landmark_10_X</th>\n","      <th>...</th>\n","      <th>Landmark_11_Y</th>\n","      <th>Landmark_12_Y</th>\n","      <th>Landmark_13_Y</th>\n","      <th>Landmark_14_Y</th>\n","      <th>Landmark_15_Y</th>\n","      <th>Landmark_16_Y</th>\n","      <th>Landmark_17_Y</th>\n","      <th>Landmark_18_Y</th>\n","      <th>Landmark_19_Y</th>\n","      <th>Landmark_20_Y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>613.0</td>\n","      <td>624.477783</td>\n","      <td>635.600708</td>\n","      <td>643.298340</td>\n","      <td>651.060425</td>\n","      <td>661.767700</td>\n","      <td>671.518250</td>\n","      <td>662.622681</td>\n","      <td>652.922424</td>\n","      <td>644.616211</td>\n","      <td>...</td>\n","      <td>314.232727</td>\n","      <td>308.617493</td>\n","      <td>295.412628</td>\n","      <td>290.851013</td>\n","      <td>291.396118</td>\n","      <td>289.685150</td>\n","      <td>291.907318</td>\n","      <td>296.169861</td>\n","      <td>298.022797</td>\n","      <td>297.472656</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>613.0</td>\n","      <td>624.864258</td>\n","      <td>636.090332</td>\n","      <td>644.156494</td>\n","      <td>652.126404</td>\n","      <td>662.782959</td>\n","      <td>672.091064</td>\n","      <td>664.026550</td>\n","      <td>654.510925</td>\n","      <td>645.767517</td>\n","      <td>...</td>\n","      <td>315.294037</td>\n","      <td>309.660828</td>\n","      <td>296.276917</td>\n","      <td>291.461975</td>\n","      <td>291.820618</td>\n","      <td>289.706665</td>\n","      <td>290.162598</td>\n","      <td>296.770996</td>\n","      <td>299.136108</td>\n","      <td>298.548370</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>613.0</td>\n","      <td>624.829529</td>\n","      <td>636.196655</td>\n","      <td>644.314819</td>\n","      <td>652.091492</td>\n","      <td>662.853821</td>\n","      <td>672.222839</td>\n","      <td>664.062866</td>\n","      <td>654.731262</td>\n","      <td>645.788940</td>\n","      <td>...</td>\n","      <td>316.576111</td>\n","      <td>310.864197</td>\n","      <td>297.356873</td>\n","      <td>292.664429</td>\n","      <td>292.918732</td>\n","      <td>291.009705</td>\n","      <td>291.110077</td>\n","      <td>298.092377</td>\n","      <td>300.402649</td>\n","      <td>299.790192</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>612.0</td>\n","      <td>623.613159</td>\n","      <td>634.820557</td>\n","      <td>642.739990</td>\n","      <td>650.420776</td>\n","      <td>661.057556</td>\n","      <td>670.442749</td>\n","      <td>662.192261</td>\n","      <td>652.940491</td>\n","      <td>644.181274</td>\n","      <td>...</td>\n","      <td>315.887909</td>\n","      <td>310.263611</td>\n","      <td>296.936981</td>\n","      <td>292.369812</td>\n","      <td>292.666565</td>\n","      <td>290.814819</td>\n","      <td>291.368408</td>\n","      <td>297.703156</td>\n","      <td>299.875793</td>\n","      <td>299.301270</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>613.0</td>\n","      <td>624.691589</td>\n","      <td>635.899292</td>\n","      <td>643.828857</td>\n","      <td>651.645386</td>\n","      <td>662.243530</td>\n","      <td>671.660889</td>\n","      <td>663.447754</td>\n","      <td>654.029480</td>\n","      <td>645.276001</td>\n","      <td>...</td>\n","      <td>314.858765</td>\n","      <td>309.267181</td>\n","      <td>295.967804</td>\n","      <td>291.302399</td>\n","      <td>291.673065</td>\n","      <td>289.705566</td>\n","      <td>290.548981</td>\n","      <td>296.624237</td>\n","      <td>298.790588</td>\n","      <td>298.176788</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>324</th>\n","      <td>614.0</td>\n","      <td>624.893372</td>\n","      <td>636.430237</td>\n","      <td>643.885437</td>\n","      <td>651.116882</td>\n","      <td>662.146729</td>\n","      <td>672.740540</td>\n","      <td>662.380066</td>\n","      <td>652.381592</td>\n","      <td>644.083984</td>\n","      <td>...</td>\n","      <td>316.741821</td>\n","      <td>310.933716</td>\n","      <td>297.826385</td>\n","      <td>293.701385</td>\n","      <td>294.388336</td>\n","      <td>293.807678</td>\n","      <td>298.590546</td>\n","      <td>299.518188</td>\n","      <td>300.213196</td>\n","      <td>299.388184</td>\n","    </tr>\n","    <tr>\n","      <th>325</th>\n","      <td>614.0</td>\n","      <td>624.902527</td>\n","      <td>636.490417</td>\n","      <td>643.990784</td>\n","      <td>651.204590</td>\n","      <td>662.245728</td>\n","      <td>672.817810</td>\n","      <td>662.496704</td>\n","      <td>652.524658</td>\n","      <td>644.143555</td>\n","      <td>...</td>\n","      <td>321.787354</td>\n","      <td>315.961487</td>\n","      <td>302.837891</td>\n","      <td>298.711334</td>\n","      <td>299.369171</td>\n","      <td>298.817474</td>\n","      <td>303.468811</td>\n","      <td>304.567688</td>\n","      <td>305.255554</td>\n","      <td>304.399445</td>\n","    </tr>\n","    <tr>\n","      <th>326</th>\n","      <td>614.0</td>\n","      <td>625.739075</td>\n","      <td>636.844604</td>\n","      <td>644.621582</td>\n","      <td>652.380493</td>\n","      <td>663.103027</td>\n","      <td>672.765137</td>\n","      <td>664.212341</td>\n","      <td>654.805542</td>\n","      <td>646.278870</td>\n","      <td>...</td>\n","      <td>325.855835</td>\n","      <td>320.249908</td>\n","      <td>306.775269</td>\n","      <td>302.205109</td>\n","      <td>302.630707</td>\n","      <td>300.643463</td>\n","      <td>301.676025</td>\n","      <td>307.433563</td>\n","      <td>309.694092</td>\n","      <td>309.265289</td>\n","    </tr>\n","    <tr>\n","      <th>327</th>\n","      <td>614.0</td>\n","      <td>625.744446</td>\n","      <td>636.839539</td>\n","      <td>644.683716</td>\n","      <td>652.624329</td>\n","      <td>663.277100</td>\n","      <td>672.813232</td>\n","      <td>664.383911</td>\n","      <td>654.751282</td>\n","      <td>646.298584</td>\n","      <td>...</td>\n","      <td>329.743164</td>\n","      <td>324.157715</td>\n","      <td>310.838806</td>\n","      <td>306.112732</td>\n","      <td>306.600647</td>\n","      <td>304.539703</td>\n","      <td>305.866608</td>\n","      <td>311.343323</td>\n","      <td>313.560730</td>\n","      <td>313.050659</td>\n","    </tr>\n","    <tr>\n","      <th>328</th>\n","      <td>614.0</td>\n","      <td>625.730286</td>\n","      <td>636.738770</td>\n","      <td>644.546326</td>\n","      <td>652.570740</td>\n","      <td>663.175842</td>\n","      <td>672.669678</td>\n","      <td>664.260437</td>\n","      <td>654.537903</td>\n","      <td>646.229919</td>\n","      <td>...</td>\n","      <td>331.553680</td>\n","      <td>326.002411</td>\n","      <td>312.764404</td>\n","      <td>307.988037</td>\n","      <td>308.526733</td>\n","      <td>306.390656</td>\n","      <td>307.922302</td>\n","      <td>313.153839</td>\n","      <td>315.387329</td>\n","      <td>314.900574</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>329 rows × 40 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a87a1f76-c254-42eb-8b24-bf6b40bc9310')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a87a1f76-c254-42eb-8b24-bf6b40bc9310 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a87a1f76-c254-42eb-8b24-bf6b40bc9310');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-66c3b58f-fb73-4638-bb34-a5d807e7f8a1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-66c3b58f-fb73-4638-bb34-a5d807e7f8a1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-66c3b58f-fb73-4638-bb34-a5d807e7f8a1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["# 동영상 만들기"],"metadata":{"id":"-NyYuwXo8did"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNCe0l5Oy7yn"},"outputs":[],"source":["import cv2\n","import os\n","# 각 이미지에 대해 랜드마크 좌표 읽어서 이미지 변형 및 저장\n","for i in range(len(y_df)):\n","    x_coords = y_df.iloc[i, :20].values\n","    y_coords = y_df.iloc[i, 20:].values\n","    landmarks = np.array(pd.DataFrame([x_coords, y_coords]).T)\n","\n","    # 입술 부분의 좌표 추출\n","    lip_points = np.array(landmarks, dtype=np.int32)\n","\n","    x_coords_in = y_df.iloc[i, 12:20].values\n","    y_coords_in = y_df.iloc[i, 32:].values\n","    landmarks_in = np.array(pd.DataFrame([x_coords_in, y_coords_in]).T)\n","\n","    # 입안 부분의 좌표 추출\n","    mouth_inner_points = np.array(landmarks_in, dtype=np.int32)\n","\n","    # 이미지 복사\n","    # 이미지 로드 (실제 이미지 파일 경로로 대체)\n","    frame_num=i\n","    image_path = f\"/content/gdrive//MyDrive/Wav2Lip/original_frame/frame_{i}.jpg\"\n","    image = cv2.imread(image_path)\n","    transformed_image = image.copy()\n","\n","    # 입술 부분을 빨간색으로 칠하기\n","    cv2.fillPoly(transformed_image, [lip_points], color=(134, 100, 224))  # 빨간색 (BGR 순서)\n","\n","    # 입안 부분을 어두운색으로 칠하기\n","    cv2.fillPoly(transformed_image, [mouth_inner_points], color=(64, 58, 52))  # 어두운색 (BGR 순서)\n","\n","    # 변형된 이미지 저장\n","    output_dir=\"/content/gdrive//MyDrive/Wav2Lip/output_image\"\n","    os.makedirs(output_dir, exist_ok=True)\n","    output_path = os.path.join(output_dir, f\"output_{i}.jpg\")\n","    cv2.imwrite(output_path, transformed_image)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8BCQhJYQCBl"},"outputs":[],"source":["import cv2\n","import imageio\n","import os\n","\n","# 사진들이 있는 디렉토리 경로\n","photo_directory = \"/content/gdrive//MyDrive/Wav2Lip/output_image/\"\n","\n","# 사진 파일들의 리스트 생성\n","photo_files = [os.path.join(photo_directory, f) for f in os.listdir(photo_directory) if f.endswith('.jpg')]\n","\n","# 동영상 파일 경로\n","video_path = \"/content/gdrive//MyDrive/Wav2Lip/mp4/video_new.mp4\"\n","\n","# 동영상 프레임의 크기 설정\n","frame_size = (1920, 1080)\n","\n","# 원하는 동영상 길이 설정 (초)\n","desired_duration = 14\n","\n","# 초당 프레임 수 계산\n","frame_rate = len(photo_files) / desired_duration\n","\n","# VideoWriter 객체 생성\n","video_writer = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, frame_size)\n","\n","# 사진들을 동영상에 추가\n","for photo in photo_files:\n","    img = cv2.imread(photo)\n","    img = cv2.resize(img, frame_size)\n","    video_writer.write(img)\n","\n","# 사용이 끝난 객체들을 해제\n","video_writer.release()\n"]},{"cell_type":"markdown","source":["# 영상에 소리 입히기"],"metadata":{"id":"gQ5IwUS2GQnx"}},{"cell_type":"code","source":["!pip uninstall moviepy decorator\n","!pip install moviepy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":850},"id":"zPvXD3Kt8CRm","executionInfo":{"status":"ok","timestamp":1701547898772,"user_tz":-540,"elapsed":16103,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"f2bc685c-df31-4432-8bd9-d310238f5439"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: moviepy 1.0.3\n","Uninstalling moviepy-1.0.3:\n","  Would remove:\n","    /usr/local/lib/python3.10/dist-packages/moviepy-1.0.3.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/moviepy/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled moviepy-1.0.3\n","Found existing installation: decorator 4.4.2\n","Uninstalling decorator-4.4.2:\n","  Would remove:\n","    /usr/local/lib/python3.10/dist-packages/decorator-4.4.2.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/decorator.py\n","Proceed (Y/n)? Y\n","  Successfully uninstalled decorator-4.4.2\n","Collecting moviepy\n","  Using cached moviepy-1.0.3-py3-none-any.whl\n","Collecting decorator<5.0,>=4.0.2 (from moviepy)\n","  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.1)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.23.5)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n","Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n","Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2023.11.17)\n","Installing collected packages: decorator, moviepy\n","  Attempting uninstall: decorator\n","    Found existing installation: decorator 5.1.1\n","    Uninstalling decorator-5.1.1:\n","      Successfully uninstalled decorator-5.1.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed decorator-4.4.2 moviepy-1.0.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["decorator"]}}},"metadata":{}}]},{"cell_type":"code","source":["from moviepy.editor import VideoFileClip, AudioFileClip\n","\n","# 동영상 파일 경로\n","video_path = \"/content/gdrive//MyDrive/Wav2Lip/mp4/video_new.mp4\"\n","\n","# 소리 파일 경로\n","audio_path = '/content/audio/audio.wav'\n","\n","# 출력 동영상 파일 경로\n","output_path = \"/content/gdrive//MyDrive/Wav2Lip/mp4/video_final_new.mp4\"\n","\n","# 동영상 클립 로드\n","video_clip = VideoFileClip(video_path)\n","\n","# 소리 클립 로드\n","audio_clip = AudioFileClip(audio_path)\n","\n","# 소리를 동영상에 추가\n","video_clip = video_clip.set_audio(audio_clip)\n","\n","# 결과를 파일로 저장\n","video_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n","\n","\n","# 사용이 끝난 객체 해제\n","video_clip.close()\n","audio_clip.close()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gq_LIgQC8CTx","executionInfo":{"status":"ok","timestamp":1701547966915,"user_tz":-540,"elapsed":53393,"user":{"displayName":"serah K","userId":"08070710945457856845"}},"outputId":"312dae2d-2647-4fb3-b36d-5b36d93d79f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Moviepy - Building video /content/gdrive//MyDrive/Wav2Lip/mp4/video_final_new.mp4.\n","MoviePy - Writing audio in video_final_newTEMP_MPY_wvf_snd.mp4\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["MoviePy - Done.\n","Moviepy - Writing video /content/gdrive//MyDrive/Wav2Lip/mp4/video_final_new.mp4\n","\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Moviepy - Done !\n","Moviepy - video ready /content/gdrive//MyDrive/Wav2Lip/mp4/video_final_new.mp4\n"]}]},{"cell_type":"code","source":["from IPython.display import display, HTML\n","from base64 import b64encode\n","\n","def play_local_video(video_path):\n","    video_encoded = b64encode(open(video_path, \"rb\").read()).decode()\n","    video_tag = f'<video controls alt=\"test\" src=\"data:video/mp4;base64,{video_encoded}\" width=\"640\" height=\"360\">'\n","    display(HTML(data=video_tag))\n","\n","# MP4 파일 재생\n","video_path = \"/content/gdrive//MyDrive/Wav2Lip/mp4/video_final_new.mp4\"\n","play_local_video(video_path)\n"],"metadata":{"id":"oDwNVp6iSpRJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 텍스트를 입력받아 딥페이크 영상 만들어주는 함수"],"metadata":{"id":"TtW9cQhBOHbW"}},{"cell_type":"code","source":["def talkingyu(text) :\n","    # 샘플레이트 설정 (예: 44100Hz)\n","    sample_rate = 24000\n","\n","    # 기본 오디오 데이터 생성 (예: 1초 길이의 무음)\n","    # np.zeros를 사용하여 모든 샘플이 0인 배열 생성\n","    duration = 1  # 1초\n","    audio_data = np.zeros(sample_rate * duration)\n","\n","    # 데이터 타입을 32비트 부동소수점 형식으로 변환\n","    audio_data = audio_data.astype(np.float32)\n","\n","    # WAV 파일로 저장\n","    wavfile.write('/content/audio/audio.wav', sample_rate, audio_data)\n","\n","    # Chunk the text into smaller pieces then combine the generated audio\n","    #같은 대사 여러번 하는게.?\n","\n","    #/content/datasets_yoo/tokens/yujaeseog17.npz\n","    # generation settings\n","    voice_name_1= \"/content/bark/bark/assets/prompts/ko_speaker_4.npz\"\n","    voice_name_2 = \"/content/drive/MyDrive/datasets_yoo/tokens/yujaeseog61.npz\"\n","    out_filepath = '/content/audio/audio.wav'\n","\n","    semantic_temp = 0.7 #높은 값 (예: 1 이상)은 더 다양하고 예측 불가능한 텍스트를 생성하는 데 기여합니다.\n","    semantic_top_k = 100 #모델이 다음 단어를 선택할 때 고려하는 후보 단어의 집합을 k개의 가장 높은 확률을 가진 단어로 제한합니다.\n","    semantic_top_p = 0.99\n","\n","    coarse_temp = 0.7\n","    coarse_top_k = 100\n","    coarse_top_p = 0.95\n","\n","    fine_temp = 0.7\n","\n","    use_semantic_history_prompt = True\n","    use_coarse_history_prompt = False\n","    use_fine_history_prompt = True\n","\n","    use_last_generation_as_history = False #지난 루프가..\n","\n","    if use_rvc:\n","        index_rate = 0.75\n","        f0up_key = -6\n","        filter_radius = 3\n","        rms_mix_rate = 0.25\n","        protect = 0.33\n","        resample_sr = SAMPLE_RATE\n","        f0method = \"harvest\" #harvest or pm\n","\n","    texts = split_and_recombine_text(text)\n","\n","    all_parts = []\n","    for i, text in tqdm(enumerate(texts), total=len(texts)):\n","        full_generation, audio_array = generate_with_settings(\n","            text,\n","            semantic_temp=semantic_temp,\n","            semantic_top_k=semantic_top_k,\n","            semantic_top_p=semantic_top_p,\n","            coarse_temp=coarse_temp,\n","            coarse_top_k=coarse_top_k,\n","            coarse_top_p=coarse_top_p,\n","            fine_temp=fine_temp,\n","            voice_name_1=voice_name_1,\n","            voice_name_2=voice_name_2,\n","            use_semantic_history_prompt=use_semantic_history_prompt,\n","            use_coarse_history_prompt=use_coarse_history_prompt,\n","            use_fine_history_prompt=use_fine_history_prompt,\n","            output_full=True\n","        )\n","        if use_last_generation_as_history:\n","            # save to npz\n","            os.makedirs('_temp', exist_ok=True)\n","            np.savez_compressed(\n","                '_temp/history.npz',\n","                semantic_prompt=full_generation['semantic_prompt'],\n","                coarse_prompt=full_generation['coarse_prompt'],\n","                fine_prompt=full_generation['fine_prompt'],\n","            )\n","            voice_name = '_temp/history.npz'\n","        write_wav(out_filepath.replace('.wav', f'_{i}') + '.wav', SAMPLE_RATE, audio_array)\n","\n","        if use_rvc:\n","            try:\n","                audio_array = vc_single(0,out_filepath.replace('.wav', f'_{i}') + '.wav',f0up_key,None,f0method,index_path,index_rate, filter_radius=filter_radius, resample_sr=resample_sr, rms_mix_rate=rms_mix_rate, protect=protect)\n","            except:\n","                audio_array = vc_single(0,out_filepath.replace('.wav', f'_{i}') + '.wav',f0up_key,None,'pm',index_path,index_rate, filter_radius=filter_radius, resample_sr=resample_sr, rms_mix_rate=rms_mix_rate, protect=protect)\n","            write_wav(out_filepath.replace('.wav', f'_{i}') + '.wav', SAMPLE_RATE, audio_array)\n","        all_parts.append(audio_array)\n","\n","    audio_array = np.concatenate(all_parts, axis=-1)\n","\n","    # save audio\n","    write_wav(out_filepath, SAMPLE_RATE, audio_array)\n","\n","    # play audio\n","    Audio(audio_array, rate=SAMPLE_RATE)\n","\n","    # WAV 파일 읽기\n","    y, sr = librosa.load(out_filepath)\n","\n","    # MFCCs 추출\n","    n_mfcc = 20  # MFCCs의 차원 수\n","    n_fft = 2048  # FFT 크기\n","    hop_length = 512  # 프레임 이동 간격\n","\n","    # MFCCs 추출\n","    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n","\n","    # 원하는 행의 개수를 지정\n","    num_rows = 329\n","    # MFCCs 데이터를 2차원 행렬로 변환\n","    mfcc_matrix_ = np.empty((0, n_mfcc))\n","\n","    # MFCCs 데이터를 행 단위로 추가\n","    for i in range(num_rows):\n","        start_frame = i * (mfccs.shape[1] // num_rows)\n","        end_frame = (i + 1) * (mfccs.shape[1] // num_rows)\n","        mfcc_sequence = mfccs[:, start_frame:end_frame]\n","        mfcc_matrix_ = np.vstack((mfcc_matrix_, mfcc_sequence.T))  # .T를 통해 전치하여 (n_mfcc, 프레임 수) 형태로\n","\n","    y=model(mfcc_matrix_)             # 예측된 표준화 좌표\n","    y=np.copy(y)\n","    y_df = pd.DataFrame(y, columns=df3.columns)\n","\n","    # 기준 프레임에 역표준화하여 적용시킴\n","    y_df.iloc[:, :20]=(y_df.iloc[:, :20].mul(sx)).add(mx)\n","    y_df.iloc[:, 20:]=(y_df.iloc[:, 20:].mul(sy)).add(my)\n","\n","    # 기준이 되는 프레임 내 x,y 좌표 min값 빼줌\n","    y_df.iloc[:, :20]=y_df.iloc[:, :20].sub(y_df.iloc[:, :20].min(axis=1), axis=0)\n","    y_df.iloc[:, 20:]=y_df.iloc[:, 20:].sub(y_df.iloc[:, 20:].min(axis=1), axis=0)\n","\n","    # 적용하려는 동영상 프레임 당 x,y좌표 min값 더해줌(x,y min값 기준으로 재조정)\n","    y_df.iloc[:, :20]=y_df.iloc[:, :20].add(df.iloc[:, :20].min(axis=1), axis=0)\n","    y_df.iloc[:, 20:]=y_df.iloc[:, 20:].add(df.iloc[:, 20:].min(axis=1), axis=0)\n","\n","    for i in range(len(y_df)):\n","        x_coords = y_df.iloc[i, :20].values\n","        y_coords = y_df.iloc[i, 20:].values\n","        landmarks = np.array(pd.DataFrame([x_coords, y_coords]).T)\n","\n","        # 입술 부분의 좌표 추출\n","        lip_points = np.array(landmarks, dtype=np.int32)\n","\n","        x_coords_in = y_df.iloc[i, 12:20].values\n","        y_coords_in = y_df.iloc[i, 32:].values\n","        landmarks_in = np.array(pd.DataFrame([x_coords_in, y_coords_in]).T)\n","\n","        # 입안 부분의 좌표 추출\n","        mouth_inner_points = np.array(landmarks_in, dtype=np.int32)\n","\n","        # 이미지 복사\n","        # 이미지 로드 (실제 이미지 파일 경로로 대체)\n","        frame_num=i\n","        image_path = f\"/content/gdrive//MyDrive/Wav2Lip/original_frame/frame_{i}.jpg\"\n","        image = cv2.imread(image_path)\n","        transformed_image = image.copy()\n","\n","        # 입술 부분을 빨간색으로 칠하기\n","        cv2.fillPoly(transformed_image, [lip_points], color=(134, 100, 224))  # 빨간색 (BGR 순서)\n","\n","        # 입안 부분을 어두운색으로 칠하기\n","        cv2.fillPoly(transformed_image, [mouth_inner_points], color=(64, 58, 52))  # 어두운색 (BGR 순서)\n","\n","        # 변형된 이미지 저장\n","        output_dir=\"/content/gdrive//MyDrive/Wav2Lip/output_image\"\n","        os.makedirs(output_dir, exist_ok=True)\n","        output_path = os.path.join(output_dir, f\"output_{i}.jpg\")\n","        cv2.imwrite(output_path, transformed_image)\n","\n","    # 사진들이 있는 디렉토리 경로\n","    photo_directory = \"/content/gdrive//MyDrive/Wav2Lip/output_image/\"\n","\n","    # 사진 파일들의 리스트 생성\n","    photo_files = [os.path.join(photo_directory, f) for f in os.listdir(photo_directory) if f.endswith('.jpg')]\n","\n","    # 동영상 파일 경로\n","    video_path = \"/content/gdrive//MyDrive/Wav2Lip/mp4/video_new.mp4\"\n","\n","    # 동영상 프레임의 크기 설정\n","    frame_size = (1920, 1080)\n","\n","    # 원하는 동영상 길이 설정 (초)\n","    desired_duration = 14\n","\n","    # 초당 프레임 수 계산\n","    frame_rate = len(photo_files) / desired_duration\n","\n","    # VideoWriter 객체 생성\n","    video_writer = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), frame_rate, frame_size)\n","\n","    # 사진들을 동영상에 추가\n","    for photo in photo_files:\n","        img = cv2.imread(photo)\n","        img = cv2.resize(img, frame_size)\n","        video_writer.write(img)\n","\n","    # 사용이 끝난 객체들을 해제\n","    video_writer.release()\n","\n","    from moviepy.editor import VideoFileClip, AudioFileClip\n","\n","    # 동영상 파일 경로\n","    video_path = \"/content/gdrive//MyDrive/Wav2Lip/mp4/video_new.mp4\"\n","\n","    # 소리 파일 경로\n","    audio_path = '/content/audio/audio.wav'\n","\n","    # 출력 동영상 파일 경로\n","    output_path = \"/content/gdrive//MyDrive/Wav2Lip/mp4/video_final_new.mp4\"\n","\n","    # 동영상 클립 로드\n","    video_clip = VideoFileClip(video_path)\n","\n","    # 소리 클립 로드\n","    audio_clip = AudioFileClip(audio_path)\n","\n","    # 소리를 동영상에 추가\n","    video_clip = video_clip.set_audio(audio_clip)\n","\n","    # 결과를 파일로 저장\n","    video_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n","\n","\n","    # 사용이 끝난 객체 해제\n","    video_clip.close()\n","    audio_clip.close()\n","\n","    # MP4 파일 재생\n","    return play_local_video(\"/content/gdrive//MyDrive/Wav2Lip/mp4/video_final_new.mp4\")"],"metadata":{"id":"fGXkxEvhONms"},"execution_count":null,"outputs":[]}]}