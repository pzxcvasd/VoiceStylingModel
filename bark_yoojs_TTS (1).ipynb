{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "codes from:\n",
        "\n",
        "https://github.com/serp-ai/bark-with-voice-clone\n",
        "\n",
        "https://github.com/JonathanFly/bark\n",
        "\n",
        "한국어, 유재석 목소리에 따라 글자를 읽어주는 TTS 를 만들기 위해 위 코드들을 참조하여 만든 한국어 TTS 모델입니다.\n",
        "\n",
        "이화여자대학교 통계적기계학습이론 프로젝트\n",
        "\n",
        "2170030 노희애"
      ],
      "metadata": {
        "id": "DIxDmTS4obam"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUS4vZga-n0t"
      },
      "source": [
        "## PREPARE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emo9JzM7CpX9"
      },
      "source": [
        "datasets_yoo, audio, fine_output 폴더 만들기."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogUYjFfhcxTG",
        "outputId": "69dd77f8-e115-4c7c-c515-77ae328cf4cf",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Connected, your runtime has 54.8 gigabytes of available RAM\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Connect and check GPU and runtime\n",
        "from psutil import virtual_memory\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    print('Not connected to a GPU', end=\"\")\n",
        "elif gpu_info.find('not found') >= 0:\n",
        "    print('Not connected to a GPU', end=\"\")\n",
        "else:\n",
        "    print('GPU Connected', end=\"\")\n",
        "print(', your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJQ4TI0_Qowr"
      },
      "source": [
        "### Setup Notebook, Install dependencies\n",
        "<small>Run both cells to install system and needed functions.  \n",
        "_If Colab for some reason crashes re-run cell 0.2 before contining._</small>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8wG_tIaOV0Q",
        "outputId": "1aa8dc49-b311-4c81-995d-99681061676a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bark'...\n",
            "remote: Enumerating objects: 1527, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 1527 (delta 318), reused 293 (delta 291), pack-reused 1168\u001b[K\n",
            "Receiving objects: 100% (1527/1527), 19.82 MiB | 15.71 MiB/s, done.\n",
            "Resolving deltas: 100% (777/777), done.\n",
            "/content/bark\n",
            "Ignoring sox: markers 'platform_system == \"Darwin\"' don't match your environment\n",
            "Ignoring soundfile: markers 'platform_system == \"Windows\"' don't match your environment\n",
            "Ignoring fairseq: markers 'platform_system == \"Windows\"' don't match your environment\n",
            "Ignoring fairseq: markers 'platform_system == \"Darwin\"' don't match your environment\n",
            "Ignoring pywin32: markers 'platform_system == \"Windows\"' don't match your environment\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 1)) (67.7.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 2)) (4.35.2)\n",
            "Collecting diffusers (from -r old_setup_files/requirements-pip.txt (line 3))\n",
            "  Downloading diffusers-0.24.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-downloader (from -r old_setup_files/requirements-pip.txt (line 4))\n",
            "  Downloading ffmpeg_downloader-0.2.0-py3-none-any.whl (27 kB)\n",
            "Collecting ffmpeg (from -r old_setup_files/requirements-pip.txt (line 5))\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ffmpeg-python (from -r old_setup_files/requirements-pip.txt (line 6))\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Collecting sox (from -r old_setup_files/requirements-pip.txt (line 7))\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Collecting fairseq (from -r old_setup_files/requirements-pip.txt (line 12))\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 13)) (0.10.1)\n",
            "Collecting boto3 (from -r old_setup_files/requirements-pip.txt (line 14))\n",
            "  Downloading boto3-1.33.6-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting funcy (from -r old_setup_files/requirements-pip.txt (line 15))\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 16)) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 17)) (1.11.4)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 18)) (0.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 19)) (4.66.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 20)) (7.34.0)\n",
            "Requirement already satisfied: huggingface_hub>0.15 in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 21)) (0.19.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 22)) (13.7.0)\n",
            "Collecting pathvalidate (from -r old_setup_files/requirements-pip.txt (line 23))\n",
            "  Downloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting rich-argparse (from -r old_setup_files/requirements-pip.txt (line 24))\n",
            "  Downloading rich_argparse-1.4.0-py3-none-any.whl (19 kB)\n",
            "Collecting encodec (from -r old_setup_files/requirements-pip.txt (line 25))\n",
            "  Downloading encodec-0.1.1.tar.gz (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 26)) (5.2.0)\n",
            "Collecting pydub (from -r old_setup_files/requirements-pip.txt (line 27))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 28)) (2.31.0)\n",
            "Collecting audio2numpy (from -r old_setup_files/requirements-pip.txt (line 29))\n",
            "  Downloading audio2numpy-0.1.2-py3-none-any.whl (10 kB)\n",
            "Collecting faiss-cpu (from -r old_setup_files/requirements-pip.txt (line 30))\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from -r old_setup_files/requirements-pip.txt (line 31)) (1.3.2)\n",
            "Collecting audiolm-pytorch (from -r old_setup_files/requirements-pip.txt (line 32))\n",
            "  Downloading audiolm_pytorch-1.8.5-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting universal-startfile (from -r old_setup_files/requirements-pip.txt (line 33))\n",
            "  Downloading universal_startfile-0.2-py3-none-any.whl (3.4 kB)\n",
            "Collecting gradio>=3.34.0 (from -r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading gradio-4.7.1-py3-none-any.whl (16.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r old_setup_files/requirements-pip.txt (line 2)) (3.13.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r old_setup_files/requirements-pip.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r old_setup_files/requirements-pip.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r old_setup_files/requirements-pip.txt (line 2)) (2023.6.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r old_setup_files/requirements-pip.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers->-r old_setup_files/requirements-pip.txt (line 3)) (9.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->-r old_setup_files/requirements-pip.txt (line 3)) (6.8.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from ffmpeg-downloader->-r old_setup_files/requirements-pip.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python->-r old_setup_files/requirements-pip.txt (line 6)) (0.18.3)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (3.0.6)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n",
            "  Downloading sacrebleu-2.3.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.4/106.4 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (2.1.0+cu118)\n",
            "Collecting bitarray (from fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n",
            "  Downloading bitarray-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (2.1.0+cu118)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (1.2.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r old_setup_files/requirements-pip.txt (line 13)) (1.0.7)\n",
            "Collecting botocore<1.34.0,>=1.33.6 (from boto3->-r old_setup_files/requirements-pip.txt (line 14))\n",
            "  Downloading botocore-1.33.6-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->-r old_setup_files/requirements-pip.txt (line 14))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.9.0,>=0.8.2 (from boto3->-r old_setup_files/requirements-pip.txt (line 14))\n",
            "  Downloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jedi>=0.16 (from ipython->-r old_setup_files/requirements-pip.txt (line 20))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->-r old_setup_files/requirements-pip.txt (line 20)) (4.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>0.15->-r old_setup_files/requirements-pip.txt (line 21)) (2023.6.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->-r old_setup_files/requirements-pip.txt (line 22)) (3.0.0)\n",
            "Collecting einops (from encodec->-r old_setup_files/requirements-pip.txt (line 25))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r old_setup_files/requirements-pip.txt (line 28)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r old_setup_files/requirements-pip.txt (line 28)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r old_setup_files/requirements-pip.txt (line 28)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r old_setup_files/requirements-pip.txt (line 28)) (2023.11.17)\n",
            "Collecting accelerate>=0.24.0 (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting beartype>=0.16.1 (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n",
            "  Downloading beartype-0.16.4-py3-none-any.whl (819 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.1/819.1 kB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ema-pytorch>=0.2.2 (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n",
            "  Downloading ema_pytorch-0.3.1-py3-none-any.whl (4.8 kB)\n",
            "Collecting gateloop-transformer>=0.0.24 (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n",
            "  Downloading gateloop_transformer-0.1.1-py3-none-any.whl (9.7 kB)\n",
            "Collecting local-attention>=1.9.0 (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n",
            "  Downloading local_attention-1.9.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting sentencepiece (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting vector-quantize-pytorch>=1.11.8 (from audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n",
            "  Downloading vector_quantize_pytorch-1.11.8-py3-none-any.whl (24 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (4.2.2)\n",
            "Collecting fastapi (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.7.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading gradio_client-0.7.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (1.5.3)\n",
            "Collecting pydantic>=2.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (0.9.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio-client==0.7.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.24.0->audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32)) (5.9.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (0.12.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.0,>=1.33.6->boto3->-r old_setup_files/requirements-pip.txt (line 14)) (2.8.2)\n",
            "Collecting rotary-embedding-torch (from gateloop-transformer>=0.0.24->audiolm-pytorch->-r old_setup_files/requirements-pip.txt (line 32))\n",
            "  Downloading rotary_embedding_torch-0.4.0-py3-none-any.whl (5.1 kB)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->-r old_setup_files/requirements-pip.txt (line 20)) (0.8.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->-r old_setup_files/requirements-pip.txt (line 22)) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (3.1.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->-r old_setup_files/requirements-pip.txt (line 13)) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (2023.3.post1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->-r old_setup_files/requirements-pip.txt (line 20)) (0.7.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->-r old_setup_files/requirements-pip.txt (line 13)) (4.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r old_setup_files/requirements-pip.txt (line 20)) (0.2.12)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.5 (from pydantic>=2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.1.1 (from librosa->-r old_setup_files/requirements-pip.txt (line 13))\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq->-r old_setup_files/requirements-pip.txt (line 12))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (4.9.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->-r old_setup_files/requirements-pip.txt (line 13)) (3.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (2.21)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (3.2.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (2.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (8.1.7)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore==1.* (from httpx->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->-r old_setup_files/requirements-pip.txt (line 3)) (3.17.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (1.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.34.0->-r old_setup_files/requirements-pip.txt (line 34)) (0.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.0,>=1.33.6->boto3->-r old_setup_files/requirements-pip.txt (line 14)) (1.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq->-r old_setup_files/requirements-pip.txt (line 12)) (1.3.0)\n",
            "Building wheels for collected packages: ffmpeg, fairseq, encodec, antlr4-python3-runtime, ffmpy\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6080 sha256=272d665e4393239a6d208ddb39aa146ef21ceb5d2b8b2ecbe5cec82f47f42841\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291814 sha256=9a0a41759d06549eab64c945d0373297ab7c312db63c8911b0cc5ff411c1140f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for encodec: filename=encodec-0.1.1-py3-none-any.whl size=45759 sha256=a0ccdfab3d19ef4e6cba7c4ebbeac9eeb18e43d8bd3bb567d1092a1664c68aa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/36/cb/81af8b985a5f5e0815312d5e52b41263237af07b977e6bcbf3\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=3484b8fcdddfc8e6b5caaa02bca519797da3a3284e614dbc6454983032c87b66\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=a7b3862833236e4e987dbce170bc02fde52b7b424ab6fb6e3add96918dbc2a73\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpeg fairseq encodec antlr4-python3-runtime ffmpy\n",
            "Installing collected packages: sentencepiece, pydub, funcy, ffmpy, ffmpeg, faiss-cpu, bitarray, antlr4-python3-runtime, websockets, universal-startfile, typing-extensions, tomlkit, sox, shellingham, semantic-version, python-multipart, portalocker, pathvalidate, orjson, jmespath, jedi, h11, ffmpeg-python, einops, colorama, beartype, audio2numpy, annotated-types, aiofiles, uvicorn, starlette, sacrebleu, pydantic-core, omegaconf, httpcore, ffmpeg-downloader, botocore, vector-quantize-pytorch, s3transfer, rotary-embedding-torch, rich-argparse, pydantic, local-attention, hydra-core, httpx, ema-pytorch, diffusers, accelerate, gradio-client, gateloop-transformer, fastapi, fairseq, encodec, boto3, gradio, audiolm-pytorch\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.25.0 aiofiles-23.2.1 annotated-types-0.6.0 antlr4-python3-runtime-4.8 audio2numpy-0.1.2 audiolm-pytorch-1.8.5 beartype-0.16.4 bitarray-2.8.3 boto3-1.33.6 botocore-1.33.6 colorama-0.4.6 diffusers-0.24.0 einops-0.7.0 ema-pytorch-0.3.1 encodec-0.1.1 fairseq-0.12.2 faiss-cpu-1.7.4 fastapi-0.104.1 ffmpeg-1.4 ffmpeg-downloader-0.2.0 ffmpeg-python-0.2.0 ffmpy-0.3.1 funcy-2.0 gateloop-transformer-0.1.1 gradio-4.7.1 gradio-client-0.7.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 hydra-core-1.0.7 jedi-0.19.1 jmespath-1.0.1 local-attention-1.9.0 omegaconf-2.0.6 orjson-3.9.10 pathvalidate-3.2.0 portalocker-2.8.2 pydantic-2.5.2 pydantic-core-2.14.5 pydub-0.25.1 python-multipart-0.0.6 rich-argparse-1.4.0 rotary-embedding-torch-0.4.0 s3transfer-0.8.2 sacrebleu-2.3.3 semantic-version-2.10.0 sentencepiece-0.1.99 shellingham-1.5.4 sox-1.4.1 starlette-0.27.0 tomlkit-0.12.0 typing-extensions-4.8.0 universal-startfile-0.2 uvicorn-0.24.0.post1 vector-quantize-pytorch-1.11.8 websockets-11.0.3\n",
            "Requirement already satisfied: encodec in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: rich-argparse in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from encodec) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from encodec) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from encodec) (2.1.0+cu118)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from encodec) (0.7.0)\n",
            "Requirement already satisfied: rich>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from rich-argparse) (13.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.0.0->rich-argparse) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.0.0->rich-argparse) (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->encodec) (2.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.0.0->rich-argparse) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->encodec) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->encodec) (1.3.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Collecting devtools\n",
            "  Downloading devtools-0.12.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.8.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
            "Collecting asttokens<3.0.0,>=2.0.0 (from devtools)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting executing>=1.1.1 (from devtools)\n",
            "  Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pygments>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from devtools) (2.16.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens<3.0.0,>=2.0.0->devtools) (1.16.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.11.17)\n",
            "Installing collected packages: executing, asttokens, devtools\n",
            "Successfully installed asttokens-2.4.1 devtools-0.12.2 executing-2.0.1\n"
          ]
        }
      ],
      "source": [
        "#@title 0.1 - Install system\n",
        "from IPython.display import clear_output\n",
        "!git clone https://github.com/JonathanFly/bark.git\n",
        "%cd bark\n",
        "!pip install -r old_setup_files/requirements-pip.txt\n",
        "!pip install encodec rich-argparse\n",
        "!pip install librosa pydub devtools\n",
        "\n",
        "#clear_output()\n",
        "#print('Cell completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jKTvqvVkOwXM"
      },
      "outputs": [],
      "source": [
        " #@title 0.2 - Setup required functions and helpers\n",
        "import os\n",
        "import time\n",
        "from bark_infinity import config\n",
        "import numpy as np\n",
        "\n",
        "logger = config.logger\n",
        "logger.setLevel(\"WARNING\")\n",
        "\n",
        "from bark_infinity import generation #이걸 보고.. conversion 가능하게 수정..\n",
        "from bark_infinity import api\n",
        "\n",
        "import rich\n",
        "from rich import print\n",
        "from rich import pretty\n",
        "from rich.pretty import pprint\n",
        "from rich import inspect\n",
        "\n",
        "import librosa\n",
        "from pydub import AudioSegment\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Audio\n",
        "from io import BytesIO\n",
        "\n",
        "# None of this code, just fiddlign with Colab stuff\n",
        "# Just to save Colab with outputs and float32 wavs are GIGANTO\n",
        "# actually this doesn't work, the iPython widget converts it back to float32? or I messed up\n",
        "\n",
        "def display_audio_int16_but(audio_arr_segments, file_name, sample_rate=generation.SAMPLE_RATE,  width='200px'):\n",
        "    file_name_label = widgets.Label(value=f\"Playing: {file_name}\")\n",
        "    file_name_label.layout.width = width\n",
        "    audio_data_int16 = audio_arr_segments\n",
        "    if isinstance(audio_data_int16, list):\n",
        "        audio_data_int16 = np.concatenate(audio_data_int16)\n",
        "\n",
        "    #audio_data_int16 = np.int16(audio_data_int16 * np.iinfo(np.int16).max)\n",
        "\n",
        "\n",
        "    audio_widget = Audio(audio_data_int16, rate=sample_rate)\n",
        "    display(file_name_label, audio_widget)\n",
        "\n",
        "\n",
        "def on_button_click(button):\n",
        "    audio_data, sample_rate = librosa.load(button.wav_path, sr=None)\n",
        "    file_name = os.path.basename(button.wav_path)\n",
        "    display_audio_int16_but(audio_data,file_name, sample_rate)\n",
        "\n",
        "def display_wav_files(directory, matchType=\".wav\"):\n",
        "    subdirs, wav_files = [], []\n",
        "\n",
        "    for item in os.listdir(directory):\n",
        "        item_path = os.path.join(directory, item)\n",
        "\n",
        "        if os.path.isfile(item_path) and item_path.endswith(matchType):\n",
        "            wav_files.append(item_path)\n",
        "        elif os.path.isdir(item_path):\n",
        "            subdirs.append(item_path)\n",
        "\n",
        "    wav_files.sort(key=lambda x: os.path.basename(x))\n",
        "\n",
        "    for wav_file in wav_files:\n",
        "\n",
        "        filename = os.path.basename(wav_file)\n",
        "        print(f\" {filename}\")\n",
        "        display( Audio(filename=wav_file, rate=generation.SAMPLE_RATE) )\n",
        "        #button = widgets.Button(description=f\"Play {filename}\")\n",
        "        #button.wav_path = wav_file\n",
        "        #button.on_click(on_button_click)\n",
        "        #display(button)\n",
        "\n",
        "    for subdir in sorted(subdirs):\n",
        "        print(f\"<{subdir}>\")\n",
        "        display_wav_files(subdir, matchType)\n",
        "\n",
        "def display_mp4_files(directory):\n",
        "    return display_wav_files(directory, '.mp4')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbIE0Bv8jxtN"
      },
      "source": [
        "### Gradio App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQfEqnxMpUk1",
        "outputId": "6900e7ee-923d-4f44-f5bd-b176de660216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-03 17:44:58.200789: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-03 17:44:58.200851: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-03 17:44:58.200876: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-03 17:44:59.141394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Pytorch version: 2.1\n",
            "=== GPU Information ===\n",
            "GPU Device: Tesla T4\n",
            "Total memory: 14.74786376953125 GB\n",
            "CUDA Version: 11.8\n",
            "PyTorch Version: 2.1.0+cu118\n",
            "   GPU Memory Free: 14.65 GB, Total: 14.75 GB\n",
            "   >9 GB Memory Free, CPU Offloading Disabled.\n",
            "OFFLOAD_CPU: False (Default is True for < 9GB GPU Memory Free)\n",
            "USE_SMALL_MODELS: False (Default is False)\n",
            "GLOBAL_ENABLE_MPS (Apple): False (Default is False)\n",
            "SUNO_HALF_PRECISION: False (Default is False)\n",
            "SUNO_HALF_BFLOAT16: False (Default is False)\n",
            "SUNO_DISABLE_COMPILE: False (Default is False)\n",
            "SUNO_USE_DIRECTML (AMD): False (Default is False)\n",
            "Torch Num CPU Threads: 4\n",
            "Bark Model Location: /root/.cache/suno/bark_v0 (Env var 'XDG_CACHE_HOME' to override)\n",
            "HF_HOME: /content/bark/bark_infinity/data/models/unclassified\n",
            "\n",
            "FFmpeg status, this should say version 6.0\n",
            "FFmpeg binaries directory: None\n",
            "FFmpeg Version: None\n",
            "FFmpeg Path: /root/.local/share/ffmpeg-downloader/ffmpeg/ffmpeg\n",
            "FFprobe Path: /root/.local/share/ffmpeg-downloader/ffmpeg/ffprobe\n",
            "FFplay Path: /root/.local/share/ffmpeg-downloader/ffmpeg/ffplay\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/bark/bark_webui.py\", line 1520, in <module>\n",
            "    audio_prompt_input = gr.Audio(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/component_meta.py\", line 152, in wrapper\n",
            "    return fn(self, **kwargs)\n",
            "TypeError: Audio.__init__() got an unexpected keyword argument 'info'\n"
          ]
        }
      ],
      "source": [
        "#@markdown Run the WebUI with all features.<br>\n",
        "#@markdown When loaded click the second link to launch WebUI in another window.\n",
        "!python bark_webui.py --share"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPlQyNOS-jNf"
      },
      "source": [
        "## TRAIN-VAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKLe_gYkQ59l"
      },
      "source": [
        "### hubert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsEiB211L-G6"
      },
      "outputs": [],
      "source": [
        "from bark.generation import _load_codec_model, generate_text_semantic\n",
        "from encodec.utils import convert_audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBfHNXKJMYq2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os.path\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.serialization import MAP_LOCATION\n",
        "\n",
        "\n",
        "class CustomTokenizer(nn.Module):\n",
        "    def __init__(self, hidden_size=1024, input_size=768, output_size=10000, version=0):\n",
        "        super(CustomTokenizer, self).__init__()\n",
        "        next_size = input_size\n",
        "        if version == 0:\n",
        "            self.lstm = nn.LSTM(input_size, hidden_size, 2, batch_first=True)\n",
        "            next_size = hidden_size\n",
        "        if version == 1:\n",
        "            self.lstm = nn.LSTM(input_size, hidden_size, 2, batch_first=True)\n",
        "            self.intermediate = nn.Linear(hidden_size, 4096)\n",
        "            next_size = 4096\n",
        "\n",
        "        self.fc = nn.Linear(next_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.optimizer: optim.Optimizer = None\n",
        "        self.lossfunc = nn.CrossEntropyLoss()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.version = version\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        if self.version == 1:\n",
        "            x = self.intermediate(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_token(self, x):\n",
        "        \"\"\"\n",
        "        Used to get the token for the first\n",
        "        :param x: An array with shape (N, input_size) where N is a whole number greater or equal to 1, and input_size is the input size used when creating the model.\n",
        "        :return: An array with shape (N,) where N is the same as N from the input. Every number in the array is a whole number in range 0...output_size - 1 where output_size is the output size used when creating the model.\n",
        "        \"\"\"\n",
        "        return torch.argmax(self(x), dim=1)\n",
        "\n",
        "    def prepare_training(self):\n",
        "        self.optimizer = optim.Adam(self.parameters(), 0.001)\n",
        "\n",
        "    def train_step(self, x_train, y_train, log_loss=False):\n",
        "        # y_train = y_train[:-1]\n",
        "        # y_train = y_train[1:]\n",
        "\n",
        "        optimizer = self.optimizer\n",
        "        lossfunc = self.lossfunc\n",
        "        # Zero the gradients\n",
        "        self.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = self(x_train)\n",
        "\n",
        "        y_train_len = len(y_train)\n",
        "        y_pred_len = y_pred.shape[0]\n",
        "\n",
        "        if y_train_len > y_pred_len:\n",
        "            diff = y_train_len - y_pred_len\n",
        "            y_train = y_train[diff:]\n",
        "        elif y_train_len < y_pred_len:\n",
        "            diff = y_pred_len - y_train_len\n",
        "            y_pred = y_pred[:-diff, :]\n",
        "\n",
        "        y_train_hot = torch.zeros(len(y_train), self.output_size)\n",
        "        y_train_hot[range(len(y_train)), y_train] = 1\n",
        "        y_train_hot = y_train_hot.to('cuda')\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = lossfunc(y_pred, y_train_hot)\n",
        "\n",
        "        # Print loss\n",
        "        if log_loss:\n",
        "            print('Loss', loss.item())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "    def save(self, path):\n",
        "        info_path = '.'.join(os.path.basename(path).split('.')[:-1]) + '/.info'\n",
        "        torch.save(self.state_dict(), path)\n",
        "        data_from_model = Data(self.input_size, self.hidden_size, self.output_size, self.version)\n",
        "        with ZipFile(path, 'a') as model_zip:\n",
        "            model_zip.writestr(info_path, data_from_model.save())\n",
        "            model_zip.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def load_from_checkpoint(path, map_location: MAP_LOCATION = None):\n",
        "        old = True\n",
        "        with ZipFile(path) as model_zip:\n",
        "            filesMatch = [file for file in model_zip.namelist() if file.endswith('/.info')]\n",
        "            file = filesMatch[0] if filesMatch else None\n",
        "            if file:\n",
        "                old = False\n",
        "                data_from_model = Data.load(model_zip.read(file).decode('utf-8'))\n",
        "            model_zip.close()\n",
        "        if old:\n",
        "            model = CustomTokenizer()\n",
        "        else:\n",
        "            model = CustomTokenizer(data_from_model.hidden_size, data_from_model.input_size, data_from_model.output_size, data_from_model.version)\n",
        "        model.load_state_dict(torch.load(path))\n",
        "        if map_location:\n",
        "            model = model.to(map_location)\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "class Data:\n",
        "    input_size: int\n",
        "    hidden_size: int\n",
        "    output_size: int\n",
        "    version: int\n",
        "\n",
        "    def __init__(self, input_size=768, hidden_size=1024, output_size=10000, version=0):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.version = version\n",
        "\n",
        "    @staticmethod\n",
        "    def load(string):\n",
        "        data = json.loads(string)\n",
        "        return Data(data['input_size'], data['hidden_size'], data['output_size'], data['version'])\n",
        "\n",
        "    def save(self):\n",
        "        data = {\n",
        "            'input_size': self.input_size,\n",
        "            'hidden_size': self.hidden_size,\n",
        "            'output_size': self.output_size,\n",
        "            'version': self.version,\n",
        "        }\n",
        "        return json.dumps(data)\n",
        "\n",
        "\n",
        "def auto_train(data_path, save_path='model.pth', load_model: str | None = None, save_epochs=1):\n",
        "    data_x, data_y = [], []\n",
        "\n",
        "    if load_model and os.path.isfile(load_model):\n",
        "        print('Loading model from', load_model)\n",
        "        model_training = CustomTokenizer.load_from_checkpoint(load_model, 'cuda')\n",
        "    else:\n",
        "        print('Creating new model.')\n",
        "        model_training = CustomTokenizer(version=1).to('cuda')  # Settings for the model to run without lstm\n",
        "    save_path = os.path.join(data_path, save_path)\n",
        "    base_save_path = '.'.join(save_path.split('.')[:-1])\n",
        "\n",
        "    sem_string = '_semantic.npy'\n",
        "    feat_string = '_semantic_features.npy'\n",
        "\n",
        "    ready = os.path.join(data_path, 'ready')\n",
        "    for input_file in os.listdir(ready):\n",
        "        full_path = os.path.join(ready, input_file)\n",
        "        if input_file.endswith(sem_string):\n",
        "            data_y.append(numpy.load(full_path))\n",
        "        elif input_file.endswith(feat_string):\n",
        "            data_x.append(numpy.load(full_path))\n",
        "    model_training.prepare_training()\n",
        "\n",
        "    epoch = 1\n",
        "\n",
        "    while 1:\n",
        "        for i in range(save_epochs):\n",
        "            j = 0\n",
        "            for x, y in zip(data_x, data_y):\n",
        "                model_training.train_step(torch.tensor(x).to('cuda'), torch.tensor(y).to('cuda'), j % 50 == 0)  # Print loss every 50 steps\n",
        "                j += 1\n",
        "        save_p = save_path\n",
        "        save_p_2 = f'{base_save_path}_epoch_{epoch}.pth'\n",
        "        model_training.save(save_p)\n",
        "        model_training.save(save_p_2)\n",
        "        print(f'Epoch {epoch} completed')\n",
        "        epoch += 1\n",
        "\n",
        "import os.path\n",
        "import shutil\n",
        "import urllib.request\n",
        "\n",
        "import huggingface_hub\n",
        "\n",
        "\n",
        "class HuBERTManager:\n",
        "    @staticmethod\n",
        "    def make_sure_hubert_installed(download_url: str = 'https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt', file_name: str = 'hubert.pt'):\n",
        "        install_dir = os.path.join('data', 'models', 'hubert')\n",
        "        if not os.path.isdir(install_dir):\n",
        "            os.makedirs(install_dir, exist_ok=True)\n",
        "        install_file = os.path.join(install_dir, file_name)\n",
        "        if not os.path.isfile(install_file):\n",
        "            print('Downloading HuBERT base model')\n",
        "            urllib.request.urlretrieve(download_url, install_file)\n",
        "            print('Downloaded HuBERT')\n",
        "        return install_file\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def make_sure_tokenizer_installed(model: str = 'quantifier_hubert_base_ls960_14.pth', repo: str = 'GitMylo/bark-voice-cloning', local_file: str = 'tokenizer.pth'):\n",
        "        install_dir = os.path.join('data', 'models', 'hubert')\n",
        "        if not os.path.isdir(install_dir):\n",
        "            os.makedirs(install_dir, exist_ok=True)\n",
        "        install_file = os.path.join(install_dir, local_file)\n",
        "        if not os.path.isfile(install_file):\n",
        "            print('Downloading HuBERT custom tokenizer')\n",
        "            huggingface_hub.hf_hub_download(repo, model, local_dir=install_dir, local_dir_use_symlinks=False)\n",
        "            shutil.move(os.path.join(install_dir, model), install_file)\n",
        "            print('Downloaded tokenizer')\n",
        "        return install_file\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from einops import pack, unpack\n",
        "\n",
        "import fairseq\n",
        "\n",
        "from torchaudio.functional import resample\n",
        "\n",
        "from audiolm_pytorch.utils import curtail_to_multiple\n",
        "\n",
        "import logging\n",
        "logging.root.setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "\n",
        "class CustomHubert(nn.Module):\n",
        "    \"\"\"\n",
        "    checkpoint and kmeans can be downloaded at https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n",
        "    or you can train your own\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        checkpoint_path,\n",
        "        target_sample_hz=16000,\n",
        "        seq_len_multiple_of=None,\n",
        "        output_layer=9,\n",
        "        device=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.target_sample_hz = target_sample_hz\n",
        "        self.seq_len_multiple_of = seq_len_multiple_of\n",
        "        self.output_layer = output_layer\n",
        "\n",
        "        if device is not None:\n",
        "            self.to(device)\n",
        "\n",
        "        model_path = Path(checkpoint_path)\n",
        "\n",
        "        assert model_path.exists(), f'path {checkpoint_path} does not exist'\n",
        "\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        load_model_input = {checkpoint_path: checkpoint}\n",
        "        model, *_ = fairseq.checkpoint_utils.load_model_ensemble_and_task(load_model_input)\n",
        "\n",
        "        if device is not None:\n",
        "            model[0].to(device)\n",
        "\n",
        "        self.model = model[0]\n",
        "        self.model.eval()\n",
        "\n",
        "    @property\n",
        "    def groups(self):\n",
        "        return 1\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(\n",
        "        self,\n",
        "        wav_input,\n",
        "        flatten=True,\n",
        "        input_sample_hz=None\n",
        "    ):\n",
        "        device = wav_input.device\n",
        "\n",
        "        if exists(input_sample_hz):\n",
        "            wav_input = resample(wav_input, input_sample_hz, self.target_sample_hz)\n",
        "\n",
        "        if exists(self.seq_len_multiple_of):\n",
        "            wav_input = curtail_to_multiple(wav_input, self.seq_len_multiple_of)\n",
        "\n",
        "        embed = self.model(\n",
        "            wav_input,\n",
        "            features_only=True,\n",
        "            mask=False,  # thanks to @maitycyrus for noticing that mask is defaulted to True in the fairseq code\n",
        "            output_layer=self.output_layer\n",
        "        )\n",
        "\n",
        "        embed, packed_shape = pack([embed['x']], '* d')\n",
        "\n",
        "        # codebook_indices = self.kmeans.predict(embed.cpu().detach().numpy())\n",
        "\n",
        "        codebook_indices = torch.from_numpy(embed.cpu().detach().numpy()).to(device)  # .long()\n",
        "\n",
        "        if flatten:\n",
        "            return codebook_indices\n",
        "\n",
        "        codebook_indices, = unpack(codebook_indices, packed_shape, '*')\n",
        "        return codebook_indices\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7x4hpdZ1U8-"
      },
      "source": [
        "### import things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-KEiFN9Di65",
        "outputId": "3106e7b7-17f0-4776-fd2d-dc44ceafbc92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bark/bark\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import json\n",
        "import math\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import logging\n",
        "import torchaudio\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn.functional as F\n",
        "from encodec.utils import convert_audio\n",
        "from accelerate import Accelerator\n",
        "from accelerate.utils import set_seed\n",
        "from transformers import BertTokenizer\n",
        "from huggingface_hub import hf_hub_download\n",
        "from packaging import version\n",
        "from diffusers.optimization import get_scheduler\n",
        "%cd /content/bark/bark\n",
        "from model import GPTConfig, GPT\n",
        "from model_fine import FineGPT, FineGPTConfig #model 앞 . 없애"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JkPRFa4IfUD"
      },
      "source": [
        "### 없는 모듈 설치 및 드라이브 접근"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8VRKfbI8znI",
        "outputId": "f49ca38c-f1ca-4137-b790-adb7c6b22dc3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbrJr9DWHM2L",
        "outputId": "6e3e2794-62eb-44b5-cd42-2934bf07ca13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "\n",
        "import warnings\n",
        "import sys\n",
        "import importlib.util\n",
        "from copy import deepcopy\n",
        "import copy\n",
        "import json\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from typing import Any, Tuple, Union, Dict\n",
        "\n",
        "from packaging import version\n",
        "\n",
        "if sys.version_info < (3, 8):\n",
        "    import importlib_metadata\n",
        "else:\n",
        "    import importlib.metadata as importlib_metadata\n",
        "\n",
        "\n",
        "def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[Tuple[bool, str], bool]:\n",
        "    # Check we're not importing a \"pkg_name\" directory somewhere but the actual library by trying to grab the version\n",
        "    package_exists = importlib.util.find_spec(pkg_name) is not None\n",
        "    package_version = \"N/A\"\n",
        "    if package_exists:\n",
        "        try:\n",
        "            package_version = importlib_metadata.version(pkg_name)\n",
        "            package_exists = True\n",
        "        except importlib_metadata.PackageNotFoundError:\n",
        "            package_exists = False\n",
        "    if return_version:\n",
        "        return package_exists, package_version\n",
        "    else:\n",
        "        return package_exists\n",
        "\n",
        "_accelerate_available, _accelerate_version = _is_package_available(\"accelerate\", return_version=True)\n",
        "_bitsandbytes_available = _is_package_available(\"bitsandbytes\")\n",
        "_torch_available, _torch_version = _is_package_available(\"torch\", return_version=True)\n",
        "\n",
        "def is_accelerate_available(check_partial_state=False):\n",
        "    if check_partial_state:\n",
        "        return _accelerate_available and version.parse(_accelerate_version) >= version.parse(\"0.19.0\")\n",
        "    return _accelerate_available\n",
        "\n",
        "def is_bitsandbytes_available():\n",
        "    return _bitsandbytes_available\n",
        "\n",
        "def is_torch_available():\n",
        "    return _torch_available\n",
        "\n",
        "if is_bitsandbytes_available():\n",
        "    import bitsandbytes as bnb\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "\n",
        "if is_accelerate_available():\n",
        "    from accelerate import init_empty_weights\n",
        "    from accelerate.utils import find_tied_parameters\n",
        "\n",
        "\n",
        "def set_module_quantized_tensor_to_device(module, tensor_name, device, value=None, fp16_statistics=None):\n",
        "    \"\"\"\n",
        "    A helper function to set a given tensor (parameter of buffer) of a module on a specific device (note that doing\n",
        "    `param.to(device)` creates a new tensor not linked to the parameter, which is why we need this function). The\n",
        "    function is adapted from `set_module_tensor_to_device` function from accelerate that is adapted to support the\n",
        "    class `Int8Params` from `bitsandbytes`.\n",
        "\n",
        "    Args:\n",
        "        module (`torch.nn.Module`):\n",
        "            The module in which the tensor we want to move lives.\n",
        "        tensor_name (`str`):\n",
        "            The full name of the parameter/buffer.\n",
        "        device (`int`, `str` or `torch.device`):\n",
        "            The device on which to set the tensor.\n",
        "        value (`torch.Tensor`, *optional*):\n",
        "            The value of the tensor (useful when going from the meta device to any other device).\n",
        "        fp16_statistics (`torch.HalfTensor`, *optional*):\n",
        "            The list of fp16 statistics to set on the module, used for serialization.\n",
        "    \"\"\"\n",
        "    # Recurse if needed\n",
        "    if \".\" in tensor_name:\n",
        "        splits = tensor_name.split(\".\")\n",
        "        for split in splits[:-1]:\n",
        "            new_module = getattr(module, split)\n",
        "            if new_module is None:\n",
        "                raise ValueError(f\"{module} has no attribute {split}.\")\n",
        "            module = new_module\n",
        "        tensor_name = splits[-1]\n",
        "\n",
        "    if tensor_name not in module._parameters and tensor_name not in module._buffers:\n",
        "        raise ValueError(f\"{module} does not have a parameter or a buffer named {tensor_name}.\")\n",
        "    is_buffer = tensor_name in module._buffers\n",
        "    old_value = getattr(module, tensor_name)\n",
        "\n",
        "    if old_value.device == torch.device(\"meta\") and device not in [\"meta\", torch.device(\"meta\")] and value is None:\n",
        "        raise ValueError(f\"{tensor_name} is on the meta device, we need a `value` to put in on {device}.\")\n",
        "\n",
        "    is_4bit = False\n",
        "    is_8bit = False\n",
        "    if is_buffer or not is_bitsandbytes_available():\n",
        "        is_8bit = False\n",
        "        is_4bit = False\n",
        "    else:\n",
        "        is_4bit = hasattr(bnb.nn, \"Params4bit\") and isinstance(module._parameters[tensor_name], bnb.nn.Params4bit)\n",
        "        is_8bit = isinstance(module._parameters[tensor_name], bnb.nn.Int8Params)\n",
        "\n",
        "    if is_8bit or is_4bit:\n",
        "        param = module._parameters[tensor_name]\n",
        "        if param.device.type != \"cuda\":\n",
        "            if value is None:\n",
        "                new_value = old_value.to(device)\n",
        "            elif isinstance(value, torch.Tensor):\n",
        "                new_value = value.to(\"cpu\")\n",
        "                if value.dtype == torch.int8:\n",
        "                    is_8bit_serializable = version.parse(importlib_metadata.version(\"bitsandbytes\")) > version.parse(\n",
        "                        \"0.37.2\"\n",
        "                    )\n",
        "                    if not is_8bit_serializable:\n",
        "                        raise ValueError(\n",
        "                            \"Detected int8 weights but the version of bitsandbytes is not compatible with int8 serialization. \"\n",
        "                            \"Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`.\"\n",
        "                        )\n",
        "            else:\n",
        "                new_value = torch.tensor(value, device=\"cpu\")\n",
        "\n",
        "            kwargs = old_value.__dict__\n",
        "            if is_8bit:\n",
        "                new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(device)\n",
        "            elif is_4bit:\n",
        "                new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(device)\n",
        "\n",
        "            module._parameters[tensor_name] = new_value\n",
        "            if fp16_statistics is not None:\n",
        "                setattr(module.weight, \"SCB\", fp16_statistics.to(device))\n",
        "\n",
        "    else:\n",
        "        if value is None:\n",
        "            new_value = old_value.to(device)\n",
        "        elif isinstance(value, torch.Tensor):\n",
        "            new_value = value.to(device)\n",
        "        else:\n",
        "            new_value = torch.tensor(value, device=device)\n",
        "\n",
        "        if is_buffer:\n",
        "            module._buffers[tensor_name] = new_value\n",
        "        else:\n",
        "            new_value = nn.Parameter(new_value, requires_grad=old_value.requires_grad)\n",
        "            module._parameters[tensor_name] = new_value\n",
        "\n",
        "\n",
        "def replace_with_bnb_linear(model, modules_to_not_convert=None, current_key_name=None, quantization_config=None):\n",
        "    \"\"\"\n",
        "    A helper function to replace all `torch.nn.Linear` modules by `bnb.nn.Linear8bit` modules from the `bitsandbytes`\n",
        "    library. This will enable running your models using mixed int8 precision as described by the paper `LLM.int8():\n",
        "    8-bit Matrix Multiplication for Transformers at Scale`. Make sure `bitsandbytes` compiled with the correct CUDA\n",
        "    version of your hardware is installed before running this function. `pip install -i https://test.pypi.org/simple/\n",
        "    bitsandbytes`\n",
        "\n",
        "    The function will be run recursively and replace all `torch.nn.Linear` modules except for the `lm_head` that should\n",
        "    be kept as a `torch.nn.Linear` module. The replacement is done under `init_empty_weights` context manager so no\n",
        "    CPU/GPU memory is required to run this function. Int8 mixed-precision matrix decomposition works by separating a\n",
        "    matrix multiplication into two streams: (1) and systematic feature outlier stream matrix multiplied in fp16\n",
        "    (0.01%), (2) a regular stream of int8 matrix multiplication (99.9%). With this method, int8 inference with no\n",
        "    predictive degradation is possible for very large models (>=176B parameters).\n",
        "\n",
        "    Parameters:\n",
        "        model (`torch.nn.Module`):\n",
        "            Input model or `torch.nn.Module` as the function is run recursively.\n",
        "        modules_to_not_convert (`List[`str`]`, *optional*, defaults to `[\"lm_head\"]`):\n",
        "            Names of the modules to not convert in `Linear8bitLt`. In practice we keep the `lm_head` in full precision\n",
        "            for numerical stability reasons.\n",
        "        current_key_name (`List[`str`]`, *optional*):\n",
        "            An array to track the current key of the recursion. This is used to check whether the current key (part of\n",
        "            it) is not in the list of modules to not convert (for instances modules that are offloaded to `cpu` or\n",
        "            `disk`).\n",
        "    \"\"\"\n",
        "    modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n",
        "    for name, module in model.named_children():\n",
        "        if current_key_name is None:\n",
        "            current_key_name = []\n",
        "\n",
        "        if isinstance(module, nn.Linear) and name not in modules_to_not_convert:\n",
        "            # Check if the current key is not in the `modules_to_not_convert`\n",
        "            if not any(key in \".\".join(current_key_name) for key in modules_to_not_convert):\n",
        "                with init_empty_weights():\n",
        "                    if quantization_config.quantization_method() == \"llm_int8\":\n",
        "                        model._modules[name] = bnb.nn.Linear8bitLt(\n",
        "                            module.in_features,\n",
        "                            module.out_features,\n",
        "                            module.bias is not None,\n",
        "                            has_fp16_weights=quantization_config.llm_int8_has_fp16_weight,\n",
        "                            threshold=quantization_config.llm_int8_threshold,\n",
        "                        )\n",
        "                    else:\n",
        "                        if (\n",
        "                            quantization_config.llm_int8_skip_modules is not None\n",
        "                            and name in quantization_config.llm_int8_skip_modules\n",
        "                        ):\n",
        "                            pass\n",
        "                        else:\n",
        "                            model._modules[name] = bnb.nn.Linear4bit(\n",
        "                                module.in_features,\n",
        "                                module.out_features,\n",
        "                                module.bias is not None,\n",
        "                                quantization_config.bnb_4bit_compute_dtype,\n",
        "                                compress_statistics=quantization_config.bnb_4bit_use_double_quant,\n",
        "                                quant_type=quantization_config.bnb_4bit_quant_type,\n",
        "                            )\n",
        "                    # Force requires grad to False to avoid unexpected errors\n",
        "                    model._modules[name].requires_grad_(False)\n",
        "        # Remove the last key for recursion\n",
        "        if len(list(module.children())) > 0:\n",
        "            replace_with_bnb_linear(\n",
        "                module,\n",
        "                modules_to_not_convert,\n",
        "                current_key_name,\n",
        "                quantization_config,\n",
        "            )\n",
        "    return model\n",
        "\n",
        "\n",
        "# For backward compatibility\n",
        "def replace_8bit_linear(*args, **kwargs):\n",
        "    warnings.warn(\n",
        "        \"`replace_8bit_linear` will be deprecated in a future version, please use `replace_with_bnb_linear` instead\",\n",
        "        FutureWarning,\n",
        "    )\n",
        "    return replace_with_bnb_linear(*args, **kwargs)\n",
        "\n",
        "\n",
        "# For backward compatiblity\n",
        "def set_module_8bit_tensor_to_device(*args, **kwargs):\n",
        "    warnings.warn(\n",
        "        \"`set_module_8bit_tensor_to_device` will be deprecated in a future version, please use `set_module_quantized_tensor_to_device` instead\",\n",
        "        FutureWarning,\n",
        "    )\n",
        "    return set_module_quantized_tensor_to_device(*args, **kwargs)\n",
        "\n",
        "\n",
        "def get_keys_to_not_convert(model):\n",
        "    r\"\"\"\n",
        "    An utility function to get the key of the module to keep in full precision if any For example for CausalLM modules\n",
        "    we may want to keep the lm_head in full precision for numerical stability reasons. For other architectures, we want\n",
        "    to keep the tied weights of the model. The function will return a list of the keys of the modules to not convert in\n",
        "    int8.\n",
        "\n",
        "    Parameters:\n",
        "    model (`torch.nn.Module`):\n",
        "        Input model\n",
        "    \"\"\"\n",
        "    # Create a copy of the model and tie the weights, then\n",
        "    # check if it contains tied weights\n",
        "    tied_model = deepcopy(model)  # this has 0 cost since it is done inside `init_empty_weights` context manager`\n",
        "    tied_model.tie_weights()\n",
        "\n",
        "    tied_params = find_tied_parameters(tied_model)\n",
        "    # For compatibility with Accelerate < 0.18\n",
        "    if isinstance(tied_params, dict):\n",
        "        tied_keys = list(tied_params.values())\n",
        "    else:\n",
        "        tied_keys = sum([x[1:] for x in tied_params], [])\n",
        "    has_tied_params = len(tied_keys) > 0\n",
        "\n",
        "    # Check if it is a base model\n",
        "    is_base_model = not hasattr(model, model.base_model_prefix)\n",
        "\n",
        "    # Ignore this for base models (BertModel, GPT2Model, etc.)\n",
        "    if (not has_tied_params) and is_base_model:\n",
        "        return []\n",
        "\n",
        "    # otherwise they have an attached head\n",
        "    list_modules = list(model.named_parameters())\n",
        "    list_last_module = [list_modules[-1][0]]\n",
        "\n",
        "    # add last module together with tied weights\n",
        "    intersection = set(list_last_module) - set(tied_keys)\n",
        "    list_untouched = tied_keys + list(intersection)\n",
        "\n",
        "    # remove \".weight\" from the keys\n",
        "    names_to_remove = [\".weight\", \".bias\"]\n",
        "    filtered_module_names = []\n",
        "    for name in list_untouched:\n",
        "        for name_to_remove in names_to_remove:\n",
        "            if name_to_remove in name:\n",
        "                name = name.replace(name_to_remove, \"\")\n",
        "        filtered_module_names.append(name)\n",
        "\n",
        "    return filtered_module_names\n",
        "\n",
        "#!/usr/bin/env python\n",
        "# coding=utf-8\n",
        "\n",
        "# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\n",
        "\n",
        "if is_torch_available():\n",
        "    import torch\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BitsAndBytesConfig:\n",
        "    \"\"\"\n",
        "    This is a wrapper class about all possible attributes and features that you can play with a model that has been\n",
        "    loaded using `bitsandbytes`.\n",
        "\n",
        "    This replaces `load_in_8bit` or `load_in_4bit`therefore both options are mutually exclusive.\n",
        "\n",
        "    Currently only supports `LLM.int8()`, `FP4`, and `NF4` quantization. If more methods are added to `bitsandbytes`,\n",
        "    then more arguments will be added to this class.\n",
        "\n",
        "    Args:\n",
        "        load_in_8bit (`bool`, *optional*, defaults to `False`):\n",
        "            This flag is used to enable 8-bit quantization with LLM.int8().\n",
        "        load_in_4bit (`bool`, *optional*, defaults to `False`):\n",
        "            This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from\n",
        "            `bitsandbytes`.\n",
        "        llm_int8_threshold (`float`, *optional*, defaults to 6):\n",
        "            This corresponds to the outlier threshold for outlier detection as described in `LLM.int8() : 8-bit Matrix\n",
        "            Multiplication for Transformers at Scale` paper: https://arxiv.org/abs/2208.07339 Any hidden states value\n",
        "            that is above this threshold will be considered an outlier and the operation on those values will be done\n",
        "            in fp16. Values are usually normally distributed, that is, most values are in the range [-3.5, 3.5], but\n",
        "            there are some exceptional systematic outliers that are very differently distributed for large models.\n",
        "            These outliers are often in the interval [-60, -6] or [6, 60]. Int8 quantization works well for values of\n",
        "            magnitude ~5, but beyond that, there is a significant performance penalty. A good default threshold is 6,\n",
        "            but a lower threshold might be needed for more unstable models (small models, fine-tuning).\n",
        "        llm_int8_skip_modules (`List[str]`, *optional*):\n",
        "            An explicit list of the modules that we do not want to convert in 8-bit. This is useful for models such as\n",
        "            Jukebox that has several heads in different places and not necessarily at the last position. For example\n",
        "            for `CausalLM` models, the last `lm_head` is kept in its original `dtype`.\n",
        "        llm_int8_enable_fp32_cpu_offload (`bool`, *optional*, defaults to `False`):\n",
        "            This flag is used for advanced use cases and users that are aware of this feature. If you want to split\n",
        "            your model in different parts and run some parts in int8 on GPU and some parts in fp32 on CPU, you can use\n",
        "            this flag. This is useful for offloading large models such as `google/flan-t5-xxl`. Note that the int8\n",
        "            operations will not be run on CPU.\n",
        "        llm_int8_has_fp16_weight (`bool`, *optional*, defaults to `False`):\n",
        "            This flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning as the weights do not\n",
        "            have to be converted back and forth for the backward pass.\n",
        "        bnb_4bit_compute_dtype (`torch.dtype` or str, *optional*, defaults to `torch.float32`):\n",
        "            This sets the computational type which might be different than the input time. For example, inputs might be\n",
        "            fp32, but computation can be set to bf16 for speedups.\n",
        "        bnb_4bit_quant_type (`str`, {fp4, fn4}, defaults to `fp4`):\n",
        "            This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and NF4 data types\n",
        "            which are specified by `fp4` or `fn4`.\n",
        "        bnb_4bit_use_double_quant (`bool`, *optional*, defaults to `False`):\n",
        "            This flag is used for nested quantization where the quantization constants from the first quantization are\n",
        "            quantized again.\n",
        "        kwargs (`Dict[str, Any]`, *optional*):\n",
        "            Additional parameters from which to initialize the configuration object.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        load_in_8bit=False,\n",
        "        load_in_4bit=False,\n",
        "        llm_int8_threshold=6.0,\n",
        "        llm_int8_skip_modules=None,\n",
        "        llm_int8_enable_fp32_cpu_offload=False,\n",
        "        llm_int8_has_fp16_weight=False,\n",
        "        bnb_4bit_compute_dtype=None,\n",
        "        bnb_4bit_quant_type=\"fp4\",\n",
        "        bnb_4bit_use_double_quant=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.load_in_8bit = load_in_8bit\n",
        "        self.load_in_4bit = load_in_4bit\n",
        "        self.llm_int8_threshold = llm_int8_threshold\n",
        "        self.llm_int8_skip_modules = llm_int8_skip_modules\n",
        "        self.llm_int8_enable_fp32_cpu_offload = llm_int8_enable_fp32_cpu_offload\n",
        "        self.llm_int8_has_fp16_weight = llm_int8_has_fp16_weight\n",
        "        self.bnb_4bit_quant_type = bnb_4bit_quant_type\n",
        "        self.bnb_4bit_use_double_quant = bnb_4bit_use_double_quant\n",
        "\n",
        "        if bnb_4bit_compute_dtype is None:\n",
        "            self.bnb_4bit_compute_dtype = torch.float32\n",
        "        elif isinstance(bnb_4bit_compute_dtype, str):\n",
        "            self.bnb_4bit_compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "        elif isinstance(bnb_4bit_compute_dtype, torch.dtype):\n",
        "            self.bnb_4bit_compute_dtype = bnb_4bit_compute_dtype\n",
        "        else:\n",
        "            raise ValueError(\"bnb_4bit_compute_dtype must be a string or a torch.dtype\")\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "    def post_init(self):\n",
        "        r\"\"\"\n",
        "        Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.\n",
        "        \"\"\"\n",
        "        if not isinstance(self.llm_int8_threshold, float):\n",
        "            raise ValueError(\"llm_int8_threshold must be a float\")\n",
        "\n",
        "        if self.llm_int8_skip_modules is not None and not isinstance(self.llm_int8_skip_modules, list):\n",
        "            raise ValueError(\"llm_int8_skip_modules must be a list of strings\")\n",
        "        if not isinstance(self.llm_int8_enable_fp32_cpu_offload, bool):\n",
        "            raise ValueError(\"llm_int8_enable_fp32_cpu_offload must be a boolean\")\n",
        "\n",
        "        if not isinstance(self.llm_int8_has_fp16_weight, bool):\n",
        "            raise ValueError(\"llm_int8_has_fp16_weight must be a boolean\")\n",
        "\n",
        "        if self.bnb_4bit_compute_dtype is not None and not isinstance(self.bnb_4bit_compute_dtype, torch.dtype):\n",
        "            raise ValueError(\"bnb_4bit_compute_dtype must be torch.dtype\")\n",
        "\n",
        "        if not isinstance(self.bnb_4bit_quant_type, str):\n",
        "            raise ValueError(\"bnb_4bit_quant_type must be a string\")\n",
        "\n",
        "        if not isinstance(self.bnb_4bit_use_double_quant, bool):\n",
        "            raise ValueError(\"bnb_4bit_use_double_quant must be a boolean\")\n",
        "\n",
        "        if self.load_in_4bit and not version.parse(importlib_metadata.version(\"bitsandbytes\")) >= version.parse(\n",
        "            \"0.39.0\"\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                \"4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\"\n",
        "            )\n",
        "\n",
        "    def is_quantizable(self):\n",
        "        r\"\"\"\n",
        "        Returns `True` if the model is quantizable, `False` otherwise.\n",
        "        \"\"\"\n",
        "        return self.load_in_8bit or self.load_in_4bit\n",
        "\n",
        "    def quantization_method(self):\n",
        "        r\"\"\"\n",
        "        This method returns the quantization method used for the model. If the model is not quantizable, it returns\n",
        "        `None`.\n",
        "        \"\"\"\n",
        "        if self.load_in_8bit:\n",
        "            return \"llm_int8\"\n",
        "        elif self.load_in_4bit and self.bnb_4bit_quant_type == \"fp4\":\n",
        "            return \"fp4\"\n",
        "        elif self.load_in_4bit and self.bnb_4bit_quant_type == \"nf4\":\n",
        "            return \"nf4\"\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, config_dict, return_unused_kwargs, **kwargs):\n",
        "        \"\"\"\n",
        "        Instantiates a [`BitsAndBytesConfig`] from a Python dictionary of parameters.\n",
        "\n",
        "        Args:\n",
        "            config_dict (`Dict[str, Any]`):\n",
        "                Dictionary that will be used to instantiate the configuration object.\n",
        "            return_unused_kwargs (`bool`):\n",
        "                Whether or not to return a list of unused keyword arguments. Used for `from_pretrained` method in\n",
        "                `PreTrainedModel`.\n",
        "            kwargs (`Dict[str, Any]`):\n",
        "                Additional parameters from which to initialize the configuration object.\n",
        "\n",
        "        Returns:\n",
        "            [`BitsAndBytesConfig`]: The configuration object instantiated from those parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        config = cls(**config_dict)\n",
        "\n",
        "        to_remove = []\n",
        "        for key, value in kwargs.items():\n",
        "            if hasattr(config, key):\n",
        "                setattr(config, key, value)\n",
        "                to_remove.append(key)\n",
        "        for key in to_remove:\n",
        "            kwargs.pop(key, None)\n",
        "\n",
        "        if return_unused_kwargs:\n",
        "            return config, kwargs\n",
        "        else:\n",
        "            return config\n",
        "\n",
        "    def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n",
        "        \"\"\"\n",
        "        Save this instance to a JSON file.\n",
        "\n",
        "        Args:\n",
        "            json_file_path (`str` or `os.PathLike`):\n",
        "                Path to the JSON file in which this configuration instance's parameters will be saved.\n",
        "            use_diff (`bool`, *optional*, defaults to `True`):\n",
        "                If set to `True`, only the difference between the config instance and the default\n",
        "                `BitsAndBytesConfig()` is serialized to JSON file.\n",
        "        \"\"\"\n",
        "        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n",
        "            config_dict = self.to_dict()\n",
        "            json_string = json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "            writer.write(json_string)\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Serializes this instance to a Python dictionary. Returns:\n",
        "            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n",
        "        \"\"\"\n",
        "\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        output[\"bnb_4bit_compute_dtype\"] = str(output[\"bnb_4bit_compute_dtype\"]).split(\".\")[1]\n",
        "\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "niL_JcuZIaHY"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/microsoft/DeepSpeedExamples/blob/master/applications/DeepSpeed-Chat/training/utils/module/lora.py\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LinearLayer_LoRA(nn.Module):\n",
        "    # a simple implementation of LoRA\n",
        "    def __init__(self,\n",
        "                 weight,\n",
        "                 lora_dim=0,\n",
        "                 lora_scaling=1,\n",
        "                 lora_dropout=0,\n",
        "                 bias=None):\n",
        "        super(LinearLayer_LoRA, self).__init__()\n",
        "        self.weight = weight\n",
        "        self.bias = bias\n",
        "\n",
        "        if lora_dim <= 0:\n",
        "            raise ValueError(\n",
        "                \"You are training to use LoRA, whose reduced dim should be larger than 1\"\n",
        "            )\n",
        "\n",
        "        rows, columns = weight.shape\n",
        "        self.lora_right_weight = nn.Parameter(torch.zeros(\n",
        "            columns,\n",
        "            lora_dim))  # apply transpose so in forward we do not need to\n",
        "        self.lora_left_weight = nn.Parameter(torch.zeros(lora_dim, rows))\n",
        "        self.lora_scaling = lora_scaling / lora_dim\n",
        "\n",
        "        if lora_dropout > 0:\n",
        "            self.lora_dropout = nn.Dropout(lora_dropout)\n",
        "        else:\n",
        "            self.lora_dropout = nn.Identity()\n",
        "\n",
        "        self.reset_parameters()\n",
        "        # disable the original weight gradient\n",
        "        self.weight.requires_grad = False\n",
        "        # fuse LoRA to the original weight\n",
        "        self.fuse_lora = False\n",
        "\n",
        "    def eval(self):\n",
        "        self.lora_dropout.eval()\n",
        "\n",
        "    #   self.fuse_lora_weight()\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        self.lora_dropout.train(mode)\n",
        "        # self.unfuse_lora_weight()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.lora_right_weight, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.lora_left_weight)\n",
        "\n",
        "    def fuse_lora_weight(self):\n",
        "        if not self.fuse_lora:\n",
        "            self.weight.data += self.lora_scaling * torch.matmul(\n",
        "                self.lora_left_weight.t(), self.lora_right_weight.t())\n",
        "        self.fuse_lora = True\n",
        "\n",
        "    def unfuse_lora_weight(self):\n",
        "        if self.fuse_lora:\n",
        "            self.weight.data -= self.lora_scaling * torch.matmul(\n",
        "                self.lora_left_weight.t(), self.lora_right_weight.t())\n",
        "        self.fuse_lora = False\n",
        "\n",
        "    def forward(self, input):\n",
        "        if self.fuse_lora:\n",
        "            return F.linear(input, self.weight, self.bias)\n",
        "        else:\n",
        "            return F.linear(\n",
        "                input, self.weight,\n",
        "                self.bias) + (self.lora_dropout(input) @ self.lora_right_weight\n",
        "                              @ self.lora_left_weight) * self.lora_scaling\n",
        "\n",
        "\n",
        "def recursive_getattr(model, module_name):\n",
        "    \"\"\"\n",
        "    From https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/compression/helper.py\n",
        "    Recursively get the attribute of a module.\n",
        "    Args:\n",
        "        model (`torch.nn.Module`)\n",
        "            The model to get the attribute from.\n",
        "        module_name (`str`)\n",
        "            The name of the module to get the attribute from.\n",
        "    \"\"\"\n",
        "    split_list = module_name.split('.')\n",
        "    output = model\n",
        "    for name in split_list:\n",
        "        output = getattr(output, name)\n",
        "    return output\n",
        "\n",
        "\n",
        "def recursive_setattr(model, module_name, module):\n",
        "    \"\"\"\n",
        "    From https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/compression/helper.py\n",
        "    Recursively set the attribute of a module.\n",
        "    Args:\n",
        "        model (`torch.nn.Module`)\n",
        "            The model to set the attribute in.\n",
        "        module_name (`str`)\n",
        "            The name of the module to set the attribute in.\n",
        "        module (`torch.nn.Module`)\n",
        "            The module to set the attribute to.\n",
        "    \"\"\"\n",
        "    split_list = module_name.split('.')\n",
        "    output = model\n",
        "    for name in split_list[:-1]:\n",
        "        output = getattr(output, name)\n",
        "    output.__setattr__(split_list[-1], module)\n",
        "\n",
        "\n",
        "# convert the linear layer to LoRA\n",
        "def convert_linear_layer_to_lora(model,\n",
        "                                 part_module_name,\n",
        "                                 lora_dim=0,\n",
        "                                 lora_scaling=1,\n",
        "                                 lora_dropout=0):\n",
        "    replace_name = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear) and part_module_name in name:\n",
        "            replace_name.append(name)\n",
        "    for name in replace_name:\n",
        "        module = recursive_getattr(model, name)\n",
        "        tmp = LinearLayer_LoRA(\n",
        "            module.weight, lora_dim, lora_scaling, lora_dropout,\n",
        "            module.bias).to(module.weight.device).to(module.weight.dtype)\n",
        "        recursive_setattr(model, name, tmp)\n",
        "    return model\n",
        "\n",
        "\n",
        "# convert the LoRA layer to linear layer\n",
        "def convert_lora_to_linear_layer(model):\n",
        "    replace_name = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, LinearLayer_LoRA):\n",
        "            replace_name.append(name)\n",
        "    for name in replace_name:\n",
        "        module = recursive_getattr(model, name)\n",
        "        module.fuse_lora_weight()\n",
        "    return model\n",
        "\n",
        "\n",
        "def only_optimize_lora_parameters(model):\n",
        "    # turn off the gradient of all the parameters except the LoRA parameters\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"lora_right_weight\" in name or \"lora_left_weight\" in name:\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOkIRR1zIoi1"
      },
      "source": [
        "### set up- fine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2UYlaS5I1so"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 2\n",
        "eval_batch_size = 2\n",
        "grad_accum = 2\n",
        "ckpt_path = 'models/fine_2.pt'\n",
        "model_type = \"fine\"\n",
        "dataset_path = '/content/drive/MyDrive/datasets_yoo/' #만들어줘\n",
        "logging_dir = 'logs/'\n",
        "log_with = 'wandb'\n",
        "hubert_path = 'data/models/hubert/hubert.pt'\n",
        "hubert_tokenizer_path = 'data/models/hubert/tokenizer.pth'\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/datasets_yoo/fine_output' #만들어줘\n",
        "resume_from_checkpoint = None\n",
        "\n",
        "checkpointing_steps = 50\n",
        "\n",
        "mixed_precision = 'fp16'\n",
        "bits = 16 #4 4 and 8 bit are a work in progress\n",
        "compute_dtype = torch.float16\n",
        "double_quant = True\n",
        "quant_type = 'nf4'\n",
        "\n",
        "lora_dim = 64\n",
        "lora_scaling = 1\n",
        "lora_dropout = 0.1\n",
        "lora_module_name = 'transformer.h'\n",
        "optimize_lora_params_only = False\n",
        "\n",
        "learning_rate = 1e-4\n",
        "scale_lr = False\n",
        "use_8bit_adam = False\n",
        "adam_beta1 = 0.9\n",
        "adam_beta2 = 0.999\n",
        "adam_epsilon = 1e-8\n",
        "weight_decay = 0.01\n",
        "\n",
        "llm_int8_skip_modules = None\n",
        "keep_in_fp32_modules = ['lm_head']\n",
        "\n",
        "lr_scheduler_type = 'linear'\n",
        "lr_warmup_steps = 30\n",
        "num_train_epochs = 200\n",
        "max_train_steps = None\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "seed = 741"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZAYy1erMLFs"
      },
      "outputs": [],
      "source": [
        "CONTEXT_WINDOW_SIZE = 1024 #모델이 한 번에 고려할 데이터의 최대 길이를 의미합니다.\n",
        "\n",
        "MAX_SEMANTIC_LEN = 256 #의미론적 분석을 위한 최대 길이를 256으로 설정합니다\n",
        "\n",
        "SEMANTIC_RATE_HZ = 49.9\n",
        "SEMANTIC_VOCAB_SIZE = 10_000 #조절..?\n",
        "\n",
        "TEXT_ENCODING_OFFSET = 10_048\n",
        "SEMANTIC_PAD_TOKEN = 10_000\n",
        "TEXT_PAD_TOKEN = 129_595\n",
        "SEMANTIC_INFER_TOKEN = 129_599\n",
        "\n",
        "MAX_COARSE_LEN = 768\n",
        "\n",
        "SAMPLE_RATE = 24_000\n",
        "CHANNELS = 1\n",
        "\n",
        "COARSE_SEMANTIC_PAD_TOKEN = 12_048\n",
        "COARSE_INFER_TOKEN = 12_050\n",
        "\n",
        "CODEBOOK_SIZE = 1024\n",
        "N_COARSE_CODEBOOKS = 2\n",
        "N_FINE_CODEBOOKS = 8\n",
        "COARSE_RATE_HZ = 75\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "USE_SMALL_MODELS = os.environ.get(\"SERP_USE_SMALL_MODELS\", False)\n",
        "\n",
        "default_cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\n",
        "CACHE_DIR = os.path.join(os.getenv(\"XDG_CACHE_HOME\", default_cache_dir), \"serp\", \"bark_v0\")\n",
        "\n",
        "\n",
        "def _clear_cuda_cache():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "\n",
        "def _md5(fname):\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(fname, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "\n",
        "def _download(from_hf_path, file_name, to_local_path):\n",
        "    to_local_path = to_local_path.replace(\"\\\\\", \"/\")\n",
        "    path = '/'.join(to_local_path.split(\"/\")[:-1])\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    hf_hub_download(repo_id=from_hf_path, filename=file_name, local_dir=path)\n",
        "    os.replace(os.path.join(path, file_name), to_local_path)\n",
        "\n",
        "\n",
        "def _tokenize(tokenizer, text):\n",
        "    return tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "\n",
        "def _detokenize(tokenizer, enc_text):\n",
        "    return tokenizer.decode(enc_text)\n",
        "\n",
        "\n",
        "def _normalize_whitespace(text):\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "\n",
        "REMOTE_MODEL_PATHS = {\n",
        "    \"text_small\": {\n",
        "        \"repo_id\": \"suno/bark\",\n",
        "        \"file_name\": \"text.pt\",\n",
        "        \"checksum\": \"b3e42bcbab23b688355cd44128c4cdd3\",\n",
        "    },\n",
        "    \"coarse_small\": {\n",
        "        \"repo_id\": \"suno/bark\",\n",
        "        \"file_name\": \"coarse.pt\",\n",
        "        \"checksum\": \"5fe964825e3b0321f9d5f3857b89194d\",\n",
        "    },\n",
        "    \"fine_small\": {\n",
        "        \"repo_id\": \"suno/bark\",\n",
        "        \"file_name\": \"fine.pt\",\n",
        "        \"checksum\": \"5428d1befe05be2ba32195496e58dc90\",\n",
        "    },\n",
        "    \"text\": {\n",
        "        \"repo_id\": \"suno/bark\",\n",
        "        \"file_name\": \"text_2.pt\",\n",
        "        \"checksum\": \"54afa89d65e318d4f5f80e8e8799026a\",\n",
        "    },\n",
        "    \"coarse\": {\n",
        "        \"repo_id\": \"suno/bark\",\n",
        "        \"file_name\": \"coarse_2.pt\",\n",
        "        \"checksum\": \"8a98094e5e3a255a5c9c0ab7efe8fd28\",\n",
        "    },\n",
        "    \"fine\": {\n",
        "        \"repo_id\": \"suno/bark\",\n",
        "        \"file_name\": \"fine_2.pt\",\n",
        "        \"checksum\": \"59d184ed44e3650774a2f0503a48a97b\",\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "def _load_model(ckpt_path, device, use_small=False, model_type=\"text\"):\n",
        "    if model_type == \"text\":\n",
        "        ConfigClass = GPTConfig\n",
        "        ModelClass = GPT\n",
        "    elif model_type == \"coarse\":\n",
        "        ConfigClass = GPTConfig\n",
        "        ModelClass = GPT\n",
        "    elif model_type == \"fine\":\n",
        "        ConfigClass = FineGPTConfig\n",
        "        ModelClass = FineGPT\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "    model_key = f\"{model_type}_small\" if use_small or USE_SMALL_MODELS else model_type\n",
        "    model_info = REMOTE_MODEL_PATHS[model_key]\n",
        "    if ckpt_path in [None, '']:\n",
        "        ckpt_path = os.path.join(CACHE_DIR, model_info[\"file_name\"])\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        logger.info(f\"{model_type} model not found, downloading into `{CACHE_DIR}`.\")\n",
        "        _download(model_info[\"repo_id\"], model_info[\"file_name\"], ckpt_path)\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    # this is a hack\n",
        "    model_args = checkpoint[\"model_args\"]\n",
        "    if \"input_vocab_size\" not in model_args:\n",
        "        model_args[\"input_vocab_size\"] = model_args[\"vocab_size\"]\n",
        "        model_args[\"output_vocab_size\"] = model_args[\"vocab_size\"]\n",
        "        del model_args[\"vocab_size\"]\n",
        "    gptconf = ConfigClass(**checkpoint[\"model_args\"])\n",
        "    model = ModelClass(gptconf)\n",
        "    state_dict = checkpoint[\"model\"]\n",
        "    # fixup checkpoint\n",
        "    unwanted_prefix = \"_orig_mod.\"\n",
        "    for k, v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "    extra_keys = set(state_dict.keys()) - set(model.state_dict().keys())\n",
        "    extra_keys = set([k for k in extra_keys if not k.endswith(\".attn.bias\")])\n",
        "    missing_keys = set(model.state_dict().keys()) - set(state_dict.keys())\n",
        "    missing_keys = set([k for k in missing_keys if not k.endswith(\".attn.bias\")])\n",
        "    if len(extra_keys) != 0:\n",
        "        raise ValueError(f\"extra keys found: {extra_keys}\")\n",
        "    if len(missing_keys) != 0:\n",
        "        raise ValueError(f\"missing keys: {missing_keys}\")\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    n_params = model.get_num_params()\n",
        "    val_loss = checkpoint[\"best_val_loss\"].item()\n",
        "    print(f\"Loaded {model_type} model with {n_params} params, val_loss={val_loss:.4f}.\")\n",
        "    del checkpoint, state_dict\n",
        "    _clear_cuda_cache()\n",
        "    if model_type == \"text\":\n",
        "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "        return model, tokenizer\n",
        "    return model\n",
        "\n",
        "\n",
        "def _flatten_codebooks(arr, offset_size=CODEBOOK_SIZE):\n",
        "    assert len(arr.shape) == 2\n",
        "    arr = arr.copy()\n",
        "    if offset_size is not None:\n",
        "        for n in range(1, arr.shape[0]):\n",
        "            arr[n, :] += offset_size * n\n",
        "    flat_arr = arr.ravel(\"F\")\n",
        "    return flat_arr\n",
        "\n",
        "\n",
        "def load_filepaths_and_text(filename, split=\"|\"):\n",
        "    with open(filename, encoding='utf-8', errors='ignore') as f:\n",
        "        filepaths_and_text = [line.strip().replace('\"', '').split(split) for line in f]\n",
        "        #print(filepaths_and_text)\n",
        "        base = os.path.dirname(filename)\n",
        "        for j in range(len(filepaths_and_text)):\n",
        "            filepaths_and_text[j][0] = os.path.join(base, filepaths_and_text[j][0])\n",
        "    return filepaths_and_text\n",
        "\n",
        "\n",
        "class TtsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, opt):\n",
        "        self.path = os.path.dirname(opt['path'])\n",
        "        self.mode = opt['mode']\n",
        "        self.audiopaths_and_text = load_filepaths_and_text(os.path.join(opt['path'] , opt['mode'] + '.txt'))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audiopath_and_text = self.audiopaths_and_text[index]\n",
        "        audiopath = audiopath_and_text[0]\n",
        "\n",
        "        tokens_path = audiopath.replace('.wav', '.npz').replace('/content/drive/MyDrive/datasets_yoo/', '/content/drive/MyDrive/datasets_yoo/tokens/')\n",
        "        with np.load(tokens_path) as data:\n",
        "             fine_tokens = data['fine']\n",
        "\n",
        "        return torch.from_numpy(fine_tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audiopaths_and_text)\n",
        "\n",
        "\n",
        "class TtsCollater():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def __call__(self, batch):\n",
        "        max_len = 1024\n",
        "        fine_tokens = []\n",
        "\n",
        "        for fine_tokens_ in batch:\n",
        "            if fine_tokens_.shape[1] > max_len:\n",
        "                start_idx = np.random.randint(0, fine_tokens_.shape[1] - max_len + 1)\n",
        "                fine_tokens_ = fine_tokens_[:, start_idx : start_idx + max_len]\n",
        "\n",
        "            pad_size = max_len - fine_tokens_.shape[1]\n",
        "            fine_tokens_ = F.pad(fine_tokens_, (0, pad_size), value=CODEBOOK_SIZE)\n",
        "\n",
        "            fine_tokens_ = fine_tokens_.T\n",
        "\n",
        "            fine_tokens.append(fine_tokens_)\n",
        "\n",
        "        return {'fine_tokens': torch.stack(fine_tokens).contiguous()}\n",
        "\n",
        "import torch.cuda.amp\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=grad_accum,\n",
        "    mixed_precision=mixed_precision,\n",
        "    log_with=log_with,\n",
        "    project_dir=logging_dir,\n",
        ")\n",
        "device = accelerator.device\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMKcSaX3W-ty",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "316525532b0b4a2a9a0ebae9c1424391",
            "2dfbbb31081a4249872eea56060e6e41",
            "0304dc9fb59e4fdfb1cbedc5e27f3db0",
            "dabe1c776139450fa83e62dd7f92bdc7",
            "e84459a790ec46ed990c8270b7235218",
            "8b6757afd74545cb893a63daf4c0d8bf",
            "710d9ac61a9744dd90b31aba55ee226a",
            "412966c4adf84d79b36bf0d18b0d9263",
            "c7610747ee2d40c3a2698385705c8b36",
            "18c2f9d1bcf349e7830247cda4eebfd7",
            "bcfac0310dc5493f873d099784c6f0fd"
          ]
        },
        "outputId": "468081cd-cfd5-4aba-a056-a09f5087464d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading HuBERT base model\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloading HuBERT base model\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloaded HuBERT\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloaded HuBERT\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading HuBERT custom tokenizer\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloading HuBERT custom tokenizer\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "quantifier_hubert_base_ls960_14.pth:   0%|          | 0.00/104M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "316525532b0b4a2a9a0ebae9c1424391"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloaded tokenizer\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Downloaded tokenizer\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "max_duration_sec = 15.12 # the maximum allowed duration in seconds\n",
        "\n",
        "path = dataset_path\n",
        "device = 'cuda' # or 'cpu'\n",
        "\n",
        "# # From https://github.com/gitmylo/bark-voice-cloning-HuBERT-quantizer\n",
        "\n",
        "hubert_manager = HuBERTManager()\n",
        "hubert_manager.make_sure_hubert_installed()\n",
        "hubert_manager.make_sure_tokenizer_installed()\n",
        "\n",
        "# # Load the HuBERT model\n",
        "hubert_model = CustomHubert(checkpoint_path=hubert_path).to(device)\n",
        "hubert_model.eval()\n",
        "for param in hubert_model.parameters():\n",
        "     param.requires_grad = False\n",
        "\n",
        "# # Load the CustomTokenizer model\n",
        "hubert_tokenizer = CustomTokenizer.load_from_checkpoint(hubert_tokenizer_path).to(device)  # Automatically uses the right layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9EoHln9fedZ",
        "outputId": "6c14b716-e392-4587-baba-36a91201f0fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bark/bark\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/encodec/v0/encodec_24khz-d7cc33bc.th\" to /root/.cache/torch/hub/checkpoints/encodec_24khz-d7cc33bc.th\n",
            "100%|██████████| 88.9M/88.9M [00:02<00:00, 31.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/bark/bark\n",
        "\n",
        "from bark.generation import _load_codec_model\n",
        "codec_model = _load_codec_model(device)\n",
        "codec_model.eval()\n",
        "for param in codec_model.parameters():\n",
        "     param.requires_grad = False\n",
        "%cd /content\n",
        "\n",
        "def get_duration(wav, sr):\n",
        "    return wav.shape[1] / sr\n",
        "\n",
        "valid_lines_train = []\n",
        "\n",
        "# convert wavs to semantic tokens\n",
        "for wav_path, txt in load_filepaths_and_text('/content/drive/MyDrive/datasets_yoo/yoojs_train.csv'):\n",
        "     wav, sr = torchaudio.load(wav_path)\n",
        "     if not get_duration(wav, sr) > max_duration_sec:\n",
        "         valid_lines_train.append((wav_path, txt))\n",
        "     wav = convert_audio(wav, sr, SAMPLE_RATE, CHANNELS).to(device)\n",
        "\n",
        "     semantic_vectors = hubert_model.forward(wav, input_sample_hz=SAMPLE_RATE)\n",
        "     semantic_tokens = hubert_tokenizer.get_token(semantic_vectors)\n",
        "\n",
        "     # save semantic tokens\n",
        "     os.makedirs(os.path.join(path, 'tokens'), exist_ok=True)\n",
        "     semantic_tokens = semantic_tokens.cpu().numpy()\n",
        "\n",
        "     # Extract discrete codes from EnCodec\n",
        "     with torch.no_grad():\n",
        "         encoded_frames = codec_model.encode(wav.unsqueeze(0))\n",
        "     codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()  # [n_q, T]\n",
        "\n",
        "     # move codes to cpu\n",
        "     codes = codes.cpu().numpy()\n",
        "\n",
        "     # save tokens\n",
        "     np.savez_compressed(os.path.join(path, 'tokens', os.path.basename(wav_path).replace('.wav', '.npz')), fine=codes, coarse=codes[:2, :], semantic=semantic_tokens)\n",
        "\n",
        "# rewrite train.txt with valid lines(유효한 것들 모아..)\n",
        "with open(path + 'train_valid.txt', 'w', encoding='utf-8') as f:\n",
        "     for wav_path, txt in valid_lines_train:\n",
        "         wav_path = os.path.relpath(wav_path, dataset_path).replace('\\\\', '/')\n",
        "         f.write(f'{wav_path}|{txt}\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqzCXwqxG7hX"
      },
      "outputs": [],
      "source": [
        "valid_lines_valid = []\n",
        "for wav_path, txt in load_filepaths_and_text(path + 'yoojs_valid.csv'):\n",
        "     wav, sr = torchaudio.load(wav_path)\n",
        "     if not get_duration(wav, sr) > max_duration_sec:\n",
        "         valid_lines_valid.append((wav_path, txt))\n",
        "     wav = convert_audio(wav, sr, SAMPLE_RATE, CHANNELS).to(device)\n",
        "\n",
        "     semantic_vectors = hubert_model.forward(wav, input_sample_hz=SAMPLE_RATE)\n",
        "     semantic_tokens = hubert_tokenizer.get_token(semantic_vectors)\n",
        "\n",
        "     # save semantic tokens\n",
        "     os.makedirs(os.path.join(path, 'tokens'), exist_ok=True)\n",
        "     semantic_tokens = semantic_tokens.cpu().numpy()\n",
        "\n",
        "     # Extract discrete codes from EnCodec\n",
        "     with torch.no_grad():\n",
        "         encoded_frames = codec_model.encode(wav.unsqueeze(0))\n",
        "     codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()  # [n_q, T]\n",
        "\n",
        "     # move codes to cpu\n",
        "     codes = codes.cpu().numpy()\n",
        "\n",
        "     # save tokens\n",
        "     np.savez_compressed(os.path.join(path, 'tokens', os.path.basename(wav_path).replace('.wav', '.npz')), fine=codes, coarse=codes[:2, :], semantic=semantic_tokens)\n",
        "\n",
        " # rewrite valid.txt with valid lines\n",
        "with open(path + 'valid_valid.txt', 'w', encoding='utf-8') as f:\n",
        "     for wav_path, txt in valid_lines_valid:\n",
        "         wav_path = os.path.relpath(wav_path, dataset_path).replace('\\\\', '/')\n",
        "         f.write(f'{wav_path}|{txt}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiAnHOIJHh0j"
      },
      "outputs": [],
      "source": [
        "\n",
        "del hubert_model\n",
        "del hubert_tokenizer\n",
        "del codec_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxIo1kaMQ5th"
      },
      "source": [
        "### train, val- fine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65,
          "referenced_widgets": [
            "fef290cdfe16416d906f9e74fc0ea96a",
            "7412a7be60404b878d20f45c4dd95635",
            "6e46d674685c497289d117fd747cf397",
            "02bdb892ef1a4040865187854f0debd7",
            "9a843368ca7040a99d6ac7006fd192a5",
            "90dba3b740fc4e7c95636289abe5f842",
            "98e9db93debb4b999cf1d5191334a66a",
            "907b9cddfa8b4e05828ee4608904621b",
            "581e7fa20e45498c914ea4a2c38176a0",
            "b2a87c4aa2664b108aaadb70e9371338",
            "2fc78374f66d41938711a3a7606f4f58"
          ]
        },
        "id": "XqkepMPfCjtu",
        "outputId": "ca073a27-8512-4999-eb10-38d0cf031265"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "fine_2.pt:   0%|          | 0.00/3.74G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fef290cdfe16416d906f9e74fc0ea96a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loaded fine model with \u001b[1;36m302090240\u001b[0m params, \u001b[33mval_loss\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1;36m.0786\u001b[0m.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loaded fine model with <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">302090240</span> params, <span style=\"color: #808000; text-decoration-color: #808000\">val_loss</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0786</span>.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = _load_model(ckpt_path, device, use_small=False, model_type=model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoONAyyaCpBQ"
      },
      "outputs": [],
      "source": [
        "if scale_lr:\n",
        "    learning_rate = (\n",
        "        learning_rate * grad_accum * train_batch_size * accelerator.num_processes\n",
        "    )\n",
        "\n",
        "if use_8bit_adam:\n",
        "    try:\n",
        "        import bitsandbytes as bnb\n",
        "    except ImportError:\n",
        "        raise ImportError(\n",
        "            \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n",
        "        )\n",
        "\n",
        "    optimizer_class = bnb.optim.AdamW8bit\n",
        "else:\n",
        "    optimizer_class = torch.optim.AdamW\n",
        "\n",
        "quantization_config=BitsAndBytesConfig(\n",
        "    load_in_4bit=bits == 4,\n",
        "    load_in_8bit=bits == 8,\n",
        "    llm_int8_threshold=6.0,\n",
        "    llm_int8_has_fp16_weight=False,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=double_quant,\n",
        "    bnb_4bit_quant_type=quant_type # {'fp4', 'nf4'}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEEm7B3mDbk2"
      },
      "outputs": [],
      "source": [
        "if bits == 4:\n",
        "    from accelerate.utils import CustomDtype\n",
        "    target_dtype = CustomDtype.INT4\n",
        "elif bits == 8:\n",
        "    target_dtype = torch.int8\n",
        "\n",
        "if lora_dim > 0:\n",
        "    for param in model.parameters():\n",
        "        if param.ndim == 1:\n",
        "            # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "            param.data = param.data.to(torch.float32)\n",
        "\n",
        "    class CastOutputToFloat(nn.Sequential):\n",
        "        def forward(self, x):\n",
        "            return super().forward(x).to(torch.float32)\n",
        "\n",
        "    # model.lm_head = CastOutputToFloat(model.lm_head)\n",
        "    for i, lm_head in enumerate(model.lm_heads):\n",
        "        model.lm_heads[i] = CastOutputToFloat(lm_head)\n",
        "\n",
        "    model = convert_linear_layer_to_lora(model, lora_module_name,\n",
        "                                            lora_dim=lora_dim, lora_scaling=lora_scaling,\n",
        "                                            lora_dropout=lora_dropout)\n",
        "    if optimize_lora_params_only:\n",
        "        model = only_optimize_lora_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXNs4_i6DgGJ"
      },
      "outputs": [],
      "source": [
        "params_to_optimize = (\n",
        "        param for param in model.parameters() if param.requires_grad\n",
        "    )\n",
        "\n",
        "optimizer = optimizer_class(\n",
        "    params_to_optimize,\n",
        "    lr=learning_rate,\n",
        "    betas=(adam_beta1, adam_beta2),\n",
        "    weight_decay=weight_decay,\n",
        "    eps=adam_epsilon,\n",
        ")\n",
        "opt_train = {\n",
        "    'path': dataset_path,\n",
        "    'mode': 'train_valid',\n",
        "}\n",
        "\n",
        "opt_val = {\n",
        "    'path': dataset_path,\n",
        "    'mode': 'valid_valid',\n",
        "}\n",
        "\n",
        "train_dataset = TtsDataset(opt_train)\n",
        "validation_dataset = TtsDataset(opt_val)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_batch_size,\n",
        "    collate_fn=TtsCollater(),\n",
        ")\n",
        "\n",
        "validation_dataloader = torch.utils.data.DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=eval_batch_size,\n",
        "    collate_fn=TtsCollater(),\n",
        ")\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=COARSE_SEMANTIC_PAD_TOKEN)\n",
        "\n",
        "# Scheduler and math around the number of training steps.\n",
        "overrode_max_train_steps = False\n",
        "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / grad_accum)\n",
        "if max_train_steps is None:\n",
        "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "    overrode_max_train_steps = True\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    lr_scheduler_type,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=lr_warmup_steps * grad_accum,\n",
        "    num_training_steps=max_train_steps * grad_accum,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3jY8YmUD4r1"
      },
      "outputs": [],
      "source": [
        "model, optimizer, train_dataloader, validation_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, validation_dataloader, lr_scheduler\n",
        ")\n",
        "accelerator.register_for_checkpointing(lr_scheduler)\n",
        "\n",
        "weight_dtype = torch.float32\n",
        "if accelerator.mixed_precision == \"fp16\":\n",
        "    weight_dtype = torch.float16\n",
        "elif accelerator.mixed_precision == \"bf16\":\n",
        "    weight_dtype = torch.bfloat16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "16N1DzroD9Ko",
        "outputId": "57c7b1d9-5247-4961-cfe2-db12d77320c8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation Loss: \u001b[1;36m13.094276428222656\u001b[0m over \u001b[1;36m1\u001b[0m samples and \u001b[1;36m1\u001b[0m batches.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Validation Loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.094276428222656</span> over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> samples and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> batches.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
        "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / grad_accum)\n",
        "max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "# Afterwards we recalculate our number of training epochs\n",
        "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "# We need to initialize the trackers we use, and also store our configuration.\n",
        "# The trackers initializes automatically on the main process.\n",
        "if accelerator.is_main_process:\n",
        "    accelerator.init_trackers(\"bark_coarse\", config={})\n",
        "\n",
        "total_batch_size = train_batch_size * accelerator.num_processes * grad_accum\n",
        "logger.info(\"***** Running training *****\")\n",
        "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
        "logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n",
        "logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
        "logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
        "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "logger.info(f\"  Gradient Accumulation steps = {grad_accum}\")\n",
        "logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
        "global_step = 0\n",
        "first_epoch = 0\n",
        "\n",
        "if resume_from_checkpoint:\n",
        "    if resume_from_checkpoint != \"latest\":\n",
        "        path = os.path.basename(resume_from_checkpoint)\n",
        "    else:\n",
        "        # Get the most recent checkpoint\n",
        "        dirs = os.listdir(output_dir)\n",
        "        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
        "        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
        "        path = dirs[-1]\n",
        "    accelerator.print(f\"Resuming from checkpoint {path}\")\n",
        "    accelerator.load_state(os.path.join(output_dir, path))\n",
        "    global_step = int(path.split(\"-\")[1])\n",
        "\n",
        "    resume_global_step = global_step * grad_accum\n",
        "    first_epoch = resume_global_step // num_update_steps_per_epoch\n",
        "    resume_step = resume_global_step % num_update_steps_per_epoch\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    model.eval()\n",
        "    validation_loss = 0.0\n",
        "    num_batches = 0\n",
        "    num_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for val_step, val_batch in enumerate(validation_dataloader):\n",
        "            # Similar to training, process the validation batch\n",
        "            fine_targets_7 = val_batch['fine_tokens'][:, :, 6]\n",
        "            fine_tokens_input_7 = torch.cat([val_batch['fine_tokens'][:, :, :6], torch.zeros_like(val_batch['fine_tokens'][:, :, 6:])], dim=2)\n",
        "            fine_targets_8 = val_batch['fine_tokens'][:, :, 7]\n",
        "            fine_tokens_input_8 = torch.cat([val_batch['fine_tokens'][:, :, :7], torch.zeros_like(val_batch['fine_tokens'][:, :, 7:])], dim=2)\n",
        "\n",
        "            # Forward pass for validation\n",
        "            logits_7 = model(6, fine_tokens_input_7)\n",
        "            logits_8 = model(7, fine_tokens_input_8)\n",
        "\n",
        "            # Calculate the validation loss\n",
        "            loss_7 = criterion(logits_7.view(-1, model.config.output_vocab_size), fine_targets_7.view(-1))\n",
        "            loss_8 = criterion(logits_8.view(-1, model.config.output_vocab_size), fine_targets_8.view(-1))\n",
        "\n",
        "            loss = (loss_7 + loss_8) / 2\n",
        "            validation_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            num_samples += val_batch['fine_tokens'].size(0)\n",
        "\n",
        "    average_validation_loss = validation_loss / num_batches\n",
        "    logger.info(f\"Validation Loss: {average_validation_loss} over {num_samples} samples and {num_batches} batches.\")\n",
        "    print(f\"Validation Loss: {average_validation_loss} over {num_samples} samples and {num_batches} batches.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXeDsX7kQ1on"
      },
      "outputs": [],
      "source": [
        "# Only show the progress bar once on each machine.\n",
        "progress_bar = tqdm(range(global_step, max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "progress_bar.set_description(\"Steps\")\n",
        "\n",
        "for epoch in range(first_epoch, num_train_epochs):\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Skip steps until we reach the resumed step\n",
        "        if resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n",
        "            if step % grad_accum == 0:\n",
        "                progress_bar.update(1)\n",
        "            continue\n",
        "\n",
        "        with accelerator.accumulate(model):\n",
        "            fine_targets_7 = batch['fine_tokens'][:, :, 6]\n",
        "            fine_tokens_input_7 = torch.cat([batch['fine_tokens'][:, :, :6], torch.zeros_like(batch['fine_tokens'][:, :, 6:])], dim=2)\n",
        "            fine_targets_8 = batch['fine_tokens'][:, :, 7]\n",
        "            fine_tokens_input_8 = torch.cat([batch['fine_tokens'][:, :, :7], torch.zeros_like(batch['fine_tokens'][:, :, 7:])], dim=2)\n",
        "\n",
        "            # Forward pass\n",
        "            logits_7 = model(6, fine_tokens_input_7)\n",
        "            logits_8 = model(7, fine_tokens_input_8)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss_7 = criterion(logits_7.view(-1, model.config.output_vocab_size), fine_targets_7.view(-1))\n",
        "            loss_8 = criterion(logits_8.view(-1, model.config.output_vocab_size), fine_targets_8.view(-1))\n",
        "\n",
        "            loss = (loss_7 + loss_8) / 2\n",
        "\n",
        "            accelerator.backward(loss)\n",
        "            if accelerator.sync_gradients:\n",
        "                params_to_clip = (\n",
        "                    param for param in model.parameters() if param.requires_grad\n",
        "                )\n",
        "                accelerator.clip_grad_norm_(params_to_clip, max_grad_norm)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
        "        if accelerator.sync_gradients:\n",
        "            progress_bar.update(1)\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % checkpointing_steps == 0:\n",
        "                if accelerator.is_main_process:\n",
        "                    save_path = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
        "                    accelerator.save_state(save_path)\n",
        "                    logger.info(f\"Saved state to {save_path}\")\n",
        "\n",
        "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
        "        progress_bar.set_postfix(**logs)\n",
        "        accelerator.log(logs, step=global_step)\n",
        "\n",
        "        if global_step >= max_train_steps:\n",
        "            break\n",
        "\n",
        "    accelerator.wait_for_everyone()\n",
        "\n",
        "if accelerator.is_main_process:\n",
        "    if lora_dim > 0:\n",
        "        model = convert_lora_to_linear_layer(model)\n",
        "    # save model\n",
        "    accelerator.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
        "\n",
        "    config = model.config.__dict__\n",
        "    # save config\n",
        "    with open(os.path.join(output_dir, \"config.json\"), \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "accelerator.end_training()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if accelerator.is_main_process:\n",
        "    if lora_dim > 0:\n",
        "        model = convert_lora_to_linear_layer(model)\n",
        "    # save model\n",
        "    accelerator.save(model.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
        "\n",
        "    config = model.config.__dict__\n",
        "    # save config\n",
        "    with open(os.path.join(output_dir, \"config.json\"), \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "accelerator.end_training()"
      ],
      "metadata": {
        "id": "IXll-6vBemjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "jvrREez5TYNN",
        "outputId": "381af7c5-34e2-4d0b-ca5e-b627e23531f6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation Loss: \u001b[1;36m0.025459157302975655\u001b[0m over \u001b[1;36m1\u001b[0m samples and \u001b[1;36m1\u001b[0m batches.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Validation Loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.025459157302975655</span> over <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> samples and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> batches.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "if accelerator.is_main_process:\n",
        "    model.eval()\n",
        "    validation_loss = 0.0\n",
        "    num_batches = 0\n",
        "    num_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for val_step, val_batch in enumerate(validation_dataloader):\n",
        "            # Similar to training, process the validation batch\n",
        "            fine_targets_7 = val_batch['fine_tokens'][:, :, 6]\n",
        "            fine_tokens_input_7 = torch.cat([val_batch['fine_tokens'][:, :, :6], torch.zeros_like(val_batch['fine_tokens'][:, :, 6:])], dim=2)\n",
        "            fine_targets_8 = val_batch['fine_tokens'][:, :, 7]\n",
        "            fine_tokens_input_8 = torch.cat([val_batch['fine_tokens'][:, :, :7], torch.zeros_like(val_batch['fine_tokens'][:, :, 7:])], dim=2)\n",
        "\n",
        "            # Forward pass for validation\n",
        "            logits_7 = model(6, fine_tokens_input_7)\n",
        "            logits_8 = model(7, fine_tokens_input_8)\n",
        "\n",
        "            # Calculate the validation loss\n",
        "            loss_7 = criterion(logits_7.view(-1, model.config.output_vocab_size), fine_targets_7.view(-1))\n",
        "            loss_8 = criterion(logits_8.view(-1, model.config.output_vocab_size), fine_targets_8.view(-1))\n",
        "\n",
        "            loss = (loss_7 + loss_8) / 2\n",
        "            validation_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            num_samples += val_batch['fine_tokens'].size(0)\n",
        "\n",
        "    average_validation_loss = validation_loss / num_batches\n",
        "    logger.info(f\"Validation Loss: {average_validation_loss} over {num_samples} samples and {num_batches} batches.\")\n",
        "    print(f\"Validation Loss: {average_validation_loss} over {num_samples} samples and {num_batches} batches.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GziFiTBUqee"
      },
      "source": [
        "## TEST & CREATE WAV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sgk16VuVWUP8"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio\n",
        "from scipy.io.wavfile import write as write_wav\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "D7F2PAbfnHmB"
      },
      "outputs": [],
      "source": [
        "#generation\n",
        "import contextlib\n",
        "import gc\n",
        "import hashlib\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "from encodec import EncodecModel\n",
        "import funcy\n",
        "import logging\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "#import GPTConfig, GPT\n",
        "#import FineGPT, FineGPTConfig\n",
        "\n",
        "if (\n",
        "    torch.cuda.is_available() and\n",
        "    hasattr(torch.cuda, \"amp\") and\n",
        "    hasattr(torch.cuda.amp, \"autocast\") and\n",
        "    hasattr(torch.cuda, \"is_bf16_supported\") and\n",
        "    torch.cuda.is_bf16_supported()\n",
        "):\n",
        "    autocast = funcy.partial(torch.cuda.amp.autocast, dtype=torch.bfloat16)\n",
        "else:\n",
        "    @contextlib.contextmanager\n",
        "    def autocast():\n",
        "        yield\n",
        "\n",
        "\n",
        "# hold models in global scope to lazy load\n",
        "global models\n",
        "models = {}\n",
        "\n",
        "global models_devices\n",
        "models_devices = {}\n",
        "\n",
        "\n",
        "CONTEXT_WINDOW_SIZE = 1024\n",
        "\n",
        "SEMANTIC_RATE_HZ = 49.9\n",
        "SEMANTIC_VOCAB_SIZE = 10_000\n",
        "\n",
        "CODEBOOK_SIZE = 1024\n",
        "N_COARSE_CODEBOOKS = 2\n",
        "N_FINE_CODEBOOKS = 8\n",
        "COARSE_RATE_HZ = 75\n",
        "\n",
        "SAMPLE_RATE = 24_000\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "#CUR_PATH = os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "\n",
        "default_cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\")\n",
        "CACHE_DIR = os.path.join(os.getenv(\"XDG_CACHE_HOME\", default_cache_dir), \"serp\", \"bark_v0\")\n",
        "\n",
        "\n",
        "USE_SMALL_MODELS = os.environ.get(\"SERP_USE_SMALL_MODELS\", False)\n",
        "GLOBAL_ENABLE_MPS = os.environ.get(\"SERP_ENABLE_MPS\", False)\n",
        "OFFLOAD_CPU = os.environ.get(\"SERP_OFFLOAD_CPU\", False)\n",
        "\n",
        "\n",
        "REMOTE_MODEL_PATHS = {\n",
        "    \"text_small\": {\n",
        "        \"repo_id\": \"suno/bark\",\n",
        "        \"file_name\": \"text.pt\",\n",
        "        \"checksum\": \"b3e42bcbab23b688355cd44128c4cdd3\",\n",
        "    },\n",
        "    \"coarse_small\": {\n",
        "        \"repo_id\": \"suno/bark\",\n",
        "        \"file_name\": \"coarse.pt\",\n",
        "        \"checksum\": \"5fe964825e3b0321f9d5f3857b89194d\",\n",
        "    },\n",
        "    \"fine_small\": {\n",
        "        \"repo_id\": \"suno/bark\",\n",
        "        \"file_name\": \"fine.pt\",\n",
        "        \"checksum\": \"5428d1befe05be2ba32195496e58dc90\",\n",
        "    },\n",
        "    \"text\": {\n",
        "        \"repo_id\": \"suno/bark\",\n",
        "        \"file_name\": \"text_2.pt\",\n",
        "        \"checksum\": \"54afa89d65e318d4f5f80e8e8799026a\",\n",
        "    },\n",
        "    \"coarse\": {\n",
        "        \"repo_id\": \"suno/bark\",\n",
        "        \"file_name\": \"coarse_2.pt\",\n",
        "        \"checksum\": \"8a98094e5e3a255a5c9c0ab7efe8fd28\",\n",
        "    },\n",
        "    \"fine\": {\n",
        "        \"repo_id\": \"suno/bark\",\n",
        "        \"file_name\": \"fine_2.pt\",\n",
        "        \"checksum\": \"59d184ed44e3650774a2f0503a48a97b\",\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "if not hasattr(torch.nn.functional, 'scaled_dot_product_attention') and torch.cuda.is_available():\n",
        "    logger.warning(\n",
        "        \"torch version does not support flash attention. You will get faster\" +\n",
        "        \" inference speed by upgrade torch to newest nightly version.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def _string_md5(s):\n",
        "    m = hashlib.md5()\n",
        "    m.update(s.encode(\"utf-8\"))\n",
        "    return m.hexdigest()\n",
        "\n",
        "\n",
        "def _md5(fname):\n",
        "    hash_md5 = hashlib.md5()\n",
        "    with open(fname, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
        "            hash_md5.update(chunk)\n",
        "    return hash_md5.hexdigest()\n",
        "\n",
        "\n",
        "def _get_ckpt_path(model_type, use_small=False, path=None):\n",
        "    model_key = f\"{model_type}_small\" if use_small or USE_SMALL_MODELS else model_type\n",
        "    model_name = REMOTE_MODEL_PATHS[model_key][\"file_name\"]\n",
        "    if path is None:\n",
        "        path = CACHE_DIR\n",
        "    return os.path.join(path, f\"{model_name}\")\n",
        "\n",
        "\n",
        "def _grab_best_device(use_gpu=True):\n",
        "    if torch.cuda.device_count() > 0 and use_gpu:\n",
        "        device = \"cuda\"\n",
        "    elif torch.backends.mps.is_available() and use_gpu and GLOBAL_ENABLE_MPS:\n",
        "        device = \"mps\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    return device\n",
        "\n",
        "\n",
        "def _download(from_hf_path, file_name, to_local_path):\n",
        "    to_local_path = to_local_path.replace(\"\\\\\", \"/\")\n",
        "    path = '/'.join(to_local_path.split(\"/\")[:-1])\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    hf_hub_download(repo_id=from_hf_path, filename=file_name, local_dir=path)\n",
        "    os.replace(os.path.join(path, file_name), to_local_path)\n",
        "\n",
        "class InferenceContext:\n",
        "    def __init__(self, benchmark=False):\n",
        "        # we can't expect inputs to be the same length, so disable benchmarking by default\n",
        "        self._chosen_cudnn_benchmark = benchmark\n",
        "        self._cudnn_benchmark = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        self._cudnn_benchmark = torch.backends.cudnn.benchmark\n",
        "        torch.backends.cudnn.benchmark = self._chosen_cudnn_benchmark\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
        "        torch.backends.cudnn.benchmark = self._cudnn_benchmark\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def _inference_mode():\n",
        "    with InferenceContext(), torch.inference_mode(), torch.no_grad(), autocast():\n",
        "        yield\n",
        "\n",
        "\n",
        "def _clear_cuda_cache():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "\n",
        "def clean_models(model_key=None):\n",
        "    global models\n",
        "    model_keys = [model_key] if model_key is not None else models.keys()\n",
        "    for k in model_keys:\n",
        "        if k in models:\n",
        "            del models[k]\n",
        "    _clear_cuda_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def _load_model(ckpt_path, device, use_small=False, model_type=\"text\"):\n",
        "    if model_type == \"text\":\n",
        "        ConfigClass = GPTConfig\n",
        "        ModelClass = GPT\n",
        "    elif model_type == \"coarse\":\n",
        "        ConfigClass = GPTConfig\n",
        "        ModelClass = GPT\n",
        "    elif model_type == \"fine\":\n",
        "        ConfigClass = FineGPTConfig\n",
        "        ModelClass = FineGPT\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "    model_key = f\"{model_type}_small\" if use_small or USE_SMALL_MODELS else model_type\n",
        "    model_info = REMOTE_MODEL_PATHS[model_key]\n",
        "    # if (\n",
        "    #     os.path.exists(ckpt_path) and\n",
        "    #     _md5(ckpt_path) != model_info[\"checksum\"]\n",
        "    # ):\n",
        "    #     logger.warning(f\"found outdated {model_type} model, removing.\")\n",
        "    #     os.remove(ckpt_path)\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        logger.info(f\"{model_type} model not found, downloading into `{CACHE_DIR}`.\")\n",
        "        _download(model_info[\"repo_id\"], model_info[\"file_name\"], ckpt_path)\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    # this is a hack\n",
        "    # check if config.json is in the same directory as the checkpoint\n",
        "    # if so, load it\n",
        "    # otherwise, assume it's in the checkpoint\n",
        "    config_path = os.path.join(os.path.dirname(ckpt_path), \"config.json\")\n",
        "    if os.path.exists(config_path):\n",
        "        with open(config_path, \"r\") as f:\n",
        "            model_args = json.load(f)\n",
        "    else:\n",
        "        model_args = checkpoint[\"model_args\"]\n",
        "    if \"input_vocab_size\" not in model_args:\n",
        "        model_args[\"input_vocab_size\"] = model_args[\"vocab_size\"]\n",
        "        model_args[\"output_vocab_size\"] = model_args[\"vocab_size\"]\n",
        "        del model_args[\"vocab_size\"]\n",
        "    gptconf = ConfigClass(**model_args)\n",
        "    model = ModelClass(gptconf)\n",
        "    if checkpoint.get(\"model\", None) is not None:\n",
        "        state_dict = checkpoint[\"model\"]\n",
        "    else:\n",
        "        state_dict = checkpoint\n",
        "    # fixup checkpoint\n",
        "    unwanted_prefix = \"_orig_mod.\"\n",
        "    for k, v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
        "    unwanted_suffixes = [\n",
        "        \"lora_right_weight\",\n",
        "        \"lora_left_weight\",\n",
        "        \"lora_right_bias\",\n",
        "        \"lora_left_bias\",\n",
        "    ]\n",
        "    for k, v in list(state_dict.items()):\n",
        "        for suffix in unwanted_suffixes:\n",
        "            if k.endswith(suffix):\n",
        "                state_dict.pop(k)\n",
        "    # super hacky - should probably refactor this\n",
        "    if state_dict.get('lm_head.0.weight', None) is not None:\n",
        "        state_dict['lm_head.weight'] = state_dict.pop('lm_head.0.weight')\n",
        "    if state_dict.get('lm_heads.0.0.weight', None) is not None:\n",
        "        state_dict['lm_heads.0.weight'] = state_dict.pop('lm_heads.0.0.weight')\n",
        "    if state_dict.get('lm_heads.1.0.weight', None) is not None:\n",
        "        state_dict['lm_heads.1.weight'] = state_dict.pop('lm_heads.1.0.weight')\n",
        "    if state_dict.get('lm_heads.2.0.weight', None) is not None:\n",
        "        state_dict['lm_heads.2.weight'] = state_dict.pop('lm_heads.2.0.weight')\n",
        "    if state_dict.get('lm_heads.3.0.weight', None) is not None:\n",
        "        state_dict['lm_heads.3.weight'] = state_dict.pop('lm_heads.3.0.weight')\n",
        "    if state_dict.get('lm_heads.4.0.weight', None) is not None:\n",
        "        state_dict['lm_heads.4.weight'] = state_dict.pop('lm_heads.4.0.weight')\n",
        "    if state_dict.get('lm_heads.5.0.weight', None) is not None:\n",
        "        state_dict['lm_heads.5.weight'] = state_dict.pop('lm_heads.5.0.weight')\n",
        "    if state_dict.get('lm_heads.6.0.weight', None) is not None:\n",
        "        state_dict['lm_heads.6.weight'] = state_dict.pop('lm_heads.6.0.weight')\n",
        "    extra_keys = set(state_dict.keys()) - set(model.state_dict().keys())\n",
        "    extra_keys = set([k for k in extra_keys if not k.endswith(\".attn.bias\")])\n",
        "    missing_keys = set(model.state_dict().keys()) - set(state_dict.keys())\n",
        "    missing_keys = set([k for k in missing_keys if not k.endswith(\".attn.bias\")])\n",
        "    if len(extra_keys) != 0:\n",
        "        print(f\"extra keys found: {extra_keys}\")\n",
        "    if len(missing_keys) != 0:\n",
        "        raise ValueError(f\"missing keys: {missing_keys}\")\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    n_params = model.get_num_params()\n",
        "    if checkpoint.get(\"best_val_loss\", None) is not None:\n",
        "        val_loss = checkpoint[\"best_val_loss\"].item()\n",
        "        logger.info(f\"model loaded: {round(n_params/1e6,1)}M params, {round(val_loss,3)} loss\")\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    del checkpoint, state_dict\n",
        "    _clear_cuda_cache()\n",
        "    if model_type == \"text\":\n",
        "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "        return {\n",
        "            \"model\": model,\n",
        "            \"tokenizer\": tokenizer,\n",
        "        }\n",
        "    return model\n",
        "\n",
        "\n",
        "def _load_codec_model(device):\n",
        "    model = EncodecModel.encodec_model_24khz()\n",
        "    model.set_target_bandwidth(6.0)\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    _clear_cuda_cache()\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_model(use_gpu=True, use_small=False, force_reload=False, model_type=\"text\", path=None):\n",
        "    _load_model_f = funcy.partial(_load_model, model_type=model_type, use_small=use_small)\n",
        "    if model_type not in (\"text\", \"coarse\", \"fine\"):\n",
        "        raise NotImplementedError()\n",
        "    global models\n",
        "    global models_devices\n",
        "    device = _grab_best_device(use_gpu=use_gpu)\n",
        "    model_key = f\"{model_type}\"\n",
        "    if OFFLOAD_CPU:\n",
        "        models_devices[model_key] = device\n",
        "        device = \"cpu\"\n",
        "    if model_key not in models or force_reload:\n",
        "        if path.endswith(\".ckpt\") or path.endswith(\".pt\") or path.endswith(\".bin\"):\n",
        "            ckpt_path = path\n",
        "        else:\n",
        "            ckpt_path = _get_ckpt_path(model_type, use_small=use_small, path=path)\n",
        "        # clean_models(model_key=model_key)\n",
        "        model = _load_model_f(ckpt_path, device)\n",
        "        models[model_key] = model\n",
        "    if model_type == \"text\":\n",
        "        models[model_key][\"model\"].to(device)\n",
        "    else:\n",
        "        models[model_key].to(device)\n",
        "    return models[model_key]\n",
        "\n",
        "\n",
        "def load_codec_model(use_gpu=True, force_reload=False):\n",
        "    global models\n",
        "    global models_devices\n",
        "    device = _grab_best_device(use_gpu=use_gpu)\n",
        "    if device == \"mps\":\n",
        "        # encodec doesn't support mps\n",
        "        device = \"cpu\"\n",
        "    model_key = \"codec\"\n",
        "    if OFFLOAD_CPU:\n",
        "        models_devices[model_key] = device\n",
        "        device = \"cpu\"\n",
        "    if model_key not in models or force_reload:\n",
        "        clean_models(model_key=model_key)\n",
        "        model = _load_codec_model(device)\n",
        "        models[model_key] = model\n",
        "    models[model_key].to(device)\n",
        "    return models[model_key]\n",
        "\n",
        "\n",
        "def preload_models_vc(\n",
        "    text_use_gpu=True,\n",
        "    text_use_small=False,\n",
        "    text_model_path=None,\n",
        "    coarse_use_gpu=True,\n",
        "    coarse_use_small=False,\n",
        "    coarse_model_path=None,\n",
        "    fine_use_gpu=True,\n",
        "    fine_use_small=False,\n",
        "    fine_model_path=None,\n",
        "    codec_use_gpu=True,\n",
        "    force_reload=False,\n",
        "    path=None,\n",
        "):\n",
        "    \"\"\"Load all the necessary models for the pipeline.\"\"\"\n",
        "    if _grab_best_device() == \"cpu\" and (\n",
        "        text_use_gpu or coarse_use_gpu or fine_use_gpu or codec_use_gpu\n",
        "    ):\n",
        "        logger.warning(\"No GPU being used. Careful, inference might be very slow!\")\n",
        "    _ = load_model(\n",
        "        model_type=\"text\", use_gpu=text_use_gpu, use_small=text_use_small, force_reload=force_reload, path=path if text_model_path is None else text_model_path\n",
        "    )\n",
        "    _ = load_model(\n",
        "        model_type=\"coarse\",\n",
        "        use_gpu=coarse_use_gpu,\n",
        "        use_small=coarse_use_small,\n",
        "        force_reload=force_reload,\n",
        "        path=path if coarse_model_path is None else coarse_model_path,\n",
        "    )\n",
        "    _ = load_model(\n",
        "        model_type=\"fine\", use_gpu=fine_use_gpu, use_small=fine_use_small, force_reload=force_reload, path=path if fine_model_path is None else fine_model_path\n",
        "    )\n",
        "    _ = load_codec_model(use_gpu=codec_use_gpu, force_reload=force_reload)\n",
        "\n",
        "\n",
        "####\n",
        "# Generation Functionality\n",
        "####\n",
        "\n",
        "def _tokenize(tokenizer, text):\n",
        "    return tokenizer.encode(text, add_special_tokens=False)\n",
        "\n",
        "\n",
        "def _detokenize(tokenizer, enc_text):\n",
        "    return tokenizer.decode(enc_text)\n",
        "\n",
        "\n",
        "def _normalize_whitespace(text):\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "TEXT_ENCODING_OFFSET = 10_048\n",
        "SEMANTIC_PAD_TOKEN = 10_000\n",
        "TEXT_PAD_TOKEN = 129_595\n",
        "SEMANTIC_INFER_TOKEN = 129_599\n",
        "\n",
        "\n",
        "def generate_text_semantic(\n",
        "    text,\n",
        "    history_prompt=None,\n",
        "    temp=0.7,\n",
        "    top_k=None,\n",
        "    top_p=None,\n",
        "    silent=False,\n",
        "    min_eos_p=0.2,\n",
        "    max_gen_duration_s=None,\n",
        "    allow_early_stop=True,\n",
        "    use_kv_caching=False,\n",
        "):\n",
        "    \"\"\"Generate semantic tokens from text.\"\"\"\n",
        "    assert isinstance(text, str)\n",
        "    text = _normalize_whitespace(text)\n",
        "    assert len(text.strip()) > 0\n",
        "    if history_prompt is not None:\n",
        "        if history_prompt.endswith(\".npz\"):\n",
        "            try:\n",
        "                semantic_history = np.load(history_prompt)[\"semantic_prompt\"]\n",
        "            except:\n",
        "                semantic_history = np.load(history_prompt)[\"semantic\"]\n",
        "        else:\n",
        "            semantic_history = np.load(\n",
        "                os.path.join(CUR_PATH, \"assets\", \"prompts\", f\"{history_prompt}.npz\")\n",
        "            )[\"semantic_prompt\"]\n",
        "        assert (\n",
        "            isinstance(semantic_history, np.ndarray)\n",
        "            and len(semantic_history.shape) == 1\n",
        "            and len(semantic_history) > 0\n",
        "            and semantic_history.min() >= 0\n",
        "            and semantic_history.max() <= SEMANTIC_VOCAB_SIZE - 1\n",
        "        )\n",
        "    else:\n",
        "        semantic_history = None\n",
        "    # load models if not yet exist\n",
        "    global models\n",
        "    global models_devices\n",
        "    if \"text\" not in models:\n",
        "        preload_models_vc()\n",
        "    model_container = models[\"text\"]\n",
        "    model = model_container[\"model\"]\n",
        "    tokenizer = model_container[\"tokenizer\"]\n",
        "    encoded_text = np.array(_tokenize(tokenizer, text)) + TEXT_ENCODING_OFFSET\n",
        "    if OFFLOAD_CPU:\n",
        "        model.to(models_devices[\"text\"])\n",
        "    device = next(model.parameters()).device\n",
        "    if len(encoded_text) > 256:\n",
        "        p = round((len(encoded_text) - 256) / len(encoded_text) * 100, 1)\n",
        "        logger.warning(f\"warning, text too long, lopping of last {p}%\")\n",
        "        encoded_text = encoded_text[:256]\n",
        "    encoded_text = np.pad(\n",
        "        encoded_text,\n",
        "        (0, 256 - len(encoded_text)),\n",
        "        constant_values=TEXT_PAD_TOKEN,\n",
        "        mode=\"constant\",\n",
        "    )\n",
        "    if semantic_history is not None:\n",
        "        semantic_history = semantic_history.astype(np.int64)\n",
        "        # lop off if history is too long, pad if needed\n",
        "        semantic_history = semantic_history[-256:]\n",
        "        semantic_history = np.pad(\n",
        "            semantic_history,\n",
        "            (0, 256 - len(semantic_history)),\n",
        "            constant_values=SEMANTIC_PAD_TOKEN,\n",
        "            mode=\"constant\",\n",
        "        )\n",
        "    else:\n",
        "        semantic_history = np.array([SEMANTIC_PAD_TOKEN] * 256)\n",
        "    x = torch.from_numpy(\n",
        "        np.hstack([\n",
        "            encoded_text, semantic_history, np.array([SEMANTIC_INFER_TOKEN])\n",
        "        ]).astype(np.int64)\n",
        "    )[None]\n",
        "    assert x.shape[1] == 256 + 256 + 1\n",
        "    with _inference_mode():\n",
        "        x = x.to(device)\n",
        "        n_tot_steps = 768\n",
        "        # custom tqdm updates since we don't know when eos will occur\n",
        "        pbar = tqdm(disable=silent, total=100)\n",
        "        pbar_state = 0\n",
        "        tot_generated_duration_s = 0\n",
        "        kv_cache = None\n",
        "        for n in range(n_tot_steps):\n",
        "            if use_kv_caching and kv_cache is not None:\n",
        "                x_input = x[:, [-1]]\n",
        "            else:\n",
        "                x_input = x\n",
        "            logits, kv_cache = model(\n",
        "                x_input, merge_context=True, use_cache=use_kv_caching, past_kv=kv_cache\n",
        "            )\n",
        "            relevant_logits = logits[0, 0, :SEMANTIC_VOCAB_SIZE]\n",
        "            if allow_early_stop:\n",
        "                relevant_logits = torch.hstack(\n",
        "                    (relevant_logits, logits[0, 0, [SEMANTIC_PAD_TOKEN]])  # eos\n",
        "                )\n",
        "            if top_p is not None:\n",
        "                # faster to convert to numpy\n",
        "                original_device = relevant_logits.device\n",
        "                relevant_logits = relevant_logits.detach().cpu().type(torch.float32).numpy()\n",
        "                sorted_indices = np.argsort(relevant_logits)[::-1]\n",
        "                sorted_logits = relevant_logits[sorted_indices]\n",
        "                cumulative_probs = np.cumsum(softmax(sorted_logits))\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].copy()\n",
        "                sorted_indices_to_remove[0] = False\n",
        "                relevant_logits[sorted_indices[sorted_indices_to_remove]] = -np.inf\n",
        "                relevant_logits = torch.from_numpy(relevant_logits)\n",
        "                relevant_logits = relevant_logits.to(original_device)\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(relevant_logits, min(top_k, relevant_logits.size(-1)))\n",
        "                relevant_logits[relevant_logits < v[-1]] = -float(\"Inf\")\n",
        "            probs = F.softmax(relevant_logits / temp, dim=-1)\n",
        "            # multinomial bugged on mps: shuttle to cpu if necessary\n",
        "            inf_device = probs.device\n",
        "            if probs.device.type == \"mps\":\n",
        "                probs = probs.to(\"cpu\")\n",
        "            item_next = torch.multinomial(probs, num_samples=1)\n",
        "            probs = probs.to(inf_device)\n",
        "            item_next = item_next.to(inf_device)\n",
        "            if allow_early_stop and (\n",
        "                item_next == SEMANTIC_VOCAB_SIZE\n",
        "                or (min_eos_p is not None and probs[-1] >= min_eos_p)\n",
        "            ):\n",
        "                # eos found, so break\n",
        "                pbar.update(100 - pbar_state)\n",
        "                break\n",
        "            x = torch.cat((x, item_next[None]), dim=1)\n",
        "            tot_generated_duration_s += 1 / SEMANTIC_RATE_HZ\n",
        "            if max_gen_duration_s is not None and tot_generated_duration_s > max_gen_duration_s:\n",
        "                pbar.update(100 - pbar_state)\n",
        "                break\n",
        "            if n == n_tot_steps - 1:\n",
        "                pbar.update(100 - pbar_state)\n",
        "                break\n",
        "            del logits, relevant_logits, probs, item_next\n",
        "            req_pbar_state = np.min([100, int(round(100 * n / n_tot_steps))])\n",
        "            if req_pbar_state > pbar_state:\n",
        "                pbar.update(req_pbar_state - pbar_state)\n",
        "            pbar_state = req_pbar_state\n",
        "        pbar.close()\n",
        "        out = x.detach().cpu().numpy().squeeze()[256 + 256 + 1 :]\n",
        "    if OFFLOAD_CPU:\n",
        "        model.to(\"cpu\")\n",
        "    assert all(0 <= out) and all(out < SEMANTIC_VOCAB_SIZE)\n",
        "    _clear_cuda_cache()\n",
        "    return out\n",
        "\n",
        "\n",
        "def _flatten_codebooks(arr, offset_size=CODEBOOK_SIZE):\n",
        "    assert len(arr.shape) == 2\n",
        "    arr = arr.copy()\n",
        "    if offset_size is not None:\n",
        "        for n in range(1, arr.shape[0]):\n",
        "            arr[n, :] += offset_size * n\n",
        "    flat_arr = arr.ravel(\"F\")\n",
        "    return flat_arr\n",
        "\n",
        "\n",
        "COARSE_SEMANTIC_PAD_TOKEN = 12_048\n",
        "COARSE_INFER_TOKEN = 12_050\n",
        "\n",
        "\n",
        "def generate_coarse(\n",
        "    x_semantic,\n",
        "    history_prompt=None,\n",
        "    temp=0.7,\n",
        "    top_k=None,\n",
        "    top_p=None,\n",
        "    silent=False,\n",
        "    max_coarse_history=630,  # min 60 (faster), max 630 (more context)\n",
        "    sliding_window_len=60,\n",
        "    use_kv_caching=False,\n",
        "):\n",
        "    \"\"\"Generate coarse audio codes from semantic tokens.\"\"\"\n",
        "    assert (\n",
        "        isinstance(x_semantic, np.ndarray)\n",
        "        and len(x_semantic.shape) == 1\n",
        "        and len(x_semantic) > 0\n",
        "        and x_semantic.min() >= 0\n",
        "        and x_semantic.max() <= SEMANTIC_VOCAB_SIZE - 1\n",
        "    )\n",
        "    assert 60 <= max_coarse_history <= 630\n",
        "    assert max_coarse_history + sliding_window_len <= 1024 - 256\n",
        "    semantic_to_coarse_ratio = COARSE_RATE_HZ / SEMANTIC_RATE_HZ * N_COARSE_CODEBOOKS\n",
        "    max_semantic_history = int(np.floor(max_coarse_history / semantic_to_coarse_ratio))\n",
        "    if history_prompt is not None:\n",
        "        if history_prompt.endswith(\".npz\"):\n",
        "            x_history = np.load(history_prompt)\n",
        "        else:\n",
        "            x_history = np.load(\n",
        "                os.path.join(CUR_PATH, \"assets\", \"prompts\", f\"{history_prompt}.npz\")\n",
        "            )\n",
        "        try:\n",
        "            x_semantic_history = x_history[\"semantic_prompt\"]\n",
        "            x_coarse_history = x_history[\"coarse_prompt\"]\n",
        "        except:\n",
        "            x_semantic_history = x_history[\"semantic\"]\n",
        "            x_coarse_history = x_history[\"coarse\"]\n",
        "        assert (\n",
        "            isinstance(x_semantic_history, np.ndarray)\n",
        "            and len(x_semantic_history.shape) == 1\n",
        "            and len(x_semantic_history) > 0\n",
        "            and x_semantic_history.min() >= 0\n",
        "            and x_semantic_history.max() <= SEMANTIC_VOCAB_SIZE - 1\n",
        "            and isinstance(x_coarse_history, np.ndarray)\n",
        "            and len(x_coarse_history.shape) == 2\n",
        "            and x_coarse_history.shape[0] == N_COARSE_CODEBOOKS\n",
        "            and x_coarse_history.shape[-1] >= 0\n",
        "            and x_coarse_history.min() >= 0\n",
        "            and x_coarse_history.max() <= CODEBOOK_SIZE - 1\n",
        "            and (\n",
        "                round(x_coarse_history.shape[-1] / len(x_semantic_history), 1)\n",
        "                == round(semantic_to_coarse_ratio / N_COARSE_CODEBOOKS, 1)\n",
        "            )\n",
        "        )\n",
        "        x_coarse_history = _flatten_codebooks(x_coarse_history) + SEMANTIC_VOCAB_SIZE\n",
        "        # trim histories correctly\n",
        "        n_semantic_hist_provided = np.min(\n",
        "            [\n",
        "                max_semantic_history,\n",
        "                len(x_semantic_history) - len(x_semantic_history) % 2,\n",
        "                int(np.floor(len(x_coarse_history) / semantic_to_coarse_ratio)),\n",
        "            ]\n",
        "        )\n",
        "        n_coarse_hist_provided = int(round(n_semantic_hist_provided * semantic_to_coarse_ratio))\n",
        "        x_semantic_history = x_semantic_history[-n_semantic_hist_provided:].astype(np.int32)\n",
        "        x_coarse_history = x_coarse_history[-n_coarse_hist_provided:].astype(np.int32)\n",
        "        # TODO: bit of a hack for time alignment (sounds better)\n",
        "        x_coarse_history = x_coarse_history[:-2]\n",
        "    else:\n",
        "        x_semantic_history = np.array([], dtype=np.int32)\n",
        "        x_coarse_history = np.array([], dtype=np.int32)\n",
        "    # load models if not yet exist\n",
        "    global models\n",
        "    global models_devices\n",
        "    if \"coarse\" not in models:\n",
        "        preload_models_vc()\n",
        "    model = models[\"coarse\"]\n",
        "    if OFFLOAD_CPU:\n",
        "        model.to(models_devices[\"coarse\"])\n",
        "    device = next(model.parameters()).device\n",
        "    # start loop\n",
        "    n_steps = int(\n",
        "        round(\n",
        "            np.floor(len(x_semantic) * semantic_to_coarse_ratio / N_COARSE_CODEBOOKS)\n",
        "            * N_COARSE_CODEBOOKS\n",
        "        )\n",
        "    )\n",
        "    assert n_steps > 0 and n_steps % N_COARSE_CODEBOOKS == 0\n",
        "    x_semantic = np.hstack([x_semantic_history, x_semantic]).astype(np.int32)\n",
        "    x_coarse = x_coarse_history.astype(np.int32)\n",
        "    base_semantic_idx = len(x_semantic_history)\n",
        "    with _inference_mode():\n",
        "        x_semantic_in = torch.from_numpy(x_semantic)[None].to(device)\n",
        "        x_coarse_in = torch.from_numpy(x_coarse)[None].to(device)\n",
        "        n_window_steps = int(np.ceil(n_steps / sliding_window_len))\n",
        "        n_step = 0\n",
        "        for _ in tqdm(range(n_window_steps), total=n_window_steps, disable=silent):\n",
        "            semantic_idx = base_semantic_idx + int(round(n_step / semantic_to_coarse_ratio))\n",
        "            # pad from right side\n",
        "            x_in = x_semantic_in[:, np.max([0, semantic_idx - max_semantic_history]) :]\n",
        "            x_in = x_in[:, :256]\n",
        "            x_in = F.pad(\n",
        "                x_in,\n",
        "                (0, 256 - x_in.shape[-1]),\n",
        "                \"constant\",\n",
        "                COARSE_SEMANTIC_PAD_TOKEN,\n",
        "            )\n",
        "            x_in = torch.hstack(\n",
        "                [\n",
        "                    x_in,\n",
        "                    torch.tensor([COARSE_INFER_TOKEN])[None].to(device),\n",
        "                    x_coarse_in[:, -max_coarse_history:],\n",
        "                ]\n",
        "            )\n",
        "            kv_cache = None\n",
        "            for _ in range(sliding_window_len):\n",
        "                if n_step >= n_steps:\n",
        "                    continue\n",
        "                is_major_step = n_step % N_COARSE_CODEBOOKS == 0\n",
        "\n",
        "                if use_kv_caching and kv_cache is not None:\n",
        "                    x_input = x_in[:, [-1]]\n",
        "                else:\n",
        "                    x_input = x_in\n",
        "\n",
        "                logits, kv_cache = model(x_input, use_cache=use_kv_caching, past_kv=kv_cache)\n",
        "                logit_start_idx = (\n",
        "                    SEMANTIC_VOCAB_SIZE + (1 - int(is_major_step)) * CODEBOOK_SIZE\n",
        "                )\n",
        "                logit_end_idx = (\n",
        "                    SEMANTIC_VOCAB_SIZE + (2 - int(is_major_step)) * CODEBOOK_SIZE\n",
        "                )\n",
        "                relevant_logits = logits[0, 0, logit_start_idx:logit_end_idx]\n",
        "                if top_p is not None:\n",
        "                    # faster to convert to numpy\n",
        "                    original_device = relevant_logits.device\n",
        "                    relevant_logits = relevant_logits.detach().cpu().type(torch.float32).numpy()\n",
        "                    sorted_indices = np.argsort(relevant_logits)[::-1]\n",
        "                    sorted_logits = relevant_logits[sorted_indices]\n",
        "                    cumulative_probs = np.cumsum(softmax(sorted_logits))\n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].copy()\n",
        "                    sorted_indices_to_remove[0] = False\n",
        "                    relevant_logits[sorted_indices[sorted_indices_to_remove]] = -np.inf\n",
        "                    relevant_logits = torch.from_numpy(relevant_logits)\n",
        "                    relevant_logits = relevant_logits.to(original_device)\n",
        "                if top_k is not None:\n",
        "                    v, _ = torch.topk(relevant_logits, min(top_k, relevant_logits.size(-1)))\n",
        "                    relevant_logits[relevant_logits < v[-1]] = -float(\"Inf\")\n",
        "                probs = F.softmax(relevant_logits / temp, dim=-1)\n",
        "                # multinomial bugged on mps: shuttle to cpu if necessary\n",
        "                inf_device = probs.device\n",
        "                if probs.device.type == \"mps\":\n",
        "                    probs = probs.to(\"cpu\")\n",
        "                item_next = torch.multinomial(probs, num_samples=1)\n",
        "                probs = probs.to(inf_device)\n",
        "                item_next = item_next.to(inf_device)\n",
        "                item_next += logit_start_idx\n",
        "                x_coarse_in = torch.cat((x_coarse_in, item_next[None]), dim=1)\n",
        "                x_in = torch.cat((x_in, item_next[None]), dim=1)\n",
        "                del logits, relevant_logits, probs, item_next\n",
        "                n_step += 1\n",
        "            del x_in\n",
        "        del x_semantic_in\n",
        "    if OFFLOAD_CPU:\n",
        "        model.to(\"cpu\")\n",
        "    gen_coarse_arr = x_coarse_in.detach().cpu().numpy().squeeze()[len(x_coarse_history) :]\n",
        "    del x_coarse_in\n",
        "    assert len(gen_coarse_arr) == n_steps\n",
        "    gen_coarse_audio_arr = gen_coarse_arr.reshape(-1, N_COARSE_CODEBOOKS).T - SEMANTIC_VOCAB_SIZE\n",
        "    for n in range(1, N_COARSE_CODEBOOKS):\n",
        "        gen_coarse_audio_arr[n, :] -= n * CODEBOOK_SIZE\n",
        "    _clear_cuda_cache()\n",
        "    return gen_coarse_audio_arr\n",
        "\n",
        "\n",
        "def generate_fine(\n",
        "    x_coarse_gen,\n",
        "    history_prompt=None,\n",
        "    temp=0.5,\n",
        "    silent=True,\n",
        "):\n",
        "    \"\"\"Generate full audio codes from coarse audio codes.\"\"\"\n",
        "    assert (\n",
        "        isinstance(x_coarse_gen, np.ndarray)\n",
        "        and len(x_coarse_gen.shape) == 2\n",
        "        and 1 <= x_coarse_gen.shape[0] <= N_FINE_CODEBOOKS - 1\n",
        "        and x_coarse_gen.shape[1] > 0\n",
        "        and x_coarse_gen.min() >= 0\n",
        "        and x_coarse_gen.max() <= CODEBOOK_SIZE - 1\n",
        "    )\n",
        "    if history_prompt is not None:\n",
        "        if history_prompt.endswith(\".npz\"):\n",
        "            try:\n",
        "                x_fine_history = np.load(history_prompt)[\"fine_prompt\"]\n",
        "            except:\n",
        "                x_fine_history = np.load(history_prompt)[\"fine\"]\n",
        "        else:\n",
        "            x_fine_history = np.load(\n",
        "                os.path.join(CUR_PATH, \"assets\", \"prompts\", f\"{history_prompt}.npz\")\n",
        "            )[\"fine_prompt\"]\n",
        "        assert (\n",
        "            isinstance(x_fine_history, np.ndarray)\n",
        "            and len(x_fine_history.shape) == 2\n",
        "            and x_fine_history.shape[0] == N_FINE_CODEBOOKS\n",
        "            and x_fine_history.shape[1] >= 0\n",
        "            and x_fine_history.min() >= 0\n",
        "            and x_fine_history.max() <= CODEBOOK_SIZE - 1\n",
        "        )\n",
        "    else:\n",
        "        x_fine_history = None\n",
        "    n_coarse = x_coarse_gen.shape[0]\n",
        "    # load models if not yet exist\n",
        "    global models\n",
        "    global models_devices\n",
        "    if \"fine\" not in models:\n",
        "        preload_models_vc()\n",
        "    model = models[\"fine\"]\n",
        "    if OFFLOAD_CPU:\n",
        "        model.to(models_devices[\"fine\"])\n",
        "    device = next(model.parameters()).device\n",
        "    # make input arr\n",
        "    in_arr = np.vstack(\n",
        "        [\n",
        "            x_coarse_gen,\n",
        "            np.zeros((N_FINE_CODEBOOKS - n_coarse, x_coarse_gen.shape[1]))\n",
        "            + CODEBOOK_SIZE,  # padding\n",
        "        ]\n",
        "    ).astype(np.int32)\n",
        "    # prepend history if available (max 512)\n",
        "    if x_fine_history is not None:\n",
        "        x_fine_history = x_fine_history.astype(np.int32)\n",
        "        in_arr = np.hstack(\n",
        "            [\n",
        "                x_fine_history[:, -512:].astype(np.int32),\n",
        "                in_arr,\n",
        "            ]\n",
        "        )\n",
        "        n_history = x_fine_history[:, -512:].shape[1]\n",
        "    else:\n",
        "        n_history = 0\n",
        "    n_remove_from_end = 0\n",
        "    # need to pad if too short (since non-causal model)\n",
        "    if in_arr.shape[1] < 1024:\n",
        "        n_remove_from_end = 1024 - in_arr.shape[1]\n",
        "        in_arr = np.hstack(\n",
        "            [\n",
        "                in_arr,\n",
        "                np.zeros((N_FINE_CODEBOOKS, n_remove_from_end), dtype=np.int32) + CODEBOOK_SIZE,\n",
        "            ]\n",
        "        )\n",
        "    # we can be lazy about fractional loop and just keep overwriting codebooks\n",
        "    n_loops = np.max([0, int(np.ceil((x_coarse_gen.shape[1] - (1024 - n_history)) / 512))]) + 1\n",
        "    with _inference_mode():\n",
        "        in_arr = torch.tensor(in_arr.T).to(device)\n",
        "        for n in tqdm(range(n_loops), disable=silent):\n",
        "            start_idx = np.min([n * 512, in_arr.shape[0] - 1024])\n",
        "            start_fill_idx = np.min([n_history + n * 512, in_arr.shape[0] - 512])\n",
        "            rel_start_fill_idx = start_fill_idx - start_idx\n",
        "            in_buffer = in_arr[start_idx : start_idx + 1024, :][None]\n",
        "            for nn in range(n_coarse, N_FINE_CODEBOOKS):\n",
        "                logits = model(nn, in_buffer)\n",
        "                if temp is None:\n",
        "                    relevant_logits = logits[0, rel_start_fill_idx:, :CODEBOOK_SIZE]\n",
        "                    codebook_preds = torch.argmax(relevant_logits, -1)\n",
        "                else:\n",
        "                    relevant_logits = logits[0, :, :CODEBOOK_SIZE] / temp\n",
        "                    probs = F.softmax(relevant_logits, dim=-1)\n",
        "                    # multinomial bugged on mps: shuttle to cpu if necessary\n",
        "                    inf_device = probs.device\n",
        "                    if probs.device.type == \"mps\":\n",
        "                        probs = probs.to(\"cpu\")\n",
        "                    codebook_preds = torch.hstack(\n",
        "                        [\n",
        "                            torch.multinomial(probs[nnn], num_samples=1).to(inf_device)\n",
        "                            for nnn in range(rel_start_fill_idx, 1024)\n",
        "                        ]\n",
        "                    )\n",
        "                in_buffer[0, rel_start_fill_idx:, nn] = codebook_preds\n",
        "                del logits, codebook_preds\n",
        "            # transfer over info into model_in and convert to numpy\n",
        "            for nn in range(n_coarse, N_FINE_CODEBOOKS):\n",
        "                in_arr[\n",
        "                    start_fill_idx : start_fill_idx + (1024 - rel_start_fill_idx), nn\n",
        "                ] = in_buffer[0, rel_start_fill_idx:, nn]\n",
        "            del in_buffer\n",
        "        gen_fine_arr = in_arr.detach().cpu().numpy().squeeze().T\n",
        "        del in_arr\n",
        "    if OFFLOAD_CPU:\n",
        "        model.to(\"cpu\")\n",
        "    gen_fine_arr = gen_fine_arr[:, n_history:]\n",
        "    if n_remove_from_end > 0:\n",
        "        gen_fine_arr = gen_fine_arr[:, :-n_remove_from_end]\n",
        "    assert gen_fine_arr.shape[-1] == x_coarse_gen.shape[-1]\n",
        "    _clear_cuda_cache()\n",
        "    return gen_fine_arr\n",
        "\n",
        "\n",
        "def codec_decode(fine_tokens):\n",
        "    \"\"\"Turn quantized audio codes into audio array using encodec.\"\"\"\n",
        "    # load models if not yet exist\n",
        "    global models\n",
        "    global models_devices\n",
        "    if \"codec\" not in models:\n",
        "        preload_models_vc()\n",
        "    model = models[\"codec\"]\n",
        "    if OFFLOAD_CPU:\n",
        "        model.to(models_devices[\"codec\"])\n",
        "    device = next(model.parameters()).device\n",
        "    arr = torch.from_numpy(fine_tokens)[None]\n",
        "    arr = arr.to(device)\n",
        "    arr = arr.transpose(0, 1)\n",
        "    emb = model.quantizer.decode(arr)\n",
        "    out = model.decoder(emb)\n",
        "    audio_arr = out.detach().cpu().numpy().squeeze()\n",
        "    del arr, emb, out\n",
        "    if OFFLOAD_CPU:\n",
        "        model.to(\"cpu\")\n",
        "    return audio_arr\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Atsnm7V761Pv"
      },
      "outputs": [],
      "source": [
        "semantic_path = None # set to None if you don't want to use finetuned semantic\n",
        "coarse_path = None # set to None if you don't want to use finetuned coarse\n",
        "fine_path = \"/content/drive/MyDrive/datasets_yoo/pytorch_model_750.bin\" # set to None if you don't want to use finetuned fine\n",
        "use_rvc = False # Set to False to use bark without RVC\n",
        "#rvc_name = 'mi-test'\n",
        "#rvc_path = f\"Retrieval-based-Voice-Conversion-WebUI/weights/{rvc_name}.pth\"\n",
        "#index_path = f\"Retrieval-based-Voice-Conversion-WebUI/logs/{rvc_name}/added_IVF256_Flat_nprobe_1_{rvc_name}_v2.index\"\n",
        "device=\"cuda:0\"\n",
        "is_half=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "7679ebdba94842e793730222c31ecb43",
            "3fa3e9e985fe4ba99d116719e98c4b8b",
            "57ebf49d76044909b159f0ec3fc42ab2",
            "51dfec969c49482281754e51a4c9778c",
            "b167fd315e344a8f8db90ddee7f87b4c",
            "70171b289e304c9a82a13ca0a492295f",
            "d571d2251e814f4ba99dd6497ef8c98e",
            "eb51cc4cbbbe4a0b8f6362816d0b381a",
            "0d22fd76b0ab408ba8982ce947ca758b",
            "3e083fb994b94f47924e6a34a298719c",
            "5f60ea35c7f64172b3b8cf99051f95eb",
            "3a27556f0a0c45a1a8eb1b04b7127834",
            "c47678a6f59d437ca952ae2d094c3861",
            "82b0f5b78aa440c2af3b6693c08d97fc",
            "33d75ee333204273a0a46b8e1283a240",
            "cc7f9823eefa4fcc97e3484af4647ae2",
            "a0486770b6634729be6dcec83c48000c",
            "89d221f036304c138c4cd052e99e7244",
            "8b0fa2900392488da3bc0fc449409a44",
            "d135c792ff0c4b908491eb957c3e827f",
            "cb3b07086ed74e978b21886c2031c83b",
            "48c8b3e08e8a4ba09a514a24a0f5e340",
            "7144ff96fc134c3bbd4ed44011294ea1",
            "97c25453452e4d528cdaf33b9259692c",
            "2b33a056b7e347fc816f5696cde7902d",
            "df8ffe58bcf14d8e85480466a90e8c40",
            "826a5d9da9644f58b0a2400f31341a03",
            "034bb8ab26624c0e95c4c64a086e4d30",
            "8546b3d5e29b4743996a55abc571bb47",
            "75dd621c86ab4aa1bdb8d1dd88ec466c",
            "cef203e2c25b474a838fee4eabdbb30b",
            "1fb18574442a4bf086bf95d57677fdab",
            "a9ccb0db815b42d49cf084aa68e8f8ee",
            "2e881b86e7f84ab08029d5e18454affd",
            "f4559d6b7f484751a1d4e266cf03d701",
            "978b3913fd194cc085d6fdf54096eebb",
            "5683f3402d724e81908bf4479e213bc4",
            "8560a9b641e6473ea01d16b60ef06f34",
            "3905e4817ed449f9a6e1717b9c8188d4",
            "e3b1874249e245b895c04b76c15b320a",
            "eebf9f171f014eb19a2bc22bf3e5d05d",
            "49c47511cad44c049023d9a2a6ad8c72",
            "07cab618c26e4e7580e752bdefc1287d",
            "98577a84e61045faa90b5bc1fc8a5dc9",
            "ba2aed87246d4170855105a7ca2c588b",
            "755937fcfd384b21be71b718f01cdfa0",
            "6dfec48e70f04d43a0fb6ee7833b8dfb",
            "d868bcb81419456c8eb2ad9c1a12bf43",
            "7480c94763fc49ffaf173f4e17f9ed20",
            "6b21eca06b474df9a7943b13cbd10ea7",
            "05f64218d14e4efebf072251aa33303d",
            "82b88458eb5f41d99fe825b8fb6ee832",
            "2e2cc75781614c6bb82decf7e747fa3b",
            "4bfdc4b2ae4d41cc932dcf3cfdb54c31",
            "6e09f777544a44cda57f6320f243226e",
            "a581f984b0b84fcdb0314609c3af4b79",
            "ee8df020601f4d4186a5c7980ec4946c",
            "453394a755744c07af6fcd45c8d03beb",
            "128439ae98ee4d1098ed6a03d38ca581",
            "64229edb1fb74e5096fa228348f1003f",
            "0ce3dc22f7834beb9c43ba2bd376958d",
            "9d587ce51b314f90ab12b0bef04c1c78",
            "6d78dc8f3bbf45628378f7eb9ffdb95f",
            "d7b0be4e73134853b04e36fea028f1cb",
            "ddfb5bc44bd14b3b9de16749b81ca23a",
            "132f07038cc3433ea284c63e4e31b12d",
            "727fe931a1584644ae2806397f895656",
            "b43efa154a82416db888601a0bfb5cf2",
            "c6dd6b0e3aa1414984a30d9120874e4b",
            "4494c64e7a334748ba6dd6c7ad1a06b8",
            "95c63393ceb84a19ad0605c2730dec8c",
            "212ad381652a4be1909543df31bc90fa",
            "3e4dca17e3c74c17bb4d931101987b29",
            "4aa9f91391594227872598ca62f666af",
            "177a16783e774dd4b3432f6dd5eab692",
            "95583572ef3f45e0b5a40b9bf56fcb6c",
            "8fa9b2e09346481f903dd376118c2061"
          ]
        },
        "id": "aSgbFqty7Yxo",
        "outputId": "ade5f985-8856-4e6f-9cf4-d9a38f8919be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "text_2.pt:   0%|          | 0.00/5.35G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7679ebdba94842e793730222c31ecb43"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a27556f0a0c45a1a8eb1b04b7127834"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7144ff96fc134c3bbd4ed44011294ea1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e881b86e7f84ab08029d5e18454affd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba2aed87246d4170855105a7ca2c588b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "coarse_2.pt:   0%|          | 0.00/3.93G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a581f984b0b84fcdb0314609c3af4b79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "fine_2.pt:   0%|          | 0.00/3.74G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "727fe931a1584644ae2806397f895656"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/encodec/v0/encodec_24khz-d7cc33bc.th\" to /root/.cache/torch/hub/checkpoints/encodec_24khz-d7cc33bc.th\n",
            "100%|██████████| 88.9M/88.9M [00:00<00:00, 99.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "preload_models_vc(\n",
        "    text_use_gpu=True,\n",
        "    text_use_small=False,\n",
        "    text_model_path=semantic_path,\n",
        "    coarse_use_gpu=True,\n",
        "    coarse_use_small=False,\n",
        "    coarse_model_path=coarse_path,\n",
        "    fine_use_gpu=True,\n",
        "    fine_use_small=False,\n",
        "    fine_model_path=fine_path,\n",
        "    codec_use_gpu=True,\n",
        "    force_reload=False,\n",
        "    path=\"models\"\n",
        ")\n",
        "\n",
        "if use_rvc:\n",
        "    from rvc_infer import get_vc, vc_single\n",
        "    get_vc(rvc_path, device, is_half)\n",
        "# simple generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAgUcUPc8NLD"
      },
      "source": [
        "### 긴문구"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DZqGbyLl8Lr-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def split_and_recombine_text(text, desired_length=100, max_length=150):\n",
        "    # from https://github.com/neonbjb/tortoise-tts\n",
        "    \"\"\"Split text it into chunks of a desired length trying to keep sentences intact.\"\"\"\n",
        "    # normalize text, remove redundant whitespace and convert non-ascii quotes to ascii\n",
        "    text = re.sub(r\"\\n\\n+\", \"\\n\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"[“”]\", '\"', text)\n",
        "\n",
        "    rv = []\n",
        "    in_quote = False\n",
        "    current = \"\"\n",
        "    split_pos = []\n",
        "    pos = -1\n",
        "    end_pos = len(text) - 1\n",
        "\n",
        "    def seek(delta):\n",
        "        nonlocal pos, in_quote, current\n",
        "        is_neg = delta < 0\n",
        "        for _ in range(abs(delta)):\n",
        "            if is_neg:\n",
        "                pos -= 1\n",
        "                current = current[:-1]\n",
        "            else:\n",
        "                pos += 1\n",
        "                current += text[pos]\n",
        "            if text[pos] == '\"':\n",
        "                in_quote = not in_quote\n",
        "        return text[pos]\n",
        "\n",
        "    def peek(delta):\n",
        "        p = pos + delta\n",
        "        return text[p] if p < end_pos and p >= 0 else \"\"\n",
        "\n",
        "    def commit():\n",
        "        nonlocal rv, current, split_pos\n",
        "        rv.append(current)\n",
        "        current = \"\"\n",
        "        split_pos = []\n",
        "\n",
        "    while pos < end_pos:\n",
        "        c = seek(1)\n",
        "        # do we need to force a split?\n",
        "        if len(current) >= max_length:\n",
        "            if len(split_pos) > 0 and len(current) > (desired_length / 2):\n",
        "                # we have at least one sentence and we are over half the desired length, seek back to the last split\n",
        "                d = pos - split_pos[-1]\n",
        "                seek(-d)\n",
        "            else:\n",
        "                # no full sentences, seek back until we are not in the middle of a word and split there\n",
        "                while c not in \"!?.\\n \" and pos > 0 and len(current) > desired_length:\n",
        "                    c = seek(-1)\n",
        "            commit()\n",
        "        # check for sentence boundaries\n",
        "        elif not in_quote and (c in \"!?\\n\" or (c == \".\" and peek(1) in \"\\n \")):\n",
        "            # seek forward if we have consecutive boundary markers but still within the max length\n",
        "            while (\n",
        "                pos < len(text) - 1 and len(current) < max_length and peek(1) in \"!?.\"\n",
        "            ):\n",
        "                c = seek(1)\n",
        "            split_pos.append(pos)\n",
        "            if len(current) >= desired_length:\n",
        "                commit()\n",
        "        # treat end of quote as a boundary if its followed by a space or newline\n",
        "        elif in_quote and peek(1) == '\"' and peek(2) in \"\\n \":\n",
        "            seek(2)\n",
        "            split_pos.append(pos)\n",
        "    rv.append(current)\n",
        "\n",
        "    # clean up, remove lines with only whitespace or punctuation\n",
        "    rv = [s.strip() for s in rv]\n",
        "    rv = [s for s in rv if len(s) > 0 and not re.match(r\"^[\\s\\.,;:!?]*$\", s)]\n",
        "\n",
        "    return rv\n",
        "\n",
        "\n",
        "def generate_with_settings(text_prompt, semantic_temp=0.7, semantic_top_k=50, semantic_top_p=0.95, coarse_temp=0.7, coarse_top_k=50, coarse_top_p=0.95, fine_temp=0.5, voice_name_1=None,voice_name_2=None, use_semantic_history_prompt=True, use_coarse_history_prompt=True, use_fine_history_prompt=True, output_full=False):\n",
        "    # generation with more control\n",
        "    x_semantic = generate_text_semantic(\n",
        "        text_prompt,\n",
        "        history_prompt=voice_name_1 if use_semantic_history_prompt else None,\n",
        "        temp=semantic_temp,\n",
        "        top_k=semantic_top_k,\n",
        "        top_p=semantic_top_p,\n",
        "    )\n",
        "\n",
        "    x_coarse_gen = generate_coarse(\n",
        "        x_semantic,\n",
        "        history_prompt=voice_name if use_coarse_history_prompt else None,\n",
        "        temp=coarse_temp,\n",
        "        top_k=coarse_top_k,\n",
        "        top_p=coarse_top_p,\n",
        "    )\n",
        "    x_fine_gen = generate_fine(\n",
        "        x_coarse_gen,\n",
        "        history_prompt=voice_name_2 if use_fine_history_prompt else None,\n",
        "        temp=fine_temp,\n",
        "    )\n",
        "\n",
        "    if output_full:\n",
        "        full_generation = {\n",
        "            'semantic_prompt': x_semantic,\n",
        "            'coarse_prompt': x_coarse_gen,\n",
        "            'fine_prompt': x_fine_gen,\n",
        "        }\n",
        "        return full_generation, codec_decode(x_fine_gen)\n",
        "    return codec_decode(x_fine_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KOEOnOJC8f2r"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"저희의 최종 결과물은 다음과 같습니다.얼굴의 경우 최대한 얼굴의 움직임이 없는 유재석 얼굴과, 입술 움직임이 정확해 트래킹이 쉬운 뉴스앵커를 합성해 딥페이크 모델을 제작하였으며.\n",
        "목소리의 경우, 유재석 목소리가 학습된 TTS 에 텍스트를 넣고, 유재석 목소리 중 하나의 엔피지와 히스토리 엔피지를 사용해 이 문장의 한 구, 한 구 가 생성될 때 마다..보정을 하여 더 정확한 유재석 목소리 웨이브파일을 생성합니다.\n",
        "이 생성된 오디오 데이터를 MFCC로 변환 후 데이터 프레임화 해, 딥페이크 모델 영상을 초 단위로 분할합니다.그 후 프레임 당 입술 랜드마크를 추출해 좌표를 표준화시키고, 데이터 프레임화 단계를 진행합니다.\n",
        "최종적으로 출력된 좌표값을 이용해 웨이브파일을 입힌 영상을 추출하면 현재 보시는 바와 같이 입력 텍스트를 읽어주는 보이스 스타일링 기반 딥페이크 모델이 완성됩니다.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HYI293NrBq_x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "\n",
        "# 샘플레이트 설정 (예: 44100Hz)\n",
        "sample_rate = 24000\n",
        "\n",
        "# 기본 오디오 데이터 생성 (예: 1초 길이의 무음)\n",
        "# np.zeros를 사용하여 모든 샘플이 0인 배열 생성\n",
        "duration = 1  # 1초\n",
        "audio_data = np.zeros(sample_rate * duration)\n",
        "\n",
        "# 데이터 타입을 32비트 부동소수점 형식으로 변환\n",
        "audio_data = audio_data.astype(np.float32)\n",
        "\n",
        "# WAV 파일로 저장\n",
        "wavfile.write('/content/audio/audio.wav', sample_rate, audio_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tzPoQTo8iW6"
      },
      "outputs": [],
      "source": [
        "# Chunk the text into smaller pieces then combine the generated audio\n",
        "#같은 대사 여러번 하는게.?\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "from IPython.display import Audio\n",
        "from scipy.io.wavfile import write as write_wav\n",
        "import os\n",
        "import numpy as np\n",
        "#/content/datasets_yoo/tokens/yujaeseog17.npz\n",
        "# generation settings\n",
        "voice_name_1= \"/content/bark/bark/assets/prompts/ko_speaker_4.npz\"\n",
        "voice_name_2 = \"/content/drive/MyDrive/datasets_yoo/tokens/yujaeseog66.npz\"\n",
        "out_filepath = '/content/audio/audio.wav'\n",
        "\n",
        "semantic_temp = 0.7 #높은 값 (예: 1 이상)은 더 다양하고 예측 불가능한 텍스트를 생성하는 데 기여합니다.\n",
        "semantic_top_k = 100 #모델이 다음 단어를 선택할 때 고려하는 후보 단어의 집합을 k개의 가장 높은 확률을 가진 단어로 제한합니다.\n",
        "semantic_top_p = 0.99\n",
        "\n",
        "coarse_temp = 0.7\n",
        "coarse_top_k = 100\n",
        "coarse_top_p = 0.95\n",
        "\n",
        "fine_temp = 0.7\n",
        "\n",
        "use_semantic_history_prompt = True\n",
        "use_coarse_history_prompt = False\n",
        "use_fine_history_prompt = True\n",
        "\n",
        "use_last_generation_as_history = False  #지난 루프가..\n",
        "\n",
        "if use_rvc:\n",
        "    index_rate = 0.75\n",
        "    f0up_key = -6\n",
        "    filter_radius = 3\n",
        "    rms_mix_rate = 0.25\n",
        "    protect = 0.33\n",
        "    resample_sr = SAMPLE_RATE\n",
        "    f0method = \"harvest\" #harvest or pm\n",
        "\n",
        "texts = split_and_recombine_text(text)\n",
        "\n",
        "all_parts = []\n",
        "for i, text in tqdm(enumerate(texts), total=len(texts)):\n",
        "    full_generation, audio_array = generate_with_settings(\n",
        "        text,\n",
        "        semantic_temp=semantic_temp,\n",
        "        semantic_top_k=semantic_top_k,\n",
        "        semantic_top_p=semantic_top_p,\n",
        "        coarse_temp=coarse_temp,\n",
        "        coarse_top_k=coarse_top_k,\n",
        "        coarse_top_p=coarse_top_p,\n",
        "        fine_temp=fine_temp,\n",
        "        voice_name_1=voice_name_1,\n",
        "        voice_name_2=voice_name_2,\n",
        "        use_semantic_history_prompt=use_semantic_history_prompt,\n",
        "        use_coarse_history_prompt=use_coarse_history_prompt,\n",
        "        use_fine_history_prompt=use_fine_history_prompt,\n",
        "        output_full=True\n",
        "    )\n",
        "    if use_last_generation_as_history:\n",
        "        # save to npz\n",
        "        os.makedirs('_temp', exist_ok=True)\n",
        "        np.savez_compressed(\n",
        "            '_temp/history.npz',\n",
        "            semantic_prompt=full_generation['semantic_prompt'],\n",
        "            coarse_prompt=full_generation['coarse_prompt'],\n",
        "            fine_prompt=full_generation['fine_prompt'],\n",
        "        )\n",
        "        voice_name = '_temp/history.npz'\n",
        "    write_wav(out_filepath.replace('.wav', f'_{i}') + '.wav', SAMPLE_RATE, audio_array)\n",
        "\n",
        "    if use_rvc:\n",
        "        try:\n",
        "            audio_array = vc_single(0,out_filepath.replace('.wav', f'_{i}') + '.wav',f0up_key,None,f0method,index_path,index_rate, filter_radius=filter_radius, resample_sr=resample_sr, rms_mix_rate=rms_mix_rate, protect=protect)\n",
        "        except:\n",
        "            audio_array = vc_single(0,out_filepath.replace('.wav', f'_{i}') + '.wav',f0up_key,None,'pm',index_path,index_rate, filter_radius=filter_radius, resample_sr=resample_sr, rms_mix_rate=rms_mix_rate, protect=protect)\n",
        "        write_wav(out_filepath.replace('.wav', f'_{i}') + '.wav', SAMPLE_RATE, audio_array)\n",
        "    all_parts.append(audio_array)\n",
        "\n",
        "audio_array = np.concatenate(all_parts, axis=-1)\n",
        "\n",
        "# save audio\n",
        "write_wav(out_filepath, SAMPLE_RATE, audio_array)\n",
        "\n",
        "# play audio\n",
        "Audio(audio_array, rate=SAMPLE_RATE)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kUS4vZga-n0t",
        "HJQ4TI0_Qowr",
        "VbIE0Bv8jxtN"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "316525532b0b4a2a9a0ebae9c1424391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2dfbbb31081a4249872eea56060e6e41",
              "IPY_MODEL_0304dc9fb59e4fdfb1cbedc5e27f3db0",
              "IPY_MODEL_dabe1c776139450fa83e62dd7f92bdc7"
            ],
            "layout": "IPY_MODEL_e84459a790ec46ed990c8270b7235218"
          }
        },
        "2dfbbb31081a4249872eea56060e6e41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b6757afd74545cb893a63daf4c0d8bf",
            "placeholder": "​",
            "style": "IPY_MODEL_710d9ac61a9744dd90b31aba55ee226a",
            "value": "quantifier_hubert_base_ls960_14.pth: 100%"
          }
        },
        "0304dc9fb59e4fdfb1cbedc5e27f3db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_412966c4adf84d79b36bf0d18b0d9263",
            "max": 103981977,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c7610747ee2d40c3a2698385705c8b36",
            "value": 103981977
          }
        },
        "dabe1c776139450fa83e62dd7f92bdc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18c2f9d1bcf349e7830247cda4eebfd7",
            "placeholder": "​",
            "style": "IPY_MODEL_bcfac0310dc5493f873d099784c6f0fd",
            "value": " 104M/104M [00:00&lt;00:00, 169MB/s]"
          }
        },
        "e84459a790ec46ed990c8270b7235218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b6757afd74545cb893a63daf4c0d8bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "710d9ac61a9744dd90b31aba55ee226a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "412966c4adf84d79b36bf0d18b0d9263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7610747ee2d40c3a2698385705c8b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18c2f9d1bcf349e7830247cda4eebfd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcfac0310dc5493f873d099784c6f0fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fef290cdfe16416d906f9e74fc0ea96a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7412a7be60404b878d20f45c4dd95635",
              "IPY_MODEL_6e46d674685c497289d117fd747cf397",
              "IPY_MODEL_02bdb892ef1a4040865187854f0debd7"
            ],
            "layout": "IPY_MODEL_9a843368ca7040a99d6ac7006fd192a5"
          }
        },
        "7412a7be60404b878d20f45c4dd95635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90dba3b740fc4e7c95636289abe5f842",
            "placeholder": "​",
            "style": "IPY_MODEL_98e9db93debb4b999cf1d5191334a66a",
            "value": "fine_2.pt: 100%"
          }
        },
        "6e46d674685c497289d117fd747cf397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_907b9cddfa8b4e05828ee4608904621b",
            "max": 3741740229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_581e7fa20e45498c914ea4a2c38176a0",
            "value": 3741740229
          }
        },
        "02bdb892ef1a4040865187854f0debd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2a87c4aa2664b108aaadb70e9371338",
            "placeholder": "​",
            "style": "IPY_MODEL_2fc78374f66d41938711a3a7606f4f58",
            "value": " 3.74G/3.74G [00:19&lt;00:00, 171MB/s]"
          }
        },
        "9a843368ca7040a99d6ac7006fd192a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90dba3b740fc4e7c95636289abe5f842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98e9db93debb4b999cf1d5191334a66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "907b9cddfa8b4e05828ee4608904621b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "581e7fa20e45498c914ea4a2c38176a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2a87c4aa2664b108aaadb70e9371338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fc78374f66d41938711a3a7606f4f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7679ebdba94842e793730222c31ecb43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3fa3e9e985fe4ba99d116719e98c4b8b",
              "IPY_MODEL_57ebf49d76044909b159f0ec3fc42ab2",
              "IPY_MODEL_51dfec969c49482281754e51a4c9778c"
            ],
            "layout": "IPY_MODEL_b167fd315e344a8f8db90ddee7f87b4c"
          }
        },
        "3fa3e9e985fe4ba99d116719e98c4b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70171b289e304c9a82a13ca0a492295f",
            "placeholder": "​",
            "style": "IPY_MODEL_d571d2251e814f4ba99dd6497ef8c98e",
            "value": "text_2.pt: 100%"
          }
        },
        "57ebf49d76044909b159f0ec3fc42ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb51cc4cbbbe4a0b8f6362816d0b381a",
            "max": 5353258741,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d22fd76b0ab408ba8982ce947ca758b",
            "value": 5353258741
          }
        },
        "51dfec969c49482281754e51a4c9778c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e083fb994b94f47924e6a34a298719c",
            "placeholder": "​",
            "style": "IPY_MODEL_5f60ea35c7f64172b3b8cf99051f95eb",
            "value": " 5.35G/5.35G [00:17&lt;00:00, 356MB/s]"
          }
        },
        "b167fd315e344a8f8db90ddee7f87b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70171b289e304c9a82a13ca0a492295f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d571d2251e814f4ba99dd6497ef8c98e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb51cc4cbbbe4a0b8f6362816d0b381a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d22fd76b0ab408ba8982ce947ca758b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e083fb994b94f47924e6a34a298719c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f60ea35c7f64172b3b8cf99051f95eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a27556f0a0c45a1a8eb1b04b7127834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c47678a6f59d437ca952ae2d094c3861",
              "IPY_MODEL_82b0f5b78aa440c2af3b6693c08d97fc",
              "IPY_MODEL_33d75ee333204273a0a46b8e1283a240"
            ],
            "layout": "IPY_MODEL_cc7f9823eefa4fcc97e3484af4647ae2"
          }
        },
        "c47678a6f59d437ca952ae2d094c3861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0486770b6634729be6dcec83c48000c",
            "placeholder": "​",
            "style": "IPY_MODEL_89d221f036304c138c4cd052e99e7244",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "82b0f5b78aa440c2af3b6693c08d97fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b0fa2900392488da3bc0fc449409a44",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d135c792ff0c4b908491eb957c3e827f",
            "value": 29
          }
        },
        "33d75ee333204273a0a46b8e1283a240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb3b07086ed74e978b21886c2031c83b",
            "placeholder": "​",
            "style": "IPY_MODEL_48c8b3e08e8a4ba09a514a24a0f5e340",
            "value": " 29.0/29.0 [00:00&lt;00:00, 2.57kB/s]"
          }
        },
        "cc7f9823eefa4fcc97e3484af4647ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0486770b6634729be6dcec83c48000c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89d221f036304c138c4cd052e99e7244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b0fa2900392488da3bc0fc449409a44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d135c792ff0c4b908491eb957c3e827f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb3b07086ed74e978b21886c2031c83b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48c8b3e08e8a4ba09a514a24a0f5e340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7144ff96fc134c3bbd4ed44011294ea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97c25453452e4d528cdaf33b9259692c",
              "IPY_MODEL_2b33a056b7e347fc816f5696cde7902d",
              "IPY_MODEL_df8ffe58bcf14d8e85480466a90e8c40"
            ],
            "layout": "IPY_MODEL_826a5d9da9644f58b0a2400f31341a03"
          }
        },
        "97c25453452e4d528cdaf33b9259692c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_034bb8ab26624c0e95c4c64a086e4d30",
            "placeholder": "​",
            "style": "IPY_MODEL_8546b3d5e29b4743996a55abc571bb47",
            "value": "vocab.txt: 100%"
          }
        },
        "2b33a056b7e347fc816f5696cde7902d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75dd621c86ab4aa1bdb8d1dd88ec466c",
            "max": 995526,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cef203e2c25b474a838fee4eabdbb30b",
            "value": 995526
          }
        },
        "df8ffe58bcf14d8e85480466a90e8c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fb18574442a4bf086bf95d57677fdab",
            "placeholder": "​",
            "style": "IPY_MODEL_a9ccb0db815b42d49cf084aa68e8f8ee",
            "value": " 996k/996k [00:00&lt;00:00, 4.09MB/s]"
          }
        },
        "826a5d9da9644f58b0a2400f31341a03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "034bb8ab26624c0e95c4c64a086e4d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8546b3d5e29b4743996a55abc571bb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75dd621c86ab4aa1bdb8d1dd88ec466c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef203e2c25b474a838fee4eabdbb30b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fb18574442a4bf086bf95d57677fdab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9ccb0db815b42d49cf084aa68e8f8ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e881b86e7f84ab08029d5e18454affd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4559d6b7f484751a1d4e266cf03d701",
              "IPY_MODEL_978b3913fd194cc085d6fdf54096eebb",
              "IPY_MODEL_5683f3402d724e81908bf4479e213bc4"
            ],
            "layout": "IPY_MODEL_8560a9b641e6473ea01d16b60ef06f34"
          }
        },
        "f4559d6b7f484751a1d4e266cf03d701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3905e4817ed449f9a6e1717b9c8188d4",
            "placeholder": "​",
            "style": "IPY_MODEL_e3b1874249e245b895c04b76c15b320a",
            "value": "tokenizer.json: 100%"
          }
        },
        "978b3913fd194cc085d6fdf54096eebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eebf9f171f014eb19a2bc22bf3e5d05d",
            "max": 1961828,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49c47511cad44c049023d9a2a6ad8c72",
            "value": 1961828
          }
        },
        "5683f3402d724e81908bf4479e213bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07cab618c26e4e7580e752bdefc1287d",
            "placeholder": "​",
            "style": "IPY_MODEL_98577a84e61045faa90b5bc1fc8a5dc9",
            "value": " 1.96M/1.96M [00:00&lt;00:00, 3.97MB/s]"
          }
        },
        "8560a9b641e6473ea01d16b60ef06f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3905e4817ed449f9a6e1717b9c8188d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3b1874249e245b895c04b76c15b320a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eebf9f171f014eb19a2bc22bf3e5d05d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49c47511cad44c049023d9a2a6ad8c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07cab618c26e4e7580e752bdefc1287d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98577a84e61045faa90b5bc1fc8a5dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba2aed87246d4170855105a7ca2c588b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_755937fcfd384b21be71b718f01cdfa0",
              "IPY_MODEL_6dfec48e70f04d43a0fb6ee7833b8dfb",
              "IPY_MODEL_d868bcb81419456c8eb2ad9c1a12bf43"
            ],
            "layout": "IPY_MODEL_7480c94763fc49ffaf173f4e17f9ed20"
          }
        },
        "755937fcfd384b21be71b718f01cdfa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b21eca06b474df9a7943b13cbd10ea7",
            "placeholder": "​",
            "style": "IPY_MODEL_05f64218d14e4efebf072251aa33303d",
            "value": "config.json: 100%"
          }
        },
        "6dfec48e70f04d43a0fb6ee7833b8dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82b88458eb5f41d99fe825b8fb6ee832",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e2cc75781614c6bb82decf7e747fa3b",
            "value": 625
          }
        },
        "d868bcb81419456c8eb2ad9c1a12bf43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bfdc4b2ae4d41cc932dcf3cfdb54c31",
            "placeholder": "​",
            "style": "IPY_MODEL_6e09f777544a44cda57f6320f243226e",
            "value": " 625/625 [00:00&lt;00:00, 55.5kB/s]"
          }
        },
        "7480c94763fc49ffaf173f4e17f9ed20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b21eca06b474df9a7943b13cbd10ea7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05f64218d14e4efebf072251aa33303d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82b88458eb5f41d99fe825b8fb6ee832": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e2cc75781614c6bb82decf7e747fa3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4bfdc4b2ae4d41cc932dcf3cfdb54c31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e09f777544a44cda57f6320f243226e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a581f984b0b84fcdb0314609c3af4b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee8df020601f4d4186a5c7980ec4946c",
              "IPY_MODEL_453394a755744c07af6fcd45c8d03beb",
              "IPY_MODEL_128439ae98ee4d1098ed6a03d38ca581"
            ],
            "layout": "IPY_MODEL_64229edb1fb74e5096fa228348f1003f"
          }
        },
        "ee8df020601f4d4186a5c7980ec4946c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ce3dc22f7834beb9c43ba2bd376958d",
            "placeholder": "​",
            "style": "IPY_MODEL_9d587ce51b314f90ab12b0bef04c1c78",
            "value": "coarse_2.pt: 100%"
          }
        },
        "453394a755744c07af6fcd45c8d03beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d78dc8f3bbf45628378f7eb9ffdb95f",
            "max": 3934534533,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7b0be4e73134853b04e36fea028f1cb",
            "value": 3934534533
          }
        },
        "128439ae98ee4d1098ed6a03d38ca581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddfb5bc44bd14b3b9de16749b81ca23a",
            "placeholder": "​",
            "style": "IPY_MODEL_132f07038cc3433ea284c63e4e31b12d",
            "value": " 3.93G/3.93G [00:10&lt;00:00, 373MB/s]"
          }
        },
        "64229edb1fb74e5096fa228348f1003f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ce3dc22f7834beb9c43ba2bd376958d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d587ce51b314f90ab12b0bef04c1c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d78dc8f3bbf45628378f7eb9ffdb95f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7b0be4e73134853b04e36fea028f1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ddfb5bc44bd14b3b9de16749b81ca23a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "132f07038cc3433ea284c63e4e31b12d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "727fe931a1584644ae2806397f895656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b43efa154a82416db888601a0bfb5cf2",
              "IPY_MODEL_c6dd6b0e3aa1414984a30d9120874e4b",
              "IPY_MODEL_4494c64e7a334748ba6dd6c7ad1a06b8"
            ],
            "layout": "IPY_MODEL_95c63393ceb84a19ad0605c2730dec8c"
          }
        },
        "b43efa154a82416db888601a0bfb5cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_212ad381652a4be1909543df31bc90fa",
            "placeholder": "​",
            "style": "IPY_MODEL_3e4dca17e3c74c17bb4d931101987b29",
            "value": "fine_2.pt: 100%"
          }
        },
        "c6dd6b0e3aa1414984a30d9120874e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aa9f91391594227872598ca62f666af",
            "max": 3741740229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_177a16783e774dd4b3432f6dd5eab692",
            "value": 3741740229
          }
        },
        "4494c64e7a334748ba6dd6c7ad1a06b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95583572ef3f45e0b5a40b9bf56fcb6c",
            "placeholder": "​",
            "style": "IPY_MODEL_8fa9b2e09346481f903dd376118c2061",
            "value": " 3.74G/3.74G [00:10&lt;00:00, 229MB/s]"
          }
        },
        "95c63393ceb84a19ad0605c2730dec8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "212ad381652a4be1909543df31bc90fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e4dca17e3c74c17bb4d931101987b29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4aa9f91391594227872598ca62f666af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "177a16783e774dd4b3432f6dd5eab692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95583572ef3f45e0b5a40b9bf56fcb6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa9b2e09346481f903dd376118c2061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}